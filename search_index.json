[["index.html", "An Introduction to Camera Trap Data Management and Analysis in R Chapter 1 Introduction 1.1 Ethos 1.2 What this course is 1.3 What this course is not 1.4 How to use this book 1.5 Get in touch 1.6 Cite the course 1.7 Acknowledgements", " An Introduction to Camera Trap Data Management and Analysis in R Christopher Beirne, the Wildlife Coexistence Lab, UBC, and the WildCam Network 2025-01-03 Chapter 1 Introduction The number of projects employing camera traps to understand ecological phenomena is growing rapidly – as are the number of statistical tools to analyze the resultant data. Consequently, the management and analysis of camera trap data can seem complex and overwhelming. This course aims to guide participants in effective ways to store, manipulate and analyze camera trap data within the R statistical environment. The idea for this course was born out of the realization that many of the analytical frameworks we apply to camera trap data do not have specific examples involving cameras, which can make learning new methods intimidating. To address this we wanted to develop a resource which takes a single data set from raw data to analysis, showing all of the steps in between! Hopefully this course will give you the tools to manage, analyze and share camera trap data in an approachable and practical way! 1.1 Ethos This course is built on the foundation of “standardized data” - data formatted to meet an international standard. The benefits of standardizing your data are explained in detail in the data standardization chapter, the principal benefit is that if you copy the data formatting used here, it should be very easy to apply the methods described here to new datasets! 1.2 What this course is An R code reference manual for basic concepts in the exploration and analysis of camera data A tool to provide hands on experience of standardized camera trap data in R A link to resources and papers which showcase the types of analyses you can perform A stepping stone to more complex analyses 1.3 What this course is not The “best” way to explore or analyse your data (although we hopefully provide a good foundation) A statistics course - the analyses presented within this guide are purposefully overly simplified in order to act as an introduction to each topic without being overwhelming A perfect copy of published analyses - some of the conclusions from our simplified models may differ from published results… trust the publications 1.4 How to use this book You will either be using this book as a companion to the “An Introduction to Camera Trap Data Management and Analysis in R” course or be working through the examples alone. Regardless, you can use this book in the following ways: If this is your first time analyzing camera data - or you are an R course participant - work through the book chapter by chapter If you are looking to apply a specific method, browse the sections from the Table of Contents, or use the search box if you are after something specific Download the example data via dropbox and ‘follow-along’ by cutting and pasting the code Explore additional “Further reading” sections which highlight key literature or R packages in each section Important note The code chunks are color coded by their function: # Green chunks are essential code which must be copied and run in # R for the document to work # Orange chunks are non-essential code which to help your # understanding and skill development When you hover over these colored chunks a ‘copy’ symbol will appear in the top right to transfer your code! ## [1] &quot;Gray code chucks are code output from R&quot; 1.5 Get in touch If you have any questions about this document and the information it contains, please email us or, better still, submit an issue on our the course GitHub page. 1.6 Cite the course If you would like to cite this course, please use the following: Beirne, C. &amp; Burton, C. (2022). An Introduction to Camera Trap Data Management and Analysis in R https://zenodo.org/doi/10.5281/zenodo.10524184 1.7 Acknowledgements This course was produced by Christopher Beirne, Cole Burton’s Wildlife Coexistence Lab at UBC, and the WildCAM Network. "],["prep.html", "Chapter 2 Preparing for the course 2.1 Install R 2.2 Install RStudio 2.3 Install the required packages 2.4 Create an R project 2.5 Download the data files 2.6 The example datasets 2.7 Practise before the course", " Chapter 2 Preparing for the course In order reproduce the data management and analysis examples detailed in this book, you will need to take the following steps. Install R Install R Studio Install associated R packages Create an R project Put the example data in the R project The steps to do this are outlined below: If you get stuck send me an email! 2.1 Install R Windows Click on this link and then Download R-4.2.1 for Windows. Mac Click on this link and then the R-4.2.1.pkgs link. If you already have R please update your R client so that it is on at least version 4.2.1. Checkout the installR package to this directly from the terminal. 2.2 Install RStudio Through the course we will use RStudio to interact with R. Please download RStudio Desktop (Free) from the RStudio website. Alternatives to RStudio exist and we are happy if you want to use one of those instead! Installation check Open RStudio. If all has gone well you should see something like this: 2.3 Install the required packages The next step is to install the packages required for the course. We need to do this in two steps: first, install packages which are on CRAN (the Comprehensive R Archive Network), and then install those which are not. Finally, we have provided the links to install and setup R Google Earth Engine (rgee). This is an optional step for advanced users only - we will not be using it in the course! 2.3.1 CRAN packages Copy and paste the following code into your R terminal and press enter. Note - if you hover over the code block a copy button appears in the top right. # A list of the required packages list.of.packages &lt;- c(&quot;activity&quot;, &quot;corrplot&quot;, &quot;cowplot&quot;, &quot;dplyr&quot;, &quot;elevatr&quot;, &quot;gfcanalysis&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;iNEXT&quot;, &quot;kableExtra&quot;, &quot;Hmsc&quot;, &quot;leaflet&quot;, &quot;lme4&quot;, &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;MCMCvis&quot;, &quot;modisfast&quot;, &quot;osmdata&quot;, &quot;pals&quot;, &quot;plotly&quot;, &quot;remotes&quot;, &quot;rmarkdown&quot;, &quot;sf&quot;, &quot;spOccupancy&quot;, &quot;stars&quot;, &quot;stringr&quot;, &quot;terra&quot;, &quot;tibble&quot;, &quot;tidyr&quot;, &quot;unmarked&quot;, &quot;viridis&quot;, &quot;jtools&quot;, &quot;vegan&quot;, &quot;MuMIn&quot;, &quot;usedist&quot;, &quot;taxize&quot;) # A check to see which ones you have and which are missing new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # Code which tells R to install the missing packages if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) 2.3.2 Other packages Some packages must be compiled from sources other than CRAN. Copy and paste following code block into your R console and press enter. # We use the remotes package to access package repositories not on the # CRAN interface (e.g. on github) library(remotes) remotes::install_github(&quot;RS-eco/traitdata&quot;) remotes::install_github(&quot;arcaravaggi/remBoot&quot;) remotes::install_github(&quot;annam21/spaceNtime&quot;) 2.3.3 An Earthdata Server account IMPORTANT to access MODIS data (and other great products) rapidly through the modisfast package you need to create your own login for the associated Earthdata servers. To create your free account go to : https://urs.earthdata.nasa.gov/. and make a note of your credentials for the Analysis Covariates chapter. 2.4 Create an R project If you do not already work within R projects - you should! It allows you to work on multiple projects at the same time, and makes specifying file paths much simpler. Finally, if you want to incorporate GiHub into your workflows in the future, the use of R Projects is essential! To create an R project for this course: Step 1 Click on File -&gt; New project Step 2 Click New Directory Step 3 Then click New Project Step 4 Give your project a name and choose a folder to save it in (you do not have to copy the options here): Step 5 Then click Create Project The best thing about R Projects is that all the files contained within it can be specified relative to the project folder - no more long file paths to deal with! Currently the ‘Files’ tab in the bottom right should be empty (aside from the .rproj file): Take note of the file path in the image above. For me it is C:/Users/Dropbox/wildco_R_course as this is where you will copy the data files to in later steps. You can also go straight to the root directory the project by clicking the three dots to the right of the file path in the Files tab! Next, lets download the data files we need for this course! 2.5 Download the data files Step 1 Click on the following dropbox link: Data Management and Analysis Files Step 2 Click download: Step 3 Extract the files (it doesn’t matter where to), then open the data_for_intro_to_camera_traps folder and copy the data folder it contains. Step 4 Then paste the data folder into your newly created R project folder. Step 5 If everything has worked, your `Files’ window in R Studio should now look like this: And you are ready for the course! 2.6 The example datasets If you navigate through data &gt; raw_data you will see that there are two files. One called example_data the other called your_data. They are summarised below: 2.6.1 example_data We have provided a subset of data derived from the WildCo Lab’s “Algar Restoration Project”. In the interest of teaching and code processing times, we have not provided a full dataset. Rather the data represent a sub-sample of sites (38 of 73 available) and years (2 of 4 available). If you want more information on this project, see the following papers (and the archived datasets they contain): Tattersall, E. R., Burgar, J. M., Fisher, J. T., &amp; Burton, A. C. (2020). Mammal seismic line use varies with restoration: Applying habitat restoration to species at risk conservation in a working landscape. Biological Conservation, 241, 108295. Tattersall, E. R., Burgar, J. M., Fisher, J. T., &amp; Burton, A. C. (2020). Boreal predator co‐occurrences reveal shared use of seismic lines in a working landscape. Ecology and Evolution, 10(3), 1678-1691. Beirne, C., Sun, C., Tattersall, E. R., Burgar, J. M., Fisher, J. T., &amp; Burton, A. C. (2021). Multispecies modelling reveals potential for habitat restoration to re‐establish boreal vertebrate community dynamics. Journal of Applied Ecology, 58(12), 2821-2832. Burton, A. C., Beirne, C., Sun, C., Granados, A., Procko, M., Chen, C., … &amp; Burgar, J. (2022). Behavioral “bycatch” from camera trap surveys yields insights on prey responses to human‐mediated predation risk. Ecology and evolution, 12(7), e9108. Sun, C., Burgar, J. M., Fisher, J. T., &amp; Burton, A. C. (2022). A cautionary tale comparing spatial count and partial identity models for estimating densities of threatened and unmarked populations. Global Ecology and Conservation, 38, e02268. Sun, C., Beirne, C., Burgar, J. M., Howey, T., Fisher, J. T., &amp; Burton, A. C. (2021). Simultaneous monitoring of vegetation dynamics and wildlife activity with camera traps to assess habitat change. Remote Sensing in Ecology and Conservation, 7(4), 666-684. 2.6.2 your_data The your_data folder contains files I have provided. This is the subset of an unpublished (and as yet un-analysed) dataset. I will leave it to you to find out more about it during the course! You can replace this data set with your own if you wish. If you want to use your own data it MUST be in ‘Wildlife Insights’ format - see the descriptions here and for a existing example/template see here. NOTE delete the description row if you are using your own data. The green/bolded columns shown in the template example are the absolute minimal essential columns - you can have more columns with other information! 2.7 Practise before the course Finally, if you want to practice your R skills before the course, here are two (of many) great resources out there: R Programming for Beginners | Complete Tutorial | R &amp; RStudio - Some great introductory videos to working in R Modern R with the tidyverse - A book introducing tidyverse data manipulation "],["preprocessing-and-labelling.html", "Chapter 3 Preprocessing and labelling 3.1 Data storage 3.2 Preprocessing 3.3 Labelling 3.4 End dates and outages", " Chapter 3 Preprocessing and labelling Once you have deployed your camera traps and brought your SD cards. We have several steps we need to perform before we can start analyzing the data: backup the data pre-process the files label the footage update deployment end dates We summarise each step below and point to useful tools where necessary. 3.1 Data storage The file structure of your data backups depends on the structure of your project. We use one of two different options, which each have their merits: 1) Location based This is likely the most intuitive method if you are manually sorting data or using an image labeller (software to manage your camera data) which uses the location as the key organizing element. You would make a folder using the ‘placename’ (unique location where a camera is deployed), then copy all of the data relating to that site within it (left). Note, if you had multiple camera deployments you would have nested folders with the ‘deployment_id’ as the name: 2) Deployment based Increasingly camera trap management platforms are ‘deployment’ driven rather than location based (e.g. Wildlife Insights). In this instance, the images are placed within a folder named with the deployment_id (the unique code corresponding to that deployment), typically within a single folder. In this scenario, we would have a folder called ‘to upload’ with all of the unique deployment folders within it. Then, once the folder has been upload to the platform, then the folder is moved to an “uploaded” folder: Crucially - make redundant copies to ensure you do not lose data. We make both local and cloud-based copies of our data sets. 3.2 Preprocessing The following steps represent optional elements to apply to your data. Whether you need them depends on your questions, the platform you are using to label your data, and the volume of images you will be processing. 3.2.1 Renaming When a camera takes images, it applies sequential names which are duplicated across cameras (e.g. RCNX0001, RCNX0002 etc). In the future, if files are accidentally moved it would be difficult (if not impossible) to trace them back their origin. One way to get around this is to rename every camera image with a unique code (e.g. placename_datetime) which will ensure that line of data you generate can be traced back to an image, regardless of how it is stored. We have created a tool which can be applied to folders of images organised by location and deployment, to create unique codes for each image:the WildCo Image Renamer. The repository has an example dataset which you can play around with to get familiar with the tool. 3.2.2 Automated Labelers Once you have backed up and renamed your images, you may want to process them with an Artificial Intelligence (AI) labeler. Although they are pretty cool and in vogue right now, the desicion to use one (or not) should be based on several points: - The number of image you have to process If you only have a small dataset (a few thousand images) it is likely easier to do manually - Whether there is an AI labeler validated for your study area and strata Despite the claims of their authors, AI labelers are not perfect. If they haven’t been validated in your survey location then use extreme caution when applying it. For example, an AI algorithm developed for terrestrial camera traps data will likely not work well on an arboreal dataset. - How much money you have For AI labelers to run quickly, you may need some very expensive computer gear or cloud computing time. Do not assume that this is cheaper than manual labor! - The resolution of the labels you require AI labelers are getting pretty good at sifting out blank images, but they have a long way to be before they can reliably split ground squirrel species (Urocitellus sp), or long-nosed armadillo species (Dasypus sp.)! For a very pragmatic and informed take on the current state of the art, see Saul Greenberg’s Automated Image Recognition for Wildlife Camera Traps: Making it Work for You. report. One of the biggest players in the game is undoubtedly Megadetector. Click the link for an overview of the machine learning model and how it might work for you. Finally, some platforms now have their own inbuilt labeling AI (e.g. Wildlife Insights), which is certainly much more accessible than developing your own. Our only advice is be weary of the identifications they generate and always check your data (a.k.a. keep a human in the loop - at least for now). 3.2.3 Sensitive images One of the benefits of AI labelers is you can use them to remove sensitive information (such as peoples identities) from images without ever looking at them. An example of this would be camera trapping in protected areas where it is not possible to ask every person if they are happy being photographed for science. Instead, we can use megadetector (or another AI labeler) to tell us when a human is detected in an image, then blur the area of that photo to remove individually identifying information. Previously researchers had to delete the human images to be compliant with privacy requirements - which throws away valuable data of human use. The WildCo lab has developed a tool to blur human images using Megadetector outputs: WildCo_Face_Blur. Click the link for details on how to use it. For a discussion of its application in a recreational ecology context see: Fennell, Mitchell, Christopher Beirne, and A. Cole Burton. “Use of object detection in camera trap image identification: Assessing a method to rapidly and accurately classify human and animal detections for research and application in recreation ecology.” Global Ecology and Conservation 35 (2022): e02104. 3.2.4 Timelapse extraction Timelapse photographs can be critical to determine when cameras are functioning, particularly in low productivity environments where wildlife detections are rare. We highly recommend you take a photo at noon each day! They can also be used to generate site-level vegetation indices, such as NDVI, as they are taken at the same time every day. However, you likely don’t want to be sort through thousands of images of leaves and grass, or if you want to extract the images to run through a different program (e.g. phenopix package - see the covariates chapter). To quickly extract timelapse images we develop some code which uses the metadata of the images to filter out timelapse photos from motion detected photos. It is packaged up as part of the WildCo_Image_renamer script. 3.3 Labelling We often get asked what the best software/data platform is for labeling images… and the pragmatic answer is that it does not matter as long as you export your data in a standardised format (see the data standardisation chapter. The truth is that different projects have different needs: If you have a poor internet connection you might need to use a standalone offline software, such as Timelapse Or if you work internationally with a large team of labelers who will tag images simultaneously, an online data platform, such as Wildlife Insights, might be essential Dan Morris has curated a fantastic list of currently available tools here: Everything I know about machine learning and camera traps. In a nutshell: Data platforms are web- and desktop-based tools used for efficient and standardized data management, sharing, and analysis of remote camera data. A number of platforms exist so it is important that users choose the one best suited to their needs. To help camera trap users make this decision, the Wildcam network has developed a comparison of different camera data platforms. It provides an overview of platforms and software used in remote camera research in western Canada. As software and online tools are often subject to frequent updates and change, we recognize this as a document subject to change over time. Click here to review the comparison (last updated June 2020). We welcome feedback at any time (info@wildcams.ca) Software are programs specifically designed for camera trap photos and their associated data is now recognized as the best method for data processing. There are quite a few programs available for practitioners, but many of them have most of the same functionalities. The relatively few unique features that distinguish programs will help to determine what software to use, and what features are needed for specific studies will vary depending on their study designs. See: Wearn, O. R. and P. Glover-Kapfer. 2017. Camera-trapping for conservation: a guide to best-practices. WWF conservation technology series 1.1 181. Young, S., J. Rode‐Margono and R. Amin. 2018. Software to facilitate and streamline camera trap data management: a review. Ecology and Evolution, 8: 9947-9957. 3.4 End dates and outages It is very important to note that camera deployments do not end when you pickup the camera - they end when the camera stops collecting comparable data. The best time to record date a camera stops functioning probably is when you are labeling images. Do not cut this corner! Below is the same camera station, at two points in time. The data from these are not comparable - if a tree fell on you whilst you were out counting animals you would probably count less effectively too! We would edit the deployment end to to reflect when it stopped recording comparable data (not all examples are as clear cut as this one). "],["standard.html", "Chapter 4 Metadata standardisation 4.1 The Wildlife Insights Minimum Metadata Standards", " Chapter 4 Metadata standardisation The images produced by camera traps alone are useless. We need to keep accurate records of how the data were collected, labelled, and manipulated if we are to achieve the goal of synthesizing data from multiple projects. Thus, metadata is simply “data that provides information about other data”. The benefits of ‘standardizing’ the metadata associated with camera traps, or other sensors of biodiversity, are hopefully clear - it should facilitate the rapid and robust exploration, analysis and sharing of information on wildlife populations. Ultimately resulting in more robust, repeatable, and timely research and management decisions. 4.1 The Wildlife Insights Minimum Metadata Standards The convention we use in this course is the data standards used by Wildlife Insights. Their standard format is composed of four different elements: Project data proj.csv a dataframe containing key information about the project itself, e.g. how the cameras were deployed and what the target features were. Image data img.csv a dataframe containing all of the information contained within each image. This information is typically added by humans, but increasing we are using artificial intelligence to speed up this process. Deployment data dep.csv a dataframe listing the activity of the camera traps involved in your study, and any issues encountered during deployments which may influence their analysis Camera data cam.csv a dataframe all the cameras deployed in the project Below we give a quick summary and explanation of each. First, read in the data files: pro &lt;- read.csv(&quot;data/raw_data/example_data/proj.csv&quot;, header=T) img &lt;- read.csv(&quot;data/raw_data/example_data/img.csv&quot;, header=T) dep &lt;- read.csv(&quot;data/raw_data/example_data/dep.csv&quot;, header=T) cam &lt;- read.csv(&quot;data/raw_data/example_data/cam.csv&quot;, header=T) Let’s look at each one in turn. 4.1.1 Project data The project files contains a general description of the project. It should give someone a helicopter overview of your project, and provide the data usage guidelines. project_id AlgarRestorationProject project_name AlgarRestorationProject project_short_name Algar project_objectives Investigate medium-large bodied mammal habitat use in response to human recreation spatially and temporally project_species NA project_species_individual NA project_sensor_layout Stratified project_sensor_layout_targeted_type Seismic lines project_bait_use No project_bait_type NA project_stratification Seismic line restoration treatements and controls project_stratification_type Offline, HumanUse project_sensor_method Sensor.Detection project_individual_animals NA project_blank_images yes project_sensor_cluster NA project_admin Cole Burton project_admin_email cole.burton@ubc.ca project_admin_organization University of British Columbia country_code NA embargo NA initiative_id NA metadata_license NA image_license NA data_citation Beirne, Christopher, Catherine Sun, Erin R. Tattersall, Joanna M. Burgar, Jason T. Fisher, and A. Cole Burton. Multispecies modelling reveals potential for habitat restoration to re‐establish boreal vertebrate community dynamics. Journal of Applied Ecology 58, no. 12 (2021): 2821-2832. count_optional no project_type image 4.1.2 Image data This file contains the image labels - what is in each picture and its properties. Each image you have processed is linked to at least one row in the detection data. Multiple rows may exist if there are multiple species in a camera trap image, or if you are identifying multiple unique individuals. project_id deployment_id image_id filename location is_blank identified_by wi_taxon_id class order family genus species common_name uncertainty timestamp number_of_objects age sex animal_recognizable individual_id individual_animal_notes behavior highlighted markings cv_confidence license placename group_size temperature AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-01.JPG Algar27__2018-04-13__13-51-01.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA NA 2018-04-13 13:51:01 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-02.JPG Algar27__2018-04-13__13-51-02.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA NA 2018-04-13 13:51:02 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-03.JPG Algar27__2018-04-13__13-51-03.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA NA 2018-04-13 13:51:03 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-06.JPG Algar27__2018-04-13__13-51-06.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA NA 2018-04-13 13:51:06 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-07.JPG Algar27__2018-04-13__13-51-07.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA NA 2018-04-13 13:51:07 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-09.JPG Algar27__2018-04-13__13-51-09.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA NA 2018-04-13 13:51:09 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA 4.1.3 Deployment data This is the camera deployment data - where the deployment occurred, when it started, when it ended and other relevant information about each unique deployment. project_id deployment_id placename longitude latitude start_date end_date bait_type bait_description feature_type feature_type_methodology camera_id camera_name quiet_period camera_functioning sensor_height height_other sensor_orientation orientation_other plot_treatment plot_treatment_description detection_distance subproject_name subproject_design event_name event_description event_type recorded_by AlgarRestorationProject ALG027_2018-04-11 ALG027 -112.4735 56.33280 2018-04-11 2018-11-15 None NA HumanUse NA NA NA 1 Camera Functioning 100 NA NA NA NA NA NA Restoration NA NA NA NA NA AlgarRestorationProject ALG027_2018-11-15 ALG027 -112.4735 56.33280 2018-11-15 2019-04-03 None NA HumanUse NA NA NA 1 Camera Functioning 100 NA NA NA NA NA NA Restoration NA NA NA NA NA AlgarRestorationProject ALG027_2019-04-03 ALG027 -112.4735 56.33280 2019-04-03 NA None NA HumanUse NA NA NA 1 Camera Functioning 100 NA NA NA NA NA NA Restoration NA NA NA NA NA AlgarRestorationProject ALG029_2018-04-07 ALG029 -112.5483 56.39474 2018-04-07 2018-11-15 None NA HumanUse NA NA NA 1 Camera Functioning 100 NA NA NA NA NA NA Restoration NA NA NA NA NA AlgarRestorationProject ALG029_2018-11-15 ALG029 -112.5483 56.39474 2018-11-15 2019-04-02 None NA HumanUse NA NA NA 1 Camera Functioning 100 NA NA NA NA NA NA Restoration NA NA NA NA NA AlgarRestorationProject ALG029_2019-04-02 ALG029 -112.5483 56.39474 2019-04-02 2019-11-20 None NA HumanUse NA NA NA 1 Camera Functioning 100 NA NA NA NA NA NA Restoration NA NA NA NA NA 4.1.4 Camera inventory An inventory of all the cameras used in the project. Ideally, each camera would be represented in the deployment data. This technically isn’t 100% necessary to analyse your dataset, although there are some scenarios where it might help. project_id camera_id camera_name make model serial_number year_purchased AlgarRestorationProject 1 C0001 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 2 C0002 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 3 C0003 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 4 C0004 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 5 C0005 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 6 C0006 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 7 C0007 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 8 C0008 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 9 C0009 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 10 C0010 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 11 C0011 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 12 C0012 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 13 C0013 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 14 C0014 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 15 C0015 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 16 C0016 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 17 C0017 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 18 C0018 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 19 C0019 NA Reconyx Hyperfire 2 NA NA AlgarRestorationProject 20 C0020 NA Reconyx Hyperfire 2 NA NA 4.1.5 Important note These are simply the minimum sheets you require - we derive a lot of other useful data frames when moving from raw camera data to analyzable camera data. See the Creating analysis dataframes section for further examples. Further Reading Forrester, T. et al. An open standard for camera trap data. Biodivers. Data J. 4, (2016). Meek, P. D., et al. “Recommended guiding principles for reporting on camera trapping research.” Biodiversity and conservation 23.9 (2014) RISC Wildlife Camera Metadata Protocol "],["error-checking.html", "Chapter 5 Error checking 5.1 Standardised exploration script 5.2 Formatting dates 5.3 Basic trapping summaries 5.4 Error checks 5.5 Diel activity check 5.6 Conclusion", " Chapter 5 Error checking The most important part of analyzing camera trap data is checking and exploring your data! Based on the projects we have worked on synthesizing multiple datasets from different sources… camera trappers are not doing a very good job of checking for errors. Working in R makes it possible to rapidly check your data, ideally in almost real time as you collect it. In an ideal world it would be worth downloading the data for your project at least once per month and checking that ‘everything’ looks good. But what constitutes ‘everything’? 5.1 Standardised exploration script In the Wildlife Coexistence Lab developed a standardized R script to check the data generated by camera trap projects. This script is kept on our WildCO Single Site Exploration GitHub page. Below we run through the important elements of checking camera trap data, and where they is a coding skill fundamental to the process, we explore it in more detail (a.k.a. skill checks). Let’s go! First, open the .Rproj file your created in the course preparation section. Then click File -&gt; New file -&gt; Rscript (alternatively you can use the R Markdown option if you are comfortable with that) After the file has opened, immediately save it as ’01_example_error_checking_and_export.R`. We will usually make a new R sheet for each chapter - however the error checking and analysis data creation chapters should be in the same document. Second, read in our standardized example datasets: # Load your data pro &lt;- read.csv(&quot;data/raw_data/example_data/proj.csv&quot;, header=T) img &lt;- read.csv(&quot;data/raw_data/example_data/img.csv&quot;, header=T) dep &lt;- read.csv(&quot;data/raw_data/example_data/dep.csv&quot;, header=T) cam &lt;- read.csv(&quot;data/raw_data/example_data/cam.csv&quot;, header=T) Next, load in the packages we will use in this chapter - we give a brief description of them too. Cut and paste the code block below. #Load Packages list.of.packages &lt;- c( &quot;leaflet&quot;, # creates interactive maps &quot;plotly&quot;, # creates interactive plots &quot;kableExtra&quot;, # Creates interactive tables &quot;tidyr&quot;, # A package for data manipulation &quot;dplyr&quot;, # A package for data manipulation &quot;viridis&quot;, # Generates colors for plots &quot;corrplot&quot;, # Plots pairwise correlations &quot;lubridate&quot;, # Easy manipulation of date objects &quot;taxize&quot;, # Package to check taxonomy &quot;sf&quot;) # Package for spatial data analysis # Check you have them in your library new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # load them if(length(new.packages)) install.packages(new.packages,repos = &quot;http://cran.us.r-project.org&quot;) lapply(list.of.packages, require, character.only = TRUE) 5.2 Formatting dates Every aspect of camera trapping involves manipulating date objects - calculating how long cameras were active, when detections occurred, working with timezones etc. Thus, as a camera trapper working in R you need to be comfortable dealing with them. Fortunately the process has been made far easier with the lubridate package. The first dates we need to convert are those in the deployment (dep) datasheet - the start and end times of each period of camera activity. The way lubridate works is you specify the order of the days, months years, hours, minutes and seconds with the codes d,m,y,h,m, and s respectively. 5.2.1 Skill check: lubridate Try importing the 25th of December in a couple of formats. Copy and run the following: #library(lubridate) # day-month-year dmy(&quot;24-12-2022&quot;) # year-month-day ymd(&quot;2022-12-24&quot;) The output should be identical. Note lubridate defaults to UTC - unless otherwise specified. Now the real power of lubridate lies in the fact that you can handle multiple different date formats in one column using the parse_date_time() function. This sometimes happens - I usually blame excel - but you could be merging two data sets formatted in different ways too. Lets try it: x &lt;- c(&quot;24-12-2022&quot;, &quot;2022-12-24&quot;, &quot;12-24-2022&quot;) #Three different date formats parse_date_time(x, c(&quot;ymd&quot;, &quot;dmy&quot;, &quot;mdy&quot;)) Again, they should give all the same output! Next, lets calculate the amount of time which has elapsed between two dates. A fundamental operation in the management of camera data. To do this we first create an interval object interval(date1, date2), then ask to return the object in days /ddays(1). Lets try it: # Specify your start and end dates start &lt;- ymd(&quot;2021-10-13&quot;) end &lt;- ymd(&quot;2021-12-11&quot;) # Specify the interval, and put it in days interval(start, end)/ddays(1) # Interval creates an &quot;interval object&quot; - run that along and see what it looks like # ddays() converts the native units of date objects in R (seconds) to days - run it on its own to see. How many days elapsed between those two dates? We can change the units of the output by changing the denominator: # Specify the interval, and put it in weeks interval(start, end)/ddays(7) ## [1] 8.428571 And in decimal years: interval(start, end)/ddays(365) ## [1] 0.1616438 Easy! If you want to learn more about the amazing functionality of the ‘lubridate’ package - check out the pdf Lubridate Cheatsheet 5.2.2 Deployment dates Lets get back to the camera data. Which lubridate format should we use for 2018-04-11? ymd() should do the job. Lets convert the date columns from character strings to date objects: # start dates dep$start_date &lt;- ymd(dep$start_date) # end dates dep$end_date &lt;- ymd(dep$end_date) Now lets make a new column in the deployment data called days, and calculate the interval for all the deployments: dep$days &lt;- interval(dep$start_date, dep$end_date)/ddays(1) We should then check the range of dates the cameras were active for. Things to look out for are: - 0’s A value of zero would mean a deployments which started and ended on the same day -&gt; it typically denotes a camera which malfunctioned instantly. - NA’s This is either an end_date which is NA e.g. if the camera was stolen, or it could be a date value which failed to parse e.g. if you had a typo in your date column such as ymd(\"202-212-24\") it would return NA - Negative numbers Are more common that you think… someone probably got the start and end dates the wrong way round when entering data. Lets look at the range of values we have: summary(dep$days) Cameras were active between 15 and 234 days per deployment, and we have an NA. Let’s see what it relates to: dep[is.na(dep$days)==T,] %&gt;% kbl() %&gt;% kable_styling(full_width = T) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) project_id deployment_id placename longitude latitude start_date end_date bait_type bait_description feature_type feature_type_methodology camera_id camera_name quiet_period camera_functioning sensor_height height_other sensor_orientation orientation_other plot_treatment plot_treatment_description detection_distance subproject_name subproject_design event_name event_description event_type recorded_by days 3 AlgarRestorationProject ALG027_2019-04-03 ALG027 -112.4735 56.3328 2019-04-03 NA None NA HumanUse NA NA NA 1 Camera Functioning 100 NA NA NA NA NA NA Restoration NA NA NA NA NA NA It was a camera that was on a ‘HumanUse’ feature - this camera was stolen! 5.2.3 Image dates We next need to convert the img$timestamp column. What lubridate format is required for a a date which looks like 2018-04-13 13:51:01? ymd_hms(&quot;2015-11-21 03:03:44&quot;) Lets apply this to our image dataset: img$timestamp &lt;- ymd_hms(img$timestamp) And do a quick check to see if all the dates parsed correctly. First check the range: range(img$timestamp) ## [1] &quot;2018-04-08 04:22:13 UTC&quot; &quot;2019-12-16 12:41:43 UTC&quot; We have data from early 2018 to late 2019. And check for NA’s: table(is.na(img$timestamp)) ## ## FALSE ## 15290 No NA’s - great! 5.3 Basic trapping summaries Now that our camera trap data are loaded into R, we can very quickly find out summary information about the dataset. These can feed directly into the methods section of your report/paper. First let’s count the number of unique locations: # Count the number of camera locations paste(length(unique(dep$placename)), &quot;locations&quot;); paste(length(unique(dep$deployment_id)), &quot;deployments&quot;);paste(nrow(img), &quot;image labels&quot;); paste(nrow(img[img$is_blank == TRUE,]), &quot;blanks&quot;) ## [1] &quot;38 locations&quot; ## [1] &quot;114 deployments&quot; ## [1] &quot;15290 image labels&quot; ## [1] &quot;2633 blanks&quot; 5.4 Error checks Lets start with the fundamental error checks required with a new data set: Camera locations Deployment date checks Image and deployment matching Taxonomy Diel time 5.4.1 Camera locations A common mistake in camera trap data sets is that the locations are not where they are supposed to be. The safest way to check your data is to plot them… preferably R! After synthesizing &gt;100 different projects from different data contributors for one project, we found ~20%(!) of submissions had a clear and obvious location errors (e.g. a camera station in the middle of the Atlantic). Don’t just take my word for it: (p.s. Mason is well worth a follow on Twitter ) 5.4.1.1 Skill check: Leaflet maps Below we make use of the fantastic ‘leaflet’ package to produce interactive plots to help us check our camera locations. leaflet has a tonne of different customization options and freely available, high resolution, base layers to choose from. Note - Leaflet is best used using tidyverse ‘pipe’ notation - %&gt;%. It allows you to add successive operations in the order that the elements occur. The simplest version of a leaflet map looks like this: m &lt;- leaflet() %&gt;% # call leaflet addTiles() %&gt;% # add the default basemap addCircleMarkers( # Add circles for stations lng=dep$longitude, lat=dep$latitude) m # return the map This is great! We can zoom in using the +/_ in the top left hand corner, and the default basemap is OpenStreetMap. The camera stations all appear to be in the right place (we don’t have any in the Atlantic). But wouldn’t it be great to see the dep$placename when we click over a symbol? Run the following: m &lt;- leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( lng=dep$longitude, lat=dep$latitude, popup=paste(dep$placename)) # include a popup with the placename! m Okay, but a map isn’t really useful until have have some satellite imagery, right? Easy: m &lt;- leaflet() %&gt;% addProviderTiles(providers$Esri.WorldImagery) %&gt;% #Add Esri Wrold imagery addCircleMarkers( lng=dep$longitude, lat=dep$latitude, popup=paste(dep$placename)) # include a popup with the placename! m Zoom in - what can you tell me about where the stations are located? Can you spot any differences between the stations? Clue: look for lines on the landscape. These lines are linear features related to oil and gas exploration, some cameras are deployed on them, others away from them. 5.4.1.2 Making corrections As you can see, we have one deployment location that is a long way from the others. It almost looks like it belongs to another project?! Let’s take a look at all of the deployments from that location (ALG069): dep[dep$placename==&quot;ALG069&quot;,c(&quot;deployment_id&quot;, &quot;placename&quot;, &quot;longitude&quot;, &quot;latitude&quot;)] ## deployment_id placename longitude latitude ## 100 ALG069_2018-04-07 ALG069 -113.5075 56.49352 ## 101 ALG069_2018-11-14 ALG069 -112.5075 56.49352 ## 102 ALG069_2019-04-02 ALG069 -112.5075 56.49352 It looks like there is a typo in one of the coordinates: -112.5075 -&gt; -113.5075. Let’s correct it: dep$longitude[dep$placename==&quot;ALG069&quot;] &lt;- -112.5075 5.4.1.3 The ultimate leaflet map We will need to check our correction has worked. Let’s also color camera locations based on their dep$feature_type, include them in the legend, and have their names show up when we click on them too! # First, set a single categorical variable of interest from station covariates for summary graphs. If you do not have an appropriate category use &quot;project_id&quot;. category &lt;- &quot;feature_type&quot; # We first convert this category to a factor with discrete levels dep[,category] &lt;- factor(dep[,category]) # then use the turbo() function to assign each level a color col.cat &lt;- turbo(length(levels(dep[,category]))) # then we apply it to the dataframe dep$colours &lt;- col.cat[dep[,category]] m &lt;- leaflet() %&gt;% addProviderTiles(providers$Esri.WorldImagery, group=&quot;Satellite&quot;) %&gt;% addTiles(group=&quot;Base&quot;) %&gt;% # Include a basemap option too addCircleMarkers(lng=dep$longitude, lat=dep$latitude, # Co lour the markers depending on the &#39;feature type&#39; color=dep$colours, # Add a popup of the placename and feature_type together popup=paste(dep$placename, dep[,category])) %&gt;% # Add a legend explaining what is going on addLegend(&quot;topleft&quot;, colors = col.cat, labels = levels(dep[,category]), title = category, labFormat = labelFormat(prefix = &quot;$&quot;), opacity = 1) %&gt;% # add a layer control box to toggle between the layers addLayersControl( baseGroups = c(&quot;Satellite&quot;, &quot;Base&quot;)) m If you click on a point you will see it’s corresponding placename and feature_type - so you can find the problem data. You can also check your treatment categories using the key. If you zoom in, all the “offline” locations should be &gt;100m away from a linear features, the other on top of them. For more examples of leaflet in R, see RStudio’s leaflet tutorial. Check the distance between camera pairs Sometimes the coordinates of a camera stations are accidentally repeated in the deployment data, which can actually be very hard to see on a map as the points will overlay perfectly. The way we check this is to calculate the pairwise distance between all of the unique deployments in the project. This helps us in two ways: we can find “cryptic” duplication events in the deployment coordinates this distance is often reported in manuscript method sections In the following code block we make first use of the simple features (sf) package - tools which make spatial operations which you would normally perform in ArcMap very easy (e.g. plotting and manipulating polygons). More on that later! # create a list of all the non-duplicated placenames camera_locs &lt;- dep %&gt;% dplyr::select(placename, latitude, longitude) %&gt;% unique() %&gt;% # remove duplicated rows (rows where the placename and coordinates match) st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = &quot;+proj=longlat&quot;) # Convert to `sf` format First lets check that none of the placenames are duplicated - this would suggest a placename with two sets of coordinates. If all is well, this should return an empty dataframe. # Check that there are no duplicated stations camera_locs[duplicated(camera_locs$placename)==T,] ## Simple feature collection with 0 features and 1 field ## Bounding box: xmin: NA ymin: NA xmax: NA ymax: NA ## Geodetic CRS: +proj=longlat ## [1] placename geometry ## &lt;0 rows&gt; (or 0-length row.names) If it returns a list of &lt;0 rows&gt;, we don’t have duplicates with different coordinates. Phew! Now let’s crunch the numbers.Paste and run the following: # distance matrix for all cameras camera_dist &lt;- st_distance(camera_locs) %&gt;% as.dist() %&gt;% usedist::dist_setNames(as.character(camera_locs$placename)) %&gt;% as.matrix() #Make temporary camera_dist_mins by converting diagonals/zeros to 999999 so we can avoid the zeros when using which.min function to find nearest cameras camera_dist_mins &lt;- camera_dist + diag(999999,dim(camera_dist)[1]) #Create new empty dataframe for appending results to camera_dist_list &lt;- data.frame(focal_cam = character(),nearest_cam = character(), dist = double()) #Cycle through each column of camera_dist_mins for (i in (1:dim(camera_dist_mins)[1])) { #Get index of minimum value of column i t &lt;- which.min(camera_dist_mins[,i]) #Combine relevant data into new_row new_row &lt;- data.frame(colnames(camera_dist_mins)[i],names(t),camera_dist_mins[t,i]) #Append the new_row to the accumulated results dataframe camera_dist_list[nrow(camera_dist_list) + 1,] = new_row } Lets summarize the output: summary(camera_dist_list$dist) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1103 1540 1978 2210 2578 5263 So the largest distance between two cameras is 5263m, the minimum is 1103m and on average it is 2210m. Again, put that straight into the methods section of your report/paper. 5.4.1.4 Do all images have a deployment associated with them? Another very useful check is to verify that all of the placenames have corresponding image data, and that all image data has corresponding deployment data! You would be surprised how often this is not the case! # check all check the placenames in images are represented in deployments # This code returns TRUE if it is and FALSE if it isn&#39;t. We can then summarize this with table() table(unique(img$placename) %in% unique(dep$placename)) ## ## TRUE ## 38 We have 38 TRUE’s, which means all the images have deployment data. Let’s check that all the placenames also have image data: # check all the placenames in deployments are represented in the images data table(unique(dep$placename) %in% unique(img$placename)) ## ## TRUE ## 38 Great. If you see any FALSE observations - you either have image data or deployments missing. Go back and check your raw data! 5.4.2 Camera activity checks The next step is to plot out the camera activity at each unique place name to see when our cameras are functioning. To make this plot we will need to use the plotly package for the first time. We use this because the plots are interactive, just like with leaflet we can zoom in and zoom out and find problem observations. It also dynamically changes the y-axis and x-axis labels to fit the data, which is very useful! Let’s experiment with some basic ‘plotly’ graphs to get warmed up: 5.4.2.1 Skill check: plotly If you are familiar with making plots in base R or ggplot, hopefully plotly is not too intimidating. Let us start with a basic scatter plot using the deployment data. library(plotly) fig &lt;- plot_ly(data = dep, # Specify your data frame x = ~longitude, y = ~latitude, # The x and y axis columns type=&quot;scatter&quot;, # and the type of plot mode=&quot;markers&quot;) # And the mode - other options = &quot;lines+markers&quot; or &quot;lines&quot; fig Like we said, this is very similar to conventional plotting tools. However, things get different when we start to specify the style of the points. In base R we might use pch= and cex= to change the style and size of the points, whereas with plotly we use the “marker” option, and include elements as a list. library(plotly) fig &lt;- plot_ly(data = dep, x = ~longitude, y = ~latitude, color=~feature_type, # We can specify color categories type=&quot;scatter&quot;, # and the type of plot mode=&quot;markers&quot;, # And the mode - other options = &quot;lines+markers&quot; or &quot;lines&quot; marker=list(size=15)) # the default size is 10 fig As ever- this just scratches the surface of the plotly package. See the Plotly graphing library for a wealth of options. 5.4.2.2 The ultimate plotly camera activity figure In the following plot, black dots denote start and end dates, lines denote periods where a camera is active. Each unique placename gets its own row on the plot - you can hover over the lines to get the deployment_id. We will use a loop to build the different elements… you don’t need to understand the code itself, just how to interpret the output. Cut and paste the following: # Call the plot p &lt;- plot_ly() # We want a separate row for each &#39;placename&#39; - so lets turn it into a factor dep$placename &lt;- as.factor(dep$placename) # loop through each place name for(i in seq_along(levels(dep$placename))) { #Subset the data to just that placename tmp &lt;- dep[dep$placename==levels(dep$placename)[i],] # Order by date tmp &lt;- tmp[order(tmp$start_date),] # Loop through each deployment at that placename for(j in 1:nrow(tmp)) { # Add a line to &#39;p&#39; p &lt;- add_trace(p, #Use the start and end date as x coordinates x = c(tmp$start_date[j], tmp$end_date[j]), #Use the counter for the y coordinates y = c(i,i), # State the type of chart type=&quot;scatter&quot;, # make a line that also has points mode = &quot;lines+markers&quot;, # Add the deployment ID as hover text hovertext=tmp$deployment_id[j], # Color it all black color=I(&quot;black&quot;), # Suppress the legend showlegend = FALSE) } } # Add a categorical y axis p &lt;- p %&gt;% layout(yaxis = list( ticktext = as.list(levels(dep$placename)), tickvals = as.list(1:length(levels(dep$placename))), tickmode = &quot;array&quot;)) p What do the gaps signify? The breaks in the line signify periods when the camera at a location was not active. You can see there was a point in 2018 when 9 out of 38 cameras had stopped working. Can you see any issues? Yes! Sometimes you will see a deployment a long way to the left or right of the plot, this is usually a date error (e.g. ALG036). 5.4.2.3 Corrections We checked the original datasheets for ALG036 and found out that the deployment end date for ALG036_2019-04-04 was incorrectly entered. It was written as 2020 not 2019. Let’s correct it: dep$end_date[dep$deployment_id==&quot;ALG036_2019-04-04&quot;] &lt;- ymd(&quot;2019-11-21&quot;) #remember to format it as a date object 5.4.3 Detection check Once we are happy that are cameras were functioning when we expected them to be, we now need to check if all of our labelled images fall within the associated deployment periods. To do this we build on the previous plot above, but also add in the image data over the top. This plot can get very messy, so we divide it into sections of ten deployments. As before, black lines show an active camera. Red dots show an image detections at that time. We only show the output of the first 10 deployments, but you should do this for every single deployment you have! Note - the code below is complex, you don’t have to understand it all unless you want to # Make a separate plot for each 20 stations For each 20 stations # To do this make a plot dataframe tmp &lt;- data.frame(&quot;deployment_id&quot;=unique(dep$deployment_id), &quot;plot_group&quot;=ceiling(1:length(unique(dep$deployment_id))/20)) dep_tmp &lt;- left_join(dep,tmp, by=&quot;deployment_id&quot;) for(i in 1:max(dep_tmp$plot_group)) { # Call the plot p &lt;- plot_ly() #Subset the data to just that placename tmp &lt;- dep_tmp[dep_tmp$plot_group==i,] # Order by placename tmp &lt;- tmp[order(tmp$placename),] # Loop through each deployment at that placename for(j in 1:nrow(tmp)) { #Subset the image data tmp_img &lt;- img[img$deployment_id==tmp$deployment_id[j],] if(nrow(tmp_img)&gt;0) { p &lt;- add_trace(p, #Use the start and end date as x coordinates x = c(tmp_img$timestamp), #Use the counter for the y coordinates y = rep(j, nrow(tmp_img)), # State the type of chart type=&quot;scatter&quot;, # make a line that also has points mode = &quot;markers&quot;, # Add the deployment ID as hover text hovertext=paste(tmp_img$genus,tmp_img$species), # Color it all black marker = list(color = &quot;red&quot;), # Suppress the legend showlegend = FALSE) } # Add a line to &#39;p&#39; p &lt;- add_trace(p, #Use the start and end date as x coordinates x = c(tmp$start_date[j], tmp$end_date[j]), #Use the counter for the y coordinates y = c(j,j), # State the type of chart type=&quot;scatter&quot;, # make a line that also has points mode = &quot;lines&quot;, # Add the deployment ID as hover text hovertext=tmp$deployment_id[j], # Color it all black color=I(&quot;black&quot;), # Suppress the legend showlegend = FALSE) } # Add custom y axis labels p &lt;- p %&gt;% layout(yaxis = list( ticktext = as.list(tmp$deployment_id), tickvals = as.list(1:nrow(tmp)), tickmode = &quot;array&quot;)) print(p) } What would a problem look like? If you have images (red dots) occurring outside a period of camera activity - that would indicate a miss-match between the deployment data and the image data. You would need to revisit your datasheets to see where this mismatch occurred. If the error is in the deployment dates - correct them as above! If the error is in the image metadata (i.e. camera was set to the wrong date), you have several options: If you are working in a platform like Wildlife Insights there is a date-time frameshift correction you can perform: see The correcting timestamps section You can correct the underlying exif data of the images using EXIF date changer Finally, you could change the dates in R using lubridate. If you look at the deployment ALG029_2019-04-02 you will see that has what has happened here. We checked the datasheets and realised that the camera’s timestamp was set to the incorrect month when the deployment began (2019-05-02 instead of 2019-04-02). # We set the wrong date for the camera collecting images in deployment #&quot;:&quot;ALG029_2019-04-02&quot; # We established that the deployment was 30 days out (as there are 30 days in April) # So we add 30 days to all of the images in that deployment. img[img$deployment_id==&quot;ALG029_2019-04-02&quot;,]$timestamp &lt;- img[img$deployment_id==&quot;ALG029_2019-04-02&quot;,]$timestamp - days(30) # Easy! You should repeat the plot above to check it has worked! 5.4.4 Taxonomy check Dealing with taxonomy in camera trap data sets can be a nightmare, particularly if your data labeling software does not give standardized lists of species (e.g. you are manually sorting images into folders). A species list is also something which is often produced for the appendix of a report or paper. Let us start with building a list of our taxonomic classifications: # First define vector of the headings you want to see (we will use this trick a lot later on) taxonomy_headings &lt;- c(&quot;class&quot;, &quot;order&quot;, &quot;family&quot;, &quot;genus&quot;, &quot;species&quot;, &quot;common_name&quot;) # Subset the image data to just those columns tmp&lt;- img[,colnames(img)%in% taxonomy_headings] # Remove duplicates tmp &lt;- tmp[duplicated(tmp)==F,] # Create an ordered species list sp_list &lt;- tmp[order(tmp$class, tmp$order, tmp$family, tmp$genus, tmp$species),] # Create a column to the species list with genus and species pasted together sp_list$sp &lt;- paste(sp_list$genus, sp_list$species, sep=&quot;.&quot;) # View the species list using kableExtra sp_list %&gt;% kbl(row.names=F) %&gt;% kable_styling(full_width = T) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;, height = &quot;250px&quot;) class order family genus species common_name sp Aves Galliformes Phasianidae Tympanuchus phasianellus NA Tympanuchus.phasianellus Aves Gruiformes Gruidae Grus canadensis NA Grus.canadensis Aves Passeriformes Corvidae Corvus corax NA Corvus.corax Aves Passeriformes Corvidae Perisoreus canadensis NA Perisoreus.canadensis Aves Strigiformes Strigidae Strix nebulosa NA Strix.nebulosa Mammalia Artiodactyla Cervidae Alces alces NA Alces.alces Mammalia Artiodactyla Cervidae Cervus canadensis NA Cervus.canadensis Mammalia Artiodactyla Cervidae Odocoileus virginianus NA Odocoileus.virginianus Mammalia Artiodactyla Cervidae Rangifer tarandus NA Rangifer.tarandus Mammalia Carnivora Canidae Canis latrans NA Canis.latrans Mammalia Carnivora Canidae Canis lupus NA Canis.lupus Mammalia Carnivora Canidae Vulpes vulpes NA Vulpes.vulpes Mammalia Carnivora Felidae Lynx canadensis NA Lynx.canadensis Mammalia Carnivora Mustelidae Lontra canadensis NA Lontra.canadensis Mammalia Carnivora Mustelidae Martes americana NA Martes.americana Mammalia Carnivora Ursidae Ursus americanus NA Ursus.americanus Mammalia Lagomorpha Leporidae Lepus americanus NA Lepus.americanus Mammalia Lagomorpha Leporidae Oryctolagus cuniculus NA Oryctolagus.cuniculus Mammalia Primates Hominidae Homo sapiens NA Homo.sapiens Mammalia Rodentia Sciuridae Tamiasciurus hudsonicus NA Tamiasciurus.hudsonicus NA NA NA blank NA .blank NA NA NA spp. NA .spp. NA NA NA Canachites canadensis NA Canachites.canadensis NA NA NA Unknown unknown NA Unknown.unknown NA NA NA Weasel spp. NA Weasel.spp. That is a lot of species classifications - are they all correct? If you are familiar with the species you may be able to know by eye, however if you are unfamiliar with them. You can also use a package called taxize. 5.4.5 Skill Check: Taxize package Lets start by seeing what taxize can do with a single species. Run the following: library(taxize) gnr_resolve(&quot;Lynx canadensis&quot;) For each hit in an online taxonomy database, you get a row in a dataframe and a confidence score in the identification. Lets try miss-spelling a common name: gnr_resolve(&quot;Lynx cramadensis&quot;) Cool! We can even recover incorrectly spelled names! What about common names? Well there is a way we can get those too using sci2comm()” sci2comm(&quot;Lynx canadensis&quot;) For more information see the Taxize documentation. In this instance, we have an external list of common names which we will use to update our img file. # Import the dataframe tmp &lt;- read.csv(&quot;data/raw_data/example_data/common_names.csv&quot;) # Join it with the existing species list sp_list$common_name &lt;- NULL sp_list &lt;- left_join(sp_list, tmp) Then let’s write our species list into our raw data folder: # Note we use the project_id from from project data frame to name the file - that was we wont overwrite it if we run things with a different project. write.csv(sp_list, paste0(&quot;data/raw_data/&quot;,pro$project_id[1],&quot;_raw_species_list.csv&quot;)) Then lets update the common_name column in our img dataframe to reflect the updated common names. We will do this using a left_join(), an operation which is invaluable when programming in R. It uses a specified “key” variable to merge two dataframes in this case we will use the ‘sp’ column. # first remove the common_name column img$common_name &lt;- NULL # add an sp column to the img dataframe - remember the genus and species columns are not pasted together yet img$sp &lt;- paste(img$genus, img$species, sep=&quot;.&quot;) # Next we do the &#39;left_join&#39; img &lt;- left_join(img, sp_list[, c(&quot;sp&quot;, &quot;common_name&quot;)], by=&quot;sp&quot;) Lets move on! 5.5 Diel activity check Sometimes when setting up a camera trap, you can input the time incorrectly. This is actually very hard to detect unless you happen to be looking for it. The way we check is to plot the detections for each species by the 24 hour clock. If were get detections of nocturnal species in the day, or vice versa, it suggests there may be a problem. Caveat A diurnal species active at night doesn’t mean there is actually a problem, camera traps have revealed that many animals are active when we thought they were not! Cool note Researchers are increasingly using this information to determine a species “availability” for detection! More on that in the density and activity chapters. For any species detected more than 10 times, we will plot when they were detected: # First lets convert our timestamp to decimal hours img$hours &lt;- hour(img$timestamp) + minute(img$timestamp)/60 + second(img$timestamp)/(60*60) # Count all of the captures tmp &lt;- img %&gt;% group_by(common_name) %&gt;% summarize(count=n()) yform &lt;- list(categoryorder = &quot;array&quot;, categoryarray = tmp$common_name) fig &lt;- plot_ly(x = img$hours, y = img$common_name,type=&quot;scatter&quot;, height=1000, text=img$deployment_id, hoverinfo=&#39;text&#39;, mode = &#39;markers&#39;, marker = list(size = 5, color = &#39;rgba(50, 100, 255, .2)&#39;, line = list(color = &#39;rgba(0, 0, 0, 0)&#39;, width = 0))) %&gt;% layout(yaxis = yform) fig # Remove the column img$hours &lt;- NULL Can you see any exclusively diurnal species? Sandhill crane would be a good candidate. They are very rarely detected at night. Can you see any exclusively nocturnal species? Snowshoe hare! More on activity data in the Activity chapter 5.6 Conclusion Congratulations - you have thoroughly error checked your camera data! We may find more errors in the data exploration chapter, so stay vigilant. "],["data-creation.html", "Chapter 6 Analysis data creation 6.1 Common analysis data formats 6.2 Independent detections 6.3 Effort look-up 6.4 Observations by time interval 6.5 Our data 6.6 Creating analysis dataframes", " Chapter 6 Analysis data creation 6.1 Common analysis data formats Although the types of analysis you can perform on camera trap data vary markedly, they often depend on three key dataframe structures. We introduce these structures here, then show you how to apply them in subsequent chapters. 6.2 Independent detections The independent detections dataframe is the work horse of the vast majority of camera trap analyses, it is from this that you build the rest of your data frames. The threshold we use for determining what is an “independent detection” is typically 30 minutes… because camera trappers are creatures of habit! If you want to dig a little deeper it to the why, there is a nice summary in Rahel Sollmans “A gentle introduction to camera‐trap data analysis”: Researchers have used different thresholds, typically 30 min (e.g., O’Brien, Kinnaird, &amp; Wibisono, 2003) to an hour (Bahaa‐el‐din et al., 2016); some researchers have argued that multiple pictures within the same day may not represent independent detections (Royle, Nichols, Karanth, &amp; Gopalaswamy, 2009). In most cases, this threshold is determined subjectively, based on the best available knowledge of the species under study. But it can also be determined based on the temporal autocorrelation (Kays &amp; Parsons, 2014) or analysis of time intervals (Yasuda, 2004) of subsequent pictures. Independent data has a single row for each independent event: 6.3 Effort look-up Image data without effort data is worthless! There are lots of instances where you need to know which stations were operating on a given day. Some people like to store this information in a site x date matrix, but they are actually not that easy to data wrangle with. A long data frame with a site and date column is the most flexible (and keeps the dates in their native POSIX formats). Effort lookups have a single row for ever day a given location has an active camera: 6.4 Observations by time interval We saved the most useful data format until last! A site, time interval, effort, and species detection dataframe integrates the independent data and daily lookup described above. You can use it to create detection rates, occupancy data frames and much more (see the subsequent chapters)! We export yearly, monthly, weekly and daily data frames from our single site exploration script - which should cover you for much of what you want to do. We include two different types of response terms: Observations = the number of independent detections per time interval Counts = sum of the independent minimum group sizes per time interval Example of an observation by time matrix: Let’s build these data frames from our example_data! 6.5 Our data First, lets create the folder to store our data! dir.create(&quot;data/processed_data&quot;) This section will follow the following steps: Filter to our target species Create a camera activity look-up Determine our “independent detections” Create our analysis data frames 6.5.1 Filter to target species # Remove observations without animals detected, where we don&#39;t know the species, and non-mammals img_sub &lt;- img %&gt;% filter(is_blank==0, # Remove the blanks is.na(img$species)==FALSE, # Remove classifications which don&#39;t have species class==&quot;Mammalia&quot;, # Subset to mammals species!=&quot;sapiens&quot;) # Subset to anything that isn&#39;t human This has resulted in the removal of 33.2% of the observations. Which are composed of the following species: img_sub %&gt;% group_by(common_name) %&gt;% summarize(n()) ## # A tibble: 14 × 2 ## common_name `n()` ## &lt;chr&gt; &lt;int&gt; ## 1 american marten 41 ## 2 black bear 1331 ## 3 canada lynx 140 ## 4 caribou 787 ## 5 coyote 21 ## 6 elk 6 ## 7 gray wolf 352 ## 8 moose 2038 ## 9 rabbit 9 ## 10 red fox 39 ## 11 red squirrel 34 ## 12 river otter 2 ## 13 snowshoe hare 629 ## 14 white-tailed deer 4790 6.5.2 Create a daily camera activity lookup Next we create the daily camera activity look up (remember, one row for every day a camera is active). # Remove any deployments without end dates tmp &lt;- dep[is.na(dep$end_date)==F,] # Create an empty list to store our days daily_lookup &lt;- list() # Loop through the deployment dataframe and create a row for every day the camera is active for(i in 1:nrow(tmp)) { if(ymd(tmp$start_date[i])!=ymd(tmp$end_date[i])) { daily_lookup[[i]] &lt;- data.frame(&quot;date&quot;=seq(ymd(tmp$start_date[i]), ymd(tmp$end_date[i]), by=&quot;days&quot;), &quot;placename&quot;=tmp$placename[i]) } } # Merge the lists into a dataframe row_lookup &lt;- bind_rows(daily_lookup) # Remove duplicates - when start and end days are the same for successive deployments row_lookup &lt;- row_lookup[duplicated(row_lookup)==F,] 6.5.3 Determine ‘independent’ camera detections We rarely analyse raw camera data, rather we filter out multiple detections of the same individual within a given event. This is called creating and “independent detections” dataframe. As stated above, it is wise to think about what you are analyzing and whether such a threshold is appropriate. For example, if your organism of interest is very abundant, for examples human hikers on a busy trail, then using a 30 minute threshold may mean that multiple independent groups of hikers are rolled into a single, huge, “event”. # Set the &quot;independence&quot; interval in minutes independent &lt;- 30 Finally we need to specify what a “count” means in this dataset. Some people do estimates of group_size in their footage - summing all of the individuals they are sure are different. Others only sum the animals they can see in each photo. Here is where you specify which to use: # Check for a `group_size` variable? table(img_sub$group_size) ## ## 1 2 3 4 5 6 ## 7923 1637 597 28 24 10 # Check for a &#39;number_ofobjects&#39; variable table(img_sub$number_of_objects) ## ## 1 2 3 ## 9772 412 35 Make your selection: # If yes use that, if no use &#39;number_of_objects&#39; img_sub$animal_count &lt;- img_sub$group_size We will now break down the algorithm into subsections to make it clear what is occurring: Order the dataframe by deployment code and species img_tmp &lt;- img_sub %&gt;% arrange(deployment_id) %&gt;% # Order by deployment_id group_by(deployment_id, sp) %&gt;% # Group species together mutate(duration = int_length(timestamp %--% lag(timestamp))) # Calculate the gap between successive detections Determine independence of images If subsequent detections occur outside of the independence threshold, assign it a unique ID code. library(stringr) # Give a random value to all cells img_tmp$event_id &lt;- 9999 # Create a counter counter &lt;- 1 # Make a unique code that has one more zero than rows in your dataframe num_code &lt;- as.numeric(paste0(nrow(img_sub),0)) # Loop through img_tmp - if gap is greater than the threshold -&gt; give it a new event ID for (i in 2:nrow(img_tmp)) { img_tmp$event_id[i-1] &lt;- paste0(&quot;E&quot;, str_pad(counter, nchar(num_code), pad = &quot;0&quot;)) if(is.na(img_tmp$duration[i]) | abs(img_tmp$duration[i]) &gt; (independent * 60)) { counter &lt;- counter + 1 } } # Update the information for the last row - the loop above always updates the previous row... leaving the last row unchanged # group ID for the last row if(img_tmp$duration[nrow(img_tmp)] &lt; (independent * 60)| is.na(img_tmp$duration[nrow(img_tmp)])){ img_tmp$event_id[nrow(img_tmp)] &lt;- img_tmp$event_id[nrow(img_tmp)-1] } else{ counter &lt;- counter + 1 img_tmp$event_id[nrow(img_tmp)] &lt;- paste0(&quot;E&quot;, str_pad(counter, nchar(num_code), pad = &quot;0&quot;)) } # remove the duration column img_tmp$duration &lt;- NULL 6.5.4 Add additional data We could stop there, however there is other information we might light to extract about each individual event: the maximum number objects detected in an event how long the event lasts how many images are in each event # find out the last and the first of the time in the group top &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% top_n(1,timestamp) %&gt;% dplyr::select(event_id, timestamp) bot &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% top_n(-1,timestamp) %&gt;% dplyr::select(event_id, timestamp) names(bot)[2] &lt;- c(&quot;timestamp_end&quot;) img_num &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% summarise(event_observations=n()) # number of images in the event event_grp &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% summarise(event_groupsize=max(animal_count)) # calculate the duration and add the other elements diff &lt;- top %&gt;% left_join(bot, by=&quot;event_id&quot;) %&gt;% mutate(event_duration=abs(int_length(timestamp %--% timestamp_end))) %&gt;% left_join(event_grp, by=&quot;event_id&quot;)%&gt;% left_join(img_num, by=&quot;event_id&quot;) # Remove columns you don&#39;t need diff$timestamp &lt;-NULL diff$timestamp_end &lt;-NULL # remove duplicates diff &lt;- diff[duplicated(diff)==F,] # Merge the img_tmp with the event data img_tmp &lt;- img_tmp %&gt;% left_join(diff,by=&quot;event_id&quot;) Finally lets subset to the first row of each event to create our independent dataframe! # Remove duplicates ind_dat &lt;- img_tmp[duplicated(img_tmp$event_id)==F,] Next we remove any detections which occur outside of our known camera activity periods: # Make a unique code for ever day and deployment where cameras were functioning tmp &lt;- paste(row_lookup$date, row_lookup$placename) #Subset ind_dat to data that matches the unique codes ind_dat &lt;- ind_dat[paste(substr(ind_dat$timestamp,1,10), ind_dat$placename) %in% tmp, ] As a final step, we make the species column a ‘factor’ - this makes all the data frame building operations much simpler: ind_dat$sp &lt;- as.factor(ind_dat$sp) And we are ready to build our dataframes! 6.6 Creating analysis dataframes Finally, this script outputs 11 useful data frames for future data analysis: 1. A data frame of “independent detections” at the 30 minute threshold you specified at the start: “data/processed_data/AlgarRestorationProject_30min_Independent.csv” write.csv(ind_dat, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_detections.csv&quot;), row.names = F) # also write the cleaned all detections file (some activity analyses require it) write.csv(img_tmp, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_raw_detections.csv&quot;), row.names = F) 2. The “daily_lookup” which is a dataframe of all days a given camera station was active. Some people use an lookup matrix for this step, but we find the long format is much easier to use in downstream analysis. - “data/processed_data/_daily_deport_lookup.csv” write.csv(row_lookup, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_daily_lookup.csv&quot;), row.names = F) 3. Unique camera locations list: When we start to build the covariates for data analysis, it is very useful to have a list of your final project’s camera locations. We create this below in a simplified form. You can include any columns which will be use for data analysis, and export it. #Subset the columns tmp &lt;- dep[, c(&quot;project_id&quot;, &quot;placename&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;feature_type&quot;)] # Remove duplicated rows tmp&lt;- tmp[duplicated(tmp)==F,] # write the file write.csv(tmp, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_camera_locations.csv&quot;), row.names = F) 4. Final species list We also want to create a final species list. We subset the data to just those included in the independent data, and then save the file. tmp &lt;- sp_list[sp_list$sp %in% ind_dat$sp,] # Remove the &#39;verified&#39; column tmp$verified &lt;- NULL # We will replace the spaces in the species names with dots, this will make things easier for us later (as column headings with spaces in are annoying). library(stringr) tmp$sp &lt;- str_replace(tmp$sp, &quot; &quot;, &quot;.&quot;) write.csv(tmp, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_species_list.csv&quot;), row.names = F) 5 &amp; 6: A ‘site x species’ matrix of the number of independent detections and species counts across the full study period: “data/processed_data/AlgarRestorationProject_30min_Independent_total_observations.csv” “data/processed_data/AlgarRestorationProject_30min_Independent_total_counts.csv” # Total counts # Station / Month / deport / Species tmp &lt;- row_lookup # Calculate the number of days at each site total_obs &lt;- tmp %&gt;% group_by(placename) %&gt;% summarise(days = n()) # Convert to a data frame total_obs &lt;- as.data.frame(total_obs) # Add columns for each species total_obs[, levels(ind_dat$sp)] &lt;- NA # Duplicate for counts total_count &lt;- total_obs # Test counter i &lt;-1 # For each station, count the number of individuals/observations for(i in 1:nrow(total_obs)) { tmp &lt;- ind_dat[ind_dat$placename==total_obs$placename[i],] tmp_stats &lt;- tmp %&gt;% group_by(sp, .drop=F) %&gt;% summarise(obs=n(), count=sum(animal_count)) total_obs[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$obs total_count[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$count } # Save them write.csv(total_obs, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_total_observations.csv&quot;), row.names = F) write.csv(total_count, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_total_counts.csv&quot;), row.names = F) 7 &amp; 8: A ‘site_month x species’ matrix of the number of independent detections and species counts across for each month in the study period: “data/processed_data/AlgarRestorationProject_30min_Monthly_total_observations.csv” “data/processed_data/AlgarRestorationProject_30min_Monthly_total_counts.csv” # Monthly counts # Station / Month / days / Covariates / Species tmp &lt;- row_lookup # Simplify the date to monthly tmp$date &lt;- substr(tmp$date,1,7) # Calculate the number of days in each month mon_obs &lt;- tmp %&gt;% group_by(placename,date ) %&gt;% summarise(days = n()) # Convert to a data frame mon_obs &lt;- as.data.frame(mon_obs) mon_obs[, levels(ind_dat$sp)] &lt;- NA mon_count &lt;- mon_obs # For each month, count the number of individuals/observations for(i in 1:nrow(mon_obs)) { tmp &lt;- ind_dat[ind_dat$placename==mon_obs$placename[i] &amp; substr(ind_dat$timestamp,1,7)== mon_obs$date[i],] tmp_stats &lt;- tmp %&gt;% group_by(sp, .drop=F) %&gt;% summarise(obs=n(), count=sum(animal_count)) mon_obs[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$obs mon_count[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$count } write.csv(mon_obs, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_monthly_observations.csv&quot;), row.names = F) write.csv(mon_count, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_monthly_counts.csv&quot;), row.names = F) 9 &amp; 10: A ‘site_week x species’ matrix of the number of independent detections and species counts across for each week in the study period: “data/processed_data/AlgarRestorationProject_30min_Weekly_total_observations.csv” “data/processed_data/AlgarRestorationProject_30min_Weekly_total_counts.csv” # Weekly format # Station / Month / days / Covariates / Species tmp &lt;- row_lookup # Simplify the date to year-week tmp$date &lt;- strftime(tmp$date, format = &quot;%Y-W%U&quot;) # The way this is coded is the counter W01 starts at the first Sunday of the year, everything before that is W00. Weeks do not roll across years. # Calculate the number of days in each week week_obs &lt;- tmp %&gt;% group_by(placename,date ) %&gt;% summarise(days = n()) # Convert to a data frame week_obs &lt;- as.data.frame(week_obs) # Add species columns week_obs[, levels(ind_dat$sp)] &lt;- NA # Duplicate for counts week_count &lt;- week_obs # For each week, count the number of individuals/observations for(i in 1:nrow(week_obs)) { tmp &lt;- ind_dat[ind_dat$placename==week_obs$placename[i] &amp; strftime(ind_dat$timestamp, format = &quot;%Y-W%U&quot;)== week_obs$date[i],] tmp_stats &lt;- tmp %&gt;% group_by(sp, .drop=F) %&gt;% summarise(obs=n(), count=sum(animal_count)) week_obs[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$obs week_count[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$count } write.csv(week_obs, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_weekly_observations.csv&quot;), row.names = F) write.csv(week_count, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_weekly_counts.csv&quot;), row.names = F) 11 &amp; 12: A ‘site_day x species’ matrix of the number of independent detections and species counts across for each day a station was active in the study period: “data/processed_data/AlgarRestorationProject_30min_Daily_total_observations.csv” “data/processed_data/AlgarRestorationProject_30min_Daily_total_counts.csv” # Daily format # Station / Month / days / Covariates / Species tmp &lt;- row_lookup tmp$days &lt;- 1 # Add species columns tmp[, levels(ind_dat$sp)] &lt;- NA day_obs &lt;- tmp day_count &lt;- tmp # For each week, count the number of individuals/observations for(i in 1:nrow(day_obs)) { tmp &lt;- ind_dat[ind_dat$placename==day_obs$placename[i] &amp; strftime(ind_dat$timestamp, format = &quot;%Y-%m-%d&quot;)== day_obs$date[i],] tmp_stats &lt;- tmp %&gt;% group_by(sp, .drop=F) %&gt;% summarise(obs=n(), count=sum(animal_count)) day_obs[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$obs day_count[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$count } write.csv(day_obs, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_daily_observations.csv&quot;), row.names = F) write.csv(day_count, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_daily_counts.csv&quot;), row.names = F) 6.6.1 Final data check Finally, as a last check that our code is creating robust analysis data frames, we check if the observations/counts are the same across each temporal scale (total/monthly/weekly/daily). Check this using the following tables. Observations tmp &lt;- cbind(data.frame(&quot;Time&quot;=c(&quot;Total&quot;, &quot;Monthly&quot;, &quot;Weekly&quot;, &quot;Daily&quot;)), rbind(colSums(total_obs[,2:ncol(total_obs)]), colSums(mon_obs[,3:ncol(mon_obs)]), colSums(week_obs[,3:ncol(week_obs)]), colSums(day_obs[,3:ncol(day_obs)]) )) tmp %&gt;% kbl() %&gt;% kable_styling(full_width = T) %&gt;% column_spec(1, bold = T, border_right = T)%&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Time days Alces.alces Canis.latrans Canis.lupus Cervus.canadensis Lepus.americanus Lontra.canadensis Lynx.canadensis Martes.americana Odocoileus.virginianus Oryctolagus.cuniculus Rangifer.tarandus Tamiasciurus.hudsonicus Ursus.americanus Vulpes.vulpes Total 20350 176 8 93 1 241 1 52 24 562 2 119 26 228 10 Monthly 20350 176 8 93 1 241 1 52 24 562 2 119 26 228 10 Weekly 20350 176 8 93 1 241 1 52 24 562 2 119 26 228 10 Daily 20350 176 8 93 1 241 1 52 24 562 2 119 26 228 10 Counts tmp &lt;- cbind(data.frame(&quot;Time&quot;=c(&quot;Total&quot;, &quot;Monthly&quot;, &quot;Weekly&quot;, &quot;Daily&quot;)), rbind(colSums(total_count[,2:ncol(total_count)]), colSums(mon_count[,3:ncol(mon_count)]), colSums(week_count[,3:ncol(week_count)]), colSums(day_count[,3:ncol(day_count)]) )) tmp %&gt;% kbl() %&gt;% kable_styling(full_width = T) %&gt;% column_spec(1, bold = T, border_right = T)%&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Time days Alces.alces Canis.latrans Canis.lupus Cervus.canadensis Lepus.americanus Lontra.canadensis Lynx.canadensis Martes.americana Odocoileus.virginianus Oryctolagus.cuniculus Rangifer.tarandus Tamiasciurus.hudsonicus Ursus.americanus Vulpes.vulpes Total 20350 227 8 133 1 244 1 53 24 678 2 159 26 256 10 Monthly 20350 227 8 133 1 244 1 53 24 678 2 159 26 256 10 Weekly 20350 227 8 133 1 244 1 53 24 678 2 159 26 256 10 Daily 20350 227 8 133 1 244 1 53 24 678 2 159 26 256 10 "],["covariates.html", "Chapter 7 Analysis covariates 7.1 Species traits 7.2 Camera station covariates 7.3 Convert and save your covariates 7.4 Correlations between predictors", " Chapter 7 Analysis covariates Once we have created the building blocks for our data analysis dataframes, we must bring in the variables which will be used in the modelling steps. It is important to not that there are millions of ways to add covariates - both in terms of how you do it, and where you derive the data from. The covariates you use will depend on the questions you have, and the context of your survey. The examples provided here are not comprehensive and serve only as a guide! Create a new .R script Call it 02_example_covariates.R. Load the required packages library(kableExtra);library(dplyr); library(sf); library(modisfast); library(lubridate); library(corrplot); library(traitdata); library(terra); library(osmdata); library(elevatr) We can simplify the covariate options we have available into two distinct categories: - Species traits The traits are species-level covariates which we think are important in structuring their responses to other covariates, such as human modification. # Start by reading in your species list sp_summary &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_species_list.csv&quot;, header=T) - Location-level covariates Location-level covariates are characteristics of the camera locations which are either fundamental to your question (such as the habitat type, degree of human modification, or distance to the nearest road), or they are things you are not directly interested in but must account for in your analyses. The way we derive and treat these variables are identical however. locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations.csv&quot;, header=T) 7.1 Species traits It is easier than ever before to add trait data to your species lists, particularly with the advent of R packages which pool multiple data sources such as the traitdata database, which to date, compiles data from 32 different sources. Below we use this package to add trait data to the project species list: # This package isn&#39;t available on Cran, so we must use the remotes package #library(remotes) #remotes::install_github(&quot;RS-eco/traitdata&quot;, build_vignettes = T, force=T) # Load the library library(traitdata) To pull the data for a specific database we use the following code: data(&quot;elton_mammals&quot;) To explore the full list of available datasets click this link. Let’s take a look at what categories we have available to us: head(elton_mammals) %&gt;% kbl() %&gt;% scroll_box(height = &quot;200px&quot;) %&gt;% kable_paper(&quot;striped&quot;, full_width = F) MSW3_ID Genus Species Family Diet.Inv Diet.Vend Diet.Vect Diet.Vfish Diet.Vunk Diet.Scav Diet.Fruit Diet.Nect Diet.Seed Diet.PlantO Diet.Source Diet.Certainty ForStrat.Value ForStrat.Certainty ForStrat.Comment Activity.Nocturnal Activity.Crepuscular Activity.Diurnal Activity.Source Activity.Certainty BodyMass.Value BodyMass.Source BodyMass.SpecLevel Full.Reference scientificNameStd 1 Tachyglossus aculeatus Tachyglossidae 100 0 0 0 0 0 0 0 0 0 Ref_1 ABC G A 1 1 0 Ref_1 ABC 3025.00 Ref_117 1 Nowak R.M. (1999). Walker’s mammals of the world. Sixth edition edn. The Johns Hopkins University Press, Baltimore, Maryland Tachyglossus aculeatus 2 Zaglossus attenboroughi Tachyglossidae 100 0 0 0 0 0 0 0 0 0 Ref_65 ABC G A 1 0 0 Ref_1 ABC 8532.39 Ref_2, Ref_3 0 Leary, T., Seri, L., Flannery, T., Wright, D., Hamilton, S., Helgen, K., Singadan, R., Menzies, J., Allison, A., James, R., Aplin, K., Salas, L. &amp; Dickman, C. 2008.�Zaglossus attenboroughi. In: IUCN 2010. IUCN Red List of Threatened Species. Version 2010. Zaglossus attenboroughi 3 Zaglossus bartoni Tachyglossidae 100 0 0 0 0 0 0 0 0 0 Ref_2 D1 G A 1 0 0 Ref_1 ABC 7180.00 Ref_131 1 Genus Average Zaglossus bartoni 4 Zaglossus bruijni Tachyglossidae 100 0 0 0 0 0 0 0 0 0 Ref_1 ABC G A 1 0 0 Ref_1 ABC 10139.50 Ref_117 1 Nowak R.M. (1999). Walker’s mammals of the world. Sixth edition edn. The Johns Hopkins University Press, Baltimore, Maryland Zaglossus bruijni 5 Ornithorhynchus anatinus Ornithorhynchidae 80 0 0 20 0 0 0 0 0 0 Ref_1 ABC G A 1 1 1 Ref_1 ABC 1484.25 Ref_117 1 Nowak R.M. (1999). Walker’s mammals of the world. Sixth edition edn. The Johns Hopkins University Press, Baltimore, Maryland Ornithorhynchus anatinus 6 Caluromys philander Didelphidae 20 0 0 0 10 0 20 0 10 40 Ref_1 ABC Ar A 1 1 0 Ref_1 ABC 229.25 Ref_117 1 Nowak R.M. (1999). Walker’s mammals of the world. Sixth edition edn. The Johns Hopkins University Press, Baltimore, Maryland Caluromys philander Lets make a new column sp which matches the species column in our ‘sp_summary’ dataset. We will use this as the “key” variable to extract the trait data. elton_mammals$sp &lt;- paste0(elton_mammals$Genus,&quot;.&quot; ,elton_mammals$Species) We do not want to take all of the trait data, so lets subset to BodyMass.Value and the activity data Activity.Nocturnal Activity.Crepuscular Activity.Diurnal. tmp &lt;- elton_mammals[, c(&quot;sp&quot;,&quot;BodyMass.Value&quot;, &quot;Activity.Nocturnal&quot;, &quot;Activity.Crepuscular&quot;, &quot;Activity.Diurnal&quot;)] # Lets rename the columns to make them more usable tmp &lt;- tmp %&gt;% rename( mass_g = BodyMass.Value, act_noct = Activity.Nocturnal, act_crep = Activity.Crepuscular, act_diur = Activity.Diurnal) sp_summary &lt;- left_join(sp_summary, tmp) And then check our output: sp_summary %&gt;% kbl() %&gt;% scroll_box(height = &quot;200px&quot;) %&gt;% kable_paper(&quot;striped&quot;, full_width = F) class order family genus species sp common_name mass_g act_noct act_crep act_diur Mammalia Artiodactyla Cervidae Alces alces Alces.alces moose 356998.16 1 1 0 Mammalia Artiodactyla Cervidae Cervus canadensis Cervus.canadensis elk NA NA NA NA Mammalia Artiodactyla Cervidae Odocoileus virginianus Odocoileus.virginianus white-tailed deer 55508.56 1 1 0 Mammalia Artiodactyla Cervidae Rangifer tarandus Rangifer.tarandus caribou 86033.98 0 0 1 Mammalia Carnivora Canidae Canis latrans Canis.latrans coyote 13406.33 1 1 0 Mammalia Carnivora Canidae Canis lupus Canis.lupus gray wolf 32183.33 1 1 0 Mammalia Carnivora Canidae Vulpes vulpes Vulpes.vulpes red fox 5476.17 1 1 0 Mammalia Carnivora Felidae Lynx canadensis Lynx.canadensis canada lynx 9373.25 1 0 0 Mammalia Carnivora Mustelidae Lontra canadensis Lontra.canadensis river otter 8087.42 1 1 0 Mammalia Carnivora Mustelidae Martes americana Martes.americana american marten 1250.00 1 0 0 Mammalia Carnivora Ursidae Ursus americanus Ursus.americanus black bear 99949.36 1 0 0 Mammalia Lagomorpha Leporidae Lepus americanus Lepus.americanus snowshoe hare 1710.02 1 0 0 Mammalia Lagomorpha Leporidae Oryctolagus cuniculus Oryctolagus.cuniculus rabbit 1832.22 1 0 0 Mammalia Rodentia Sciuridae Tamiasciurus hudsonicus Tamiasciurus.hudsonicus red squirrel 201.17 0 0 1 If there are any NA’s, it could be for several reasons: There is no trait data for that species - in this case you could either: leave them as NA’s (excluding them from later analyses) or if you are lucky, your analysis framework might be able to accommodate missing trait data Give the species the mean values obtained from other species in its genus There is a mismatch in taxonomic resolution - you are working with a subspecies that isn’t recognized. This is the case here with elk! Lets replace it with the data for (Cervus elaphus) sp_summary[sp_summary$sp==&quot;Cervus.canadensis&quot;, c(&quot;mass_g&quot;, &quot;act_noct&quot;,&quot;act_crep&quot;,&quot;act_diur&quot;)] &lt;- elton_mammals[elton_mammals$sp==&quot;Cervus.elaphus&quot;, c(&quot;BodyMass.Value&quot;, &quot;Activity.Nocturnal&quot;, &quot;Activity.Crepuscular&quot;, &quot;Activity.Diurnal&quot;)] Whatever you do, remember to report it in your methods section! Let’s save our species list for a rainy day! write.csv(sp_summary, paste0(&quot;data/processed_data/&quot;, locs$project_id[1],&quot;_species_list.csv&quot;), row.names = F) 7.2 Camera station covariates It is common to have a suite of covariates which you would like to investigate the effects of in your datasets. These could take the form of habitat designations or treatment types. These may already be included with your deployment data, or you may need to derive them from a variety of remote sources. In their simplest form, these variable are time invariant (they do not change), however you may have variables which change through time as well (we discuss these at the end). In the following steps, we walk through the process of manipulating and deriving example covariates. For the time invariant covariates, we will add them to our locs dataframe imported above. 7.2.1 Locally collected covariates You may have collected some data in the field when deploying or checking your camera traps, and kept that data separate from your camera trap data (e.g. vegetation assessments). Provided that the naming convention you gave to these dataframes is the same as in your camera data (e.g. the location is in a column called placename) - you can do a ’left_join()` to merge the two datasets. Import a sample set of local covariates: local_covs &lt;- read.csv(&quot;data/raw_data/example_covariates/example_dataframe.csv&quot;) Lets take a look at the data structure: placename line_of_sight_m ALG001 137.12500 ALG002 131.52778 ALG003 353.65833 ALG004 158.04167 ALG005 305.81944 ALG006 60.12500 ALG007 310.58333 ALG008 112.75000 ALG009 299.02778 ALG010 102.30556 ALG011 223.56944 ALG012 140.91667 ALG013 394.56944 ALG014 196.87500 ALG015 163.11111 ALG016 116.11111 ALG017 138.19444 ALG018 304.29167 ALG019 330.97222 ALG020 204.40278 ALG021 264.94444 ALG022 229.13889 ALG023 218.29167 ALG024 425.43056 ALG025 56.97222 ALG026 200.05556 ALG027 252.95833 ALG028 277.50000 ALG029 206.52778 ALG030 43.38889 ALG031 334.27778 ALG032 83.00000 ALG033 165.00000 ALG034 337.79167 ALG035 439.61111 ALG036 62.69444 ALG037 392.61111 ALG038 352.75000 ALG039 339.76389 ALG040 182.75000 ALG041 109.25000 ALG042 219.56944 ALG043 62.41667 ALG044 374.26389 ALG045 294.83333 ALG046 34.50000 ALG047 363.80556 ALG048 392.93056 ALG049 80.77778 ALG050 139.36111 ALG051 208.12500 ALG052 13.95833 ALG053 99.30556 ALG054 35.47222 ALG055 189.06944 ALG056 55.41667 ALG057 93.04167 ALG058 80.91667 ALG059 41.00000 ALG060 189.90278 ALG061 11.16667 ALG062 16.00000 ALG063 28.94444 ALG064 34.27778 ALG065 20.11111 ALG066 36.94444 ALG067 188.83333 ALG068 72.27778 ALG069 22.85556 ALG070 28.61111 ALG071 20.05556 ALG072 52.44444 ALG073 55.94444 It is a dataframe where the survey locations are rows and the local covariates, in this case line_of_sight_m, are columns. To add this data to our station data, we use a left_join() operation from the dplyr() package. It uses a key variable which is common in both data frames to add data from the “right-hand side” to the rows in the “left-hand side” which are not already present. Any rows present in the right-hand side which are not in the left-hand side will be skipped. locs &lt;- left_join(locs, local_covs) # From the dplyr package For more examples of joins using dplyr() see: https://dplyr.tidyverse.org/reference/mutate-joins.html 7.2.2 Remotely collected covariates To exploit remotely collected data sources we need to use a package to help us with spatial data. 7.2.2.1 Key skills: sf package The most intuitive package to learn spatial operations in R is the simple features package (a.k.a. sf). sf allows you to use spatial dataframes in the style of a typical R dataframe. We use this package frequently as it allows you to rapidly change coordinate projection systems (e.g. lat/long to UTM) and rapidly perform spatial operations. Lets convert our “normal” dataframe to an sf dataframe: locs_sf &lt;- st_as_sf(locs, # We specify the dataframe coords=c(&quot;longitude&quot;, &quot;latitude&quot;), # The XY coordinates crs=4326) # And the projection code What does an sf object look like? Like a normal dataframe but with a weird header: locs_sf ## Simple feature collection with 38 features and 4 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -112.6467 ymin: 56.15983 xmax: -112.3848 ymax: 56.49352 ## Geodetic CRS: WGS 84 ## First 10 features: ## project_id placename feature_type line_of_sight_m ## 1 AlgarRestorationProject ALG027 HumanUse 252.95833 ## 2 AlgarRestorationProject ALG029 HumanUse 206.52778 ## 3 AlgarRestorationProject ALG031 HumanUse 334.27778 ## 4 AlgarRestorationProject ALG032 HumanUse 83.00000 ## 5 AlgarRestorationProject ALG035 HumanUse 439.61111 ## 6 AlgarRestorationProject ALG036 NatRegen 62.69444 ## 7 AlgarRestorationProject ALG037 HumanUse 392.61111 ## 8 AlgarRestorationProject ALG038 HumanUse 352.75000 ## 9 AlgarRestorationProject ALG039 HumanUse 339.76389 ## 10 AlgarRestorationProject ALG043 NatRegen 62.41667 ## geometry ## 1 POINT (-112.4735 56.3328) ## 2 POINT (-112.5483 56.39474) ## 3 POINT (-112.482 56.30899) ## 4 POINT (-112.3968 56.40197) ## 5 POINT (-112.4761 56.38428) ## 6 POINT (-112.4058 56.23178) ## 7 POINT (-112.4449 56.27898) ## 8 POINT (-112.4792 56.27039) ## 9 POINT (-112.4094 56.30127) ## 10 POINT (-112.5842 56.38715) That header is important - it tells you the type of data you have (lines, points, polygons etc), and the projection information (CRS). We like using sf as it is very easy to transform coordinates into different projections using st_transform(). Use the website epsg.io to find the CRS codes for the projection you want - e.g. UTM 12N = 26712, then plug it into the following: locs_utm &lt;- st_transform(locs_sf, crs=26712) Check the header of locs_utm and you should see that the CRS has changed! Plotting sf objects is a little bit odd at first. If you try to plot them normally you get lots of replicated plots (one for each column) - try it: plot(locs_utm) It can be useful as it varied the colors based on the properties of the column. Typically, however, we just want to plot the points themselves. We do that by wrapping the object in st_geometry() this just extracts the geometry of the object. plot(st_geometry(locs_utm)) axis(1) axis(2) We will use st_geometry() frequently below. For more in depth information of sf functionality see: https://r-spatial.github.io/sf/articles/sf1.html 7.2.3 Extracting data from local rasters Often we have raster data layers stored which we would like to link to our camera locations. We have included one such example here, a raster which reflects the depth from the soil surface to the water table - a proxy for habitat type in this study site. The layer comes from the 1m Wet Area Mapping (WAM) layer: White, Barry, et al. “Using the cartographic depth-to-water index to locate small streams and associated wet areas across landscapes.” Canadian Water Resources Journal 37.4 (2012): 333-347. NOTE the raster has been down scaled to reduce its size for this course - it is no longer at 1m resolution. The only time we deviate from the sf package is to deal with rasters. Raster objects in R are processed really slowly, especially if the raster is large. So instead we use the terra package. library(terra) # Import the example raster using the stars package ras &lt;- rast(&quot;data/raw_data/example_covariates/example_raster.tif&quot;) # Covert your sf locations to the same projection as your raster then put it in terra `vect` format, locs_terra &lt;- locs_sf %&gt;% st_transform(crs=st_crs(ras)) %&gt;% # change the projection to match the raster vect() # Turn it into a terra object Lets check our layers match up! plot(ras) # The terra package makes nice raster plots with legends plot(locs_terra, add=T) # Add the survey locations as black dots Great! Now lets buffer our camera locations by 250 meters, and take the average depth to water for each location: # Buffer by 250m locs_terra &lt;- buffer(locs_terra,250) # Extract the values to a temporary object - tmp tmp &lt;- terra::extract(ras, locs_terra, fun=mean) # Make a new column in locs_sf called water_depth_m # They are ordered the same way so no need for a fancy join locs_sf$water_depth_m &lt;- tmp$Depth2WatAlgar Finally, lets check the distribution of our data! # Basic boxplot in base R boxplot(locs_sf$water_depth_m) Most locations are on the water table (lowland sites), others are above it (upload sites), and they have different vegetation characteristics in the field. 7.2.4 elevatr package Camera studies often occur over a range of elevations - and we can quickly extract these elevations using the elevatr package and an sf dataframe. There are two options, epqsONLY AVAILABLE FOR NORTH AMERICA: library(elevatr) locs_sf &lt;- get_elev_point(locs_sf, src=&quot;epqs&quot;) # Can be slow if you have &gt;500 camera points Or Amazon Web Service Terrain Tiles… available globally! locs_sf &lt;- get_elev_point(locs_sf, src=&quot;aws&quot;, #Amazon Web Service Terrain Tiles - available globally z = 12) # z specifies the zoom level -&gt; the lower the value the faster the code runs, but the coarser the elevation values are The src option specifies the sources of the DEM data. We use aws Amazon Web Service Terrain Tiles - which are available globally. The z option specifies the resolution of the underlying DEM, the high the value, the more detailed it is. However, it will take longer to run so do not go crazy. Regardless of which method you use, let’s plot the output: boxplot(locs_sf$elevation) An elevation of ~ 500m was expected. Great! If you want to download a full elevation raster for your area of interests, see the introduction to elevatr 7.2.5 Open Street Maps Open Street Map (OSM) is an incredible resource for generating covariates for camera trap studies. For example, we might be interested in the distance to the nearest rivers, roads, or trails. All of these anthropogenic features are available in OSM! CAREFUL OSM data is user contributed and often incomplete and patchy. Always plot your data and never assume it is complete without checking it first. For an example fo this see water bodies below. First lets load the osmdata package. library(osmdata) The types of features we can extract using the osmdata package are listed here: https://wiki.openstreetmap.org/wiki/Map_features. 7.2.5.1 Highways Camera trap projects are often interested in human disturbance, of which, highways are an important part. Let’s start by defining our area of interest. All osmdata queries begin with a bounding box defining the area of the query: # First buffer our points by 10km to create an area of interest (aoi) aoi &lt;- st_bbox(st_buffer(locs_sf, 10000)) # Units are in meters We then use this bounding box to return all of the features which cross into it: highway &lt;- opq(aoi) %&gt;% #using the bounding box add_osm_feature(key=&quot;highway&quot;) %&gt;% #extract all highway features osmdata_sf() # convert them into simple features format The data you extract is its own “class” of data made up from multiple data types: str(highway) Which looks very intimidating! However, the key thing is that it is made up of multiple data slices, each of which represents an sf dataset. Let’s take a look at three of these $osm_points $osm_lines $osm_polygons par(mfrow=c(1,3)) plot(st_geometry(highway$osm_points), main=&quot;osm_points&quot;) plot(st_geometry(highway$osm_lines), main=&quot;osm_lines&quot;) plot(st_geometry(highway$osm_polygons), main=&quot;osm_polygons&quot;) The points or the lines datasets look must useful to us, there is nothing in the polygon layer. Let’s use the lines element and add out camera stations: par(mfrow=c(1,1)) plot(st_as_sfc(aoi)) # st_as_sfc created a polygon from a `bbox` object plot(st_geometry(highway$osm_lines), add=T) plot(st_geometry(locs_sf), col=&quot;red&quot;, add=T) We can now calculate the distances from our cameras to these objects using the following codes: st_nearest_feature gives us the index number of the feature which is closest to each station. We can the use this to request the distance from that nearest feature to each camera station using st_distance. Which, put together, looks like: # Create an index of the nearest object in `highway$osm_lines` to locs_sf index &lt;- st_nearest_feature(locs_sf, highway$osm_lines) # Use that index to ask for the distance to that object locs_sf$road_dist_m &lt;- st_distance(locs_sf, highway$osm_lines[index,], by_element=T) # Note `by_element=T` tells st_distance to evaluate things line by line. 7.2.5.2 water bodies We also might want to calculate the distances to the nearest water body, and important resource for wildlife. We can do that using the following: water &lt;- opq(aoi) %&gt;% add_osm_feature(key = &#39;natural&#39;, value = &#39;water&#39;) %&gt;% osmdata_sf() Lets check our data: par(mfrow=c(1,2)) plot(st_geometry(water$osm_points), main=&quot;osm_points&quot;) #plot(st_geometry(water$osm_lines), main=&quot;osm_lines&quot;) # lines is empty and wont plot plot(st_geometry(water$osm_polygons), main=&quot;osm_polygons&quot;) In this instance, the lines is empty, out best bet are the points or the polygons options! index &lt;- st_nearest_feature(locs_sf, water$osm_polygons) locs_sf$water_dist_m &lt;- st_distance(locs_sf, water$osm_polygons[index,], by_element=T) # Note `by_element=T` tells st_distance to evaluate things line by line. For more examples of using the osmdata package see: the projects github page 7.2.6 Vegetation productivity 7.2.6.1 modisfast modisfast is an R package designed for easy and fast downloads of MODIS Land products, VIIRS Land products, and GPM (Global Precipitation Measurement Mission) Earth Observation data. It allows for easy access to time series data accross large spatial extents directly to your computer! These are the data layers commonly used to extract normalized difference vegetation index (NDVI) and Enhanced Vegetation Index (EVI) information. When using modisfast you should reference: Taconet et al., (2024). modisfast: An R package for fast and efficient access to MODIS, VIIRS and GPM Earth Observation data. Journal of Open Source Software, 9(103), 7343, For a quick into to the package see: https://github.com/ptaconet/modisfast IMPORTANT to use this package you should create your own login for the Earthdata servers, then specify your EOSDIS credentials. To create your free account go to : https://urs.earthdata.nasa.gov/. Let’s load the package: library(modisfast) # Specify your own credentials: log &lt;- mf_login(credentials = c(&quot;your_username&quot;, &quot;your_password&quot;)) # set your own EOSDIS username and password For modisfat to work, we need to provide a sf polygon with an “id” column. We can do this easily using the aoi object we created earlier” # Create aoi if sf format aoi_sf &lt;- st_as_sf(data.frame(id = &quot;algar&quot;, geom = st_as_sfc(aoi), wkt = &quot;geom&quot;, crs = 4326)) # a ROI of interest, format sf polygon # MODIS collections and variables (bands) of interest collection &lt;- &quot;MOD13Q1.061&quot; # run mf_list_collections() for an exhaustive list of collections available variables &lt;- c(&quot;_250m_16_days_NDVI&quot;) # run mf_list_variables(&quot;MOD13Q1.061&quot;) for an exhaustive list of variables available for the collection &quot;MOD13A3.061&quot; # Specify the time range - we will use a winter-summer transition time_range &lt;- as.Date(c(&quot;2019-03-01&quot;, &quot;2019-07-01&quot;)) # a time range of interest ## Get the URLs of the data urls &lt;- mf_get_url( collection = collection, variables = variables, roi = aoi_sf, time_range = time_range ) ## Building the URLs... ## ✔ URL(s) built. ## Estimated maximum size of data to be downloaded is 4 Mb ## Download the data. By default the data is downloaded in a temporary directory, but you can specify a folder res_dl &lt;- mf_download_data(urls, parallel = TRUE) ## 1 datasets in total : 0 already downloaded and 1 datasets to download ## Downloading the data (destination folder: /var/folders/95/0ydz4d79163427j3k5crp3fh0000gn/T//RtmpCUP7ah/modisfast_175e1350023e ) ... ## Estimated maximum size of data to be downloaded is ~ 4 Mb ## ✔ Data were all properly downloaded under the folder(s) /var/folders/95/0ydz4d79163427j3k5crp3fh0000gn/T//RtmpCUP7ah/modisfast_175e1350023e/data/algar/MOD13Q1.061 ## ## Actual size of downloaded data is 2 Mb ## ℹ To import the data in R, use the function modisfast::mf_import_data() rather than terra::rast() or stars::read_stars(). More info at help(mf_import_data) # Import it into R r &lt;- mf_import_data( path = dirname(res_dl$destfile[1]), collection = collection, proj_epsg = 26912 ) ## Importing the dataset as a SpatRaster object... ## ✔ Dataset imported # Plot the resulting NDVI data terra::plot(r, col = rev(terrain.colors(20)), range=c(0,1)) # Specify the range of values you want to plot # Extract the data to camera locations tmp &lt;- terra::extract(r, locs_terra, fun=mean, ID=F) # Extract - each layer will become its own column locs_sf$mean_ndvi &lt;- rowMeans(tmp) # take the mean of each column Easy! Check the output: boxplot(locs_sf$mean_ndvi, ylab=&quot;Mean NDVI score&quot;, las=1) Here we have created an average NDVI score across five months, but we could get monthly averages. Some products are even available at daily resolution! 7.2.7 Digging deeper If you want to dig into estimating NDVI metrics from camera trap viewshed, rather than from satellite data, check out the phenopix R package. It allows the user to extract visual information from time lapse images. It provides a quantitative daily measure of vegetation phenology at each site (e.g. green-up, senescence, snow cover). Alberton, B. et al. 2017. Introducing digital cameras to monitor plant phenology in the tropics: applications for conservation. Perspect. Ecol. Conserv Filippa, G. et al. 2017. phenopix: Process Digital Images of a Vegetation Cover. R package version 2.3.1. The Phenopix package has a five step process: a region of interest (ROI) is identified; the red, green, and blue digital numbers from each image in the time series is extracted and an index of relative ‘greenness’ is computed and plotted from the digital numbers; the vegetation indices’ data points are filtered to remove inconsistencies; a curve is fit to the data and phenophases are determined from the curve; phenophase uncertainties are calculated. To see an application and comparison of these metrics, we highly recommend that you check out Catherine Sun’s (WildCo alumni) paper on the subject: Sun, Catherine, et al. “Simultaneous monitoring of vegetation dynamics and wildlife activity with camera traps to assess habitat change.” Remote Sensing in Ecology and Conservation 7.4 (2021): 666-684. And the code associated with this publication on the WildCo GitHub Page 7.3 Convert and save your covariates # Convert columns to numeric locs_sf$road_dist_m &lt;- as.numeric(locs_sf$road_dist_m) # Convert it back to a dataframe locs_sf$geometry &lt;- NULL locs &lt;- left_join(locs, locs_sf) # Write the dataset write.csv(locs, paste0(&quot;data/processed_data/&quot;, locs$project_id[1],&quot;_camera_locations_and_covariates.csv&quot;), row.names=F) 7.4 Correlations between predictors So we have used a variety of different techniques to generate covariates for our subsequent analyses. However, it is important to note that we cannot just through these variables into a model. One way to check if your different variables are confound/correlated is using the corrplot package. library(corrplot) # First we need to create a correlation matrix between the different variables of interest M &lt;- cor(locs[, c(&quot;line_of_sight_m&quot;, &quot;water_depth_m&quot;, &quot;elevation&quot;, &quot;road_dist_m&quot;, &quot;mean_ndvi&quot;)]) Now lets make the basic corrplot: corrplot(M) The cells denote pairwise correlations between the rows and the columns. The great thing about corrplot is customization option are near endless - see the corrplot vignette. Let’s make a better, more informative, corrplot! corrplot(M, #The correlation matrix we made method=&quot;color&quot;, # How we want the cells type=&quot;upper&quot;, # Just show the upper part (it is usually mirrored) order=&quot;hclust&quot;, # Order the variables using the hclust method addCoef.col = &quot;black&quot;, # Add coefficient of correlation tl.col=&quot;black&quot;, tl.srt=45, # Control the text label color and rotation diag=F # Suppress the diagonal correlations (which are 1 anyway) ) In general there is very low correlation between our different predictors! If we were seeing pairwise correlations &gt;0.7 we perhaps wouldn’t include those in the same model. "],["exploration.html", "Chapter 8 Analysis data exploration 8.1 Final locations plot 8.2 Independent detections summary 8.3 Temporal patterns in capture rates 8.4 Species-specific capture rates 8.5 Spatial patterns in capture rates 8.6 Species co-occurences 8.7 Covariate plots", " Chapter 8 Analysis data exploration Now things start to get really interesting - we are getting closer to analyzing our data. Before we get into building any models however, we must thoroughly explore our data. We want to ask questions like? How many species did we detect? Which are the most common? Where did we detect them? When did we detect them? How do species detections relate to our covariates? In the error checking section we focused our ‘data exploration’ on figures which would help us find issues with our data, now we want to shift gears and create plots which actually tell us about patterns in our data. To reflect the change from error check to patterns, all of the datasets we use will now be coming out of the data/processed_data/ folder. Create a new .R script Call it 03_example_exploration.R. Load the required packages # Check you have them and load them list.of.packages &lt;- c(&quot;kableExtra&quot;, &quot;tidyr&quot;, &quot;leaflet&quot;, &quot;dplyr&quot;, &quot;viridis&quot;, &quot;corrplot&quot;, &quot;lubridate&quot;, &quot;plotly&quot;) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) 8.1 Final locations plot So lets read in the camera_locations.csv and plot the final survey locations in leaflet. We repeat this as we may have filtered out some stations in the error checking section - for example if they failed to collect any useful data: locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations_and_covariates.csv&quot;) # If you want to color by a category do it here: category &lt;- &quot;feature_type&quot; # First lets choose a category to color locs[,category] &lt;- factor(locs[,category]) col.cat &lt;- turbo(length(levels(locs[,category]))) # Add it to the dataframe locs$colours &lt;- col.cat[locs[,category]] m &lt;- leaflet() %&gt;% # Add a satellite image layer addProviderTiles(providers$Esri.WorldImagery, group=&quot;Satellite&quot;) %&gt;% addProviderTiles(providers$Esri.WorldTopoMap, group=&quot;Base&quot;) %&gt;% addCircleMarkers(lng=locs$longitude, lat=locs$latitude, # Color the markers depending on the &#39;feature type&#39; color=locs$colours, # Add a popup of the deployment code popup=paste(locs$placename, locs[,category])) %&gt;% # Add a legend explaining what is going on addLegend(&quot;bottomleft&quot;, colors = col.cat, labels = levels(locs[,category]), title = category, labFormat = labelFormat(prefix = &quot;$&quot;), opacity = 1 ) %&gt;% # add a layer control box to toggle between the layers addLayersControl( baseGroups = c(&quot;Satellite&quot;, &quot;Base&quot;), options = layersControlOptions(collapsed = FALSE) ) m 8.2 Independent detections summary When you are writing papers or reports based on camera data, it is useful to have a capture summary table in the main text or as an appendix. We will use the species list we created to append summary information to: # Also read in the species list sp_summary &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_species_list.csv&quot;, header=T) 8.2.1 Total number of captures To summaries the wildlife detections in this project we can make use of the ...total_observations.csv files: total_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_total_observations.csv&quot;, header=T) The format of the output tables is what we would call ‘wide’ format - we have multiple species observation on the same row. placename days Alces.alces Canis.latrans Canis.lupus Cervus.canadensis Lepus.americanus Lontra.canadensis Lynx.canadensis Martes.americana Odocoileus.virginianus Oryctolagus.cuniculus Rangifer.tarandus Tamiasciurus.hudsonicus Ursus.americanus Vulpes.vulpes ALG027 358 4 4 13 1 0 0 10 0 3 0 0 0 32 0 ALG029 593 17 0 2 0 0 0 1 0 35 0 0 0 9 0 ALG031 593 11 0 8 0 1 1 0 6 1 1 21 0 2 1 ALG032 592 2 0 0 0 0 0 1 0 2 0 5 0 0 0 ALG035 594 11 0 2 0 7 0 1 0 1 0 6 3 2 0 ALG036 417 1 0 0 0 42 0 1 0 5 0 0 15 0 0 ALG037 592 2 0 0 0 0 0 0 0 0 0 3 0 2 0 ALG038 593 0 0 0 0 0 0 0 1 0 0 12 0 1 0 ALG039 591 2 0 6 0 0 0 0 0 0 0 5 0 3 0 ALG043 392 6 0 0 0 13 0 0 0 21 0 0 0 2 0 ALG044 392 15 0 3 0 2 0 0 1 10 0 0 0 7 2 ALG045 418 5 0 5 0 3 0 4 3 0 0 1 0 6 0 ALG046 592 9 0 0 0 14 0 1 0 18 0 0 0 0 1 ALG047 507 1 1 13 0 1 0 0 0 15 0 5 1 9 0 ALG048 593 0 0 2 0 0 0 1 1 1 0 7 0 4 0 ALG049 341 2 0 0 0 0 0 0 1 14 0 0 4 10 0 ALG052 590 13 0 3 0 9 0 1 1 65 0 1 0 17 0 ALG053 592 3 0 0 0 0 0 1 0 0 0 1 0 2 3 ALG054 595 1 0 1 0 25 0 3 0 28 0 0 0 2 0 ALG055 592 3 3 26 0 29 0 9 1 19 0 0 2 10 2 ALG056 595 8 0 0 0 0 0 1 0 6 0 0 0 2 0 ALG057 405 0 0 1 0 0 0 0 0 13 1 0 0 2 0 ALG058 595 2 0 0 0 58 0 3 0 51 0 1 0 11 0 ALG059 465 7 0 0 0 2 0 1 0 0 0 3 0 9 0 ALG060 592 1 0 0 0 1 0 1 2 0 0 0 0 6 0 ALG061 590 0 0 0 0 0 0 0 4 60 0 0 0 5 0 ALG062 592 1 0 4 0 4 0 1 0 48 0 0 0 16 0 ALG063 593 1 0 1 0 1 0 0 1 23 0 4 1 2 0 ALG064 592 2 0 0 0 0 0 0 0 0 0 10 0 0 0 ALG065 447 26 0 0 0 0 0 0 1 0 0 1 0 2 0 ALG066 591 2 0 0 0 0 0 1 0 1 0 6 0 0 0 ALG067 391 2 0 2 0 0 0 0 0 3 0 0 0 0 0 ALG068 592 3 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG069 592 6 0 0 0 3 0 4 0 64 0 0 0 25 0 ALG070 408 2 0 0 0 0 0 0 1 4 0 3 0 1 0 ALG071 595 3 0 1 0 21 0 2 0 46 0 0 0 22 1 ALG072 595 1 0 0 0 5 0 4 0 5 0 1 0 3 0 ALG073 593 1 0 0 0 0 0 0 0 0 0 23 0 0 0 Sometimes, however, we might want a “longer” format where every row represents a unique species_site combination. We can do this using the ‘pivot_longer’ function: long_obs &lt;- total_obs %&gt;% pivot_longer(cols=sp_summary$sp, # The columns we want to create into rows - species names_to=&quot;sp&quot;, # What we what the number column to be called values_to = &quot;count&quot;) # Takes the values in the species columns and calls them `count` We now have a dataframe where each row is a unique species at a given location (e.g. ALG027) - a.k.a. long format! placename days sp count ALG027 358 Alces.alces 4 ALG027 358 Cervus.canadensis 1 ALG027 358 Odocoileus.virginianus 3 ALG027 358 Rangifer.tarandus 0 ALG027 358 Canis.latrans 4 ALG027 358 Canis.lupus 13 ALG027 358 Vulpes.vulpes 0 ALG027 358 Lynx.canadensis 10 ALG027 358 Lontra.canadensis 0 ALG027 358 Martes.americana 0 ALG027 358 Ursus.americanus 32 ALG027 358 Lepus.americanus 0 ALG027 358 Oryctolagus.cuniculus 0 ALG027 358 Tamiasciurus.hudsonicus 0 ALG029 593 Alces.alces 17 ALG029 593 Cervus.canadensis 0 ALG029 593 Odocoileus.virginianus 35 ALG029 593 Rangifer.tarandus 0 ALG029 593 Canis.latrans 0 ALG029 593 Canis.lupus 2 ALG029 593 Vulpes.vulpes 0 ALG029 593 Lynx.canadensis 1 ALG029 593 Lontra.canadensis 0 ALG029 593 Martes.americana 0 ALG029 593 Ursus.americanus 9 ALG029 593 Lepus.americanus 0 ALG029 593 Oryctolagus.cuniculus 0 ALG029 593 Tamiasciurus.hudsonicus 0 ALG031 593 Alces.alces 11 ALG031 593 Cervus.canadensis 0 ALG031 593 Odocoileus.virginianus 1 ALG031 593 Rangifer.tarandus 21 ALG031 593 Canis.latrans 0 ALG031 593 Canis.lupus 8 ALG031 593 Vulpes.vulpes 1 ALG031 593 Lynx.canadensis 0 ALG031 593 Lontra.canadensis 1 ALG031 593 Martes.americana 6 ALG031 593 Ursus.americanus 2 ALG031 593 Lepus.americanus 1 ALG031 593 Oryctolagus.cuniculus 1 ALG031 593 Tamiasciurus.hudsonicus 0 ALG032 592 Alces.alces 2 ALG032 592 Cervus.canadensis 0 ALG032 592 Odocoileus.virginianus 2 ALG032 592 Rangifer.tarandus 5 ALG032 592 Canis.latrans 0 ALG032 592 Canis.lupus 0 ALG032 592 Vulpes.vulpes 0 ALG032 592 Lynx.canadensis 1 ALG032 592 Lontra.canadensis 0 ALG032 592 Martes.americana 0 ALG032 592 Ursus.americanus 0 ALG032 592 Lepus.americanus 0 ALG032 592 Oryctolagus.cuniculus 0 ALG032 592 Tamiasciurus.hudsonicus 0 ALG035 594 Alces.alces 11 ALG035 594 Cervus.canadensis 0 ALG035 594 Odocoileus.virginianus 1 ALG035 594 Rangifer.tarandus 6 ALG035 594 Canis.latrans 0 ALG035 594 Canis.lupus 2 ALG035 594 Vulpes.vulpes 0 ALG035 594 Lynx.canadensis 1 ALG035 594 Lontra.canadensis 0 ALG035 594 Martes.americana 0 ALG035 594 Ursus.americanus 2 ALG035 594 Lepus.americanus 7 ALG035 594 Oryctolagus.cuniculus 0 ALG035 594 Tamiasciurus.hudsonicus 3 ALG036 417 Alces.alces 1 ALG036 417 Cervus.canadensis 0 ALG036 417 Odocoileus.virginianus 5 ALG036 417 Rangifer.tarandus 0 ALG036 417 Canis.latrans 0 ALG036 417 Canis.lupus 0 ALG036 417 Vulpes.vulpes 0 ALG036 417 Lynx.canadensis 1 ALG036 417 Lontra.canadensis 0 ALG036 417 Martes.americana 0 ALG036 417 Ursus.americanus 0 ALG036 417 Lepus.americanus 42 ALG036 417 Oryctolagus.cuniculus 0 ALG036 417 Tamiasciurus.hudsonicus 15 ALG037 592 Alces.alces 2 ALG037 592 Cervus.canadensis 0 ALG037 592 Odocoileus.virginianus 0 ALG037 592 Rangifer.tarandus 3 ALG037 592 Canis.latrans 0 ALG037 592 Canis.lupus 0 ALG037 592 Vulpes.vulpes 0 ALG037 592 Lynx.canadensis 0 ALG037 592 Lontra.canadensis 0 ALG037 592 Martes.americana 0 ALG037 592 Ursus.americanus 2 ALG037 592 Lepus.americanus 0 ALG037 592 Oryctolagus.cuniculus 0 ALG037 592 Tamiasciurus.hudsonicus 0 ALG038 593 Alces.alces 0 ALG038 593 Cervus.canadensis 0 ALG038 593 Odocoileus.virginianus 0 ALG038 593 Rangifer.tarandus 12 ALG038 593 Canis.latrans 0 ALG038 593 Canis.lupus 0 ALG038 593 Vulpes.vulpes 0 ALG038 593 Lynx.canadensis 0 ALG038 593 Lontra.canadensis 0 ALG038 593 Martes.americana 1 ALG038 593 Ursus.americanus 1 ALG038 593 Lepus.americanus 0 ALG038 593 Oryctolagus.cuniculus 0 ALG038 593 Tamiasciurus.hudsonicus 0 ALG039 591 Alces.alces 2 ALG039 591 Cervus.canadensis 0 ALG039 591 Odocoileus.virginianus 0 ALG039 591 Rangifer.tarandus 5 ALG039 591 Canis.latrans 0 ALG039 591 Canis.lupus 6 ALG039 591 Vulpes.vulpes 0 ALG039 591 Lynx.canadensis 0 ALG039 591 Lontra.canadensis 0 ALG039 591 Martes.americana 0 ALG039 591 Ursus.americanus 3 ALG039 591 Lepus.americanus 0 ALG039 591 Oryctolagus.cuniculus 0 ALG039 591 Tamiasciurus.hudsonicus 0 ALG043 392 Alces.alces 6 ALG043 392 Cervus.canadensis 0 ALG043 392 Odocoileus.virginianus 21 ALG043 392 Rangifer.tarandus 0 ALG043 392 Canis.latrans 0 ALG043 392 Canis.lupus 0 ALG043 392 Vulpes.vulpes 0 ALG043 392 Lynx.canadensis 0 ALG043 392 Lontra.canadensis 0 ALG043 392 Martes.americana 0 ALG043 392 Ursus.americanus 2 ALG043 392 Lepus.americanus 13 ALG043 392 Oryctolagus.cuniculus 0 ALG043 392 Tamiasciurus.hudsonicus 0 ALG044 392 Alces.alces 15 ALG044 392 Cervus.canadensis 0 ALG044 392 Odocoileus.virginianus 10 ALG044 392 Rangifer.tarandus 0 ALG044 392 Canis.latrans 0 ALG044 392 Canis.lupus 3 ALG044 392 Vulpes.vulpes 2 ALG044 392 Lynx.canadensis 0 ALG044 392 Lontra.canadensis 0 ALG044 392 Martes.americana 1 ALG044 392 Ursus.americanus 7 ALG044 392 Lepus.americanus 2 ALG044 392 Oryctolagus.cuniculus 0 ALG044 392 Tamiasciurus.hudsonicus 0 ALG045 418 Alces.alces 5 ALG045 418 Cervus.canadensis 0 ALG045 418 Odocoileus.virginianus 0 ALG045 418 Rangifer.tarandus 1 ALG045 418 Canis.latrans 0 ALG045 418 Canis.lupus 5 ALG045 418 Vulpes.vulpes 0 ALG045 418 Lynx.canadensis 4 ALG045 418 Lontra.canadensis 0 ALG045 418 Martes.americana 3 ALG045 418 Ursus.americanus 6 ALG045 418 Lepus.americanus 3 ALG045 418 Oryctolagus.cuniculus 0 ALG045 418 Tamiasciurus.hudsonicus 0 ALG046 592 Alces.alces 9 ALG046 592 Cervus.canadensis 0 ALG046 592 Odocoileus.virginianus 18 ALG046 592 Rangifer.tarandus 0 ALG046 592 Canis.latrans 0 ALG046 592 Canis.lupus 0 ALG046 592 Vulpes.vulpes 1 ALG046 592 Lynx.canadensis 1 ALG046 592 Lontra.canadensis 0 ALG046 592 Martes.americana 0 ALG046 592 Ursus.americanus 0 ALG046 592 Lepus.americanus 14 ALG046 592 Oryctolagus.cuniculus 0 ALG046 592 Tamiasciurus.hudsonicus 0 ALG047 507 Alces.alces 1 ALG047 507 Cervus.canadensis 0 ALG047 507 Odocoileus.virginianus 15 ALG047 507 Rangifer.tarandus 5 ALG047 507 Canis.latrans 1 ALG047 507 Canis.lupus 13 ALG047 507 Vulpes.vulpes 0 ALG047 507 Lynx.canadensis 0 ALG047 507 Lontra.canadensis 0 ALG047 507 Martes.americana 0 ALG047 507 Ursus.americanus 9 ALG047 507 Lepus.americanus 1 ALG047 507 Oryctolagus.cuniculus 0 ALG047 507 Tamiasciurus.hudsonicus 1 ALG048 593 Alces.alces 0 ALG048 593 Cervus.canadensis 0 ALG048 593 Odocoileus.virginianus 1 ALG048 593 Rangifer.tarandus 7 ALG048 593 Canis.latrans 0 ALG048 593 Canis.lupus 2 ALG048 593 Vulpes.vulpes 0 ALG048 593 Lynx.canadensis 1 ALG048 593 Lontra.canadensis 0 ALG048 593 Martes.americana 1 ALG048 593 Ursus.americanus 4 ALG048 593 Lepus.americanus 0 ALG048 593 Oryctolagus.cuniculus 0 ALG048 593 Tamiasciurus.hudsonicus 0 ALG049 341 Alces.alces 2 ALG049 341 Cervus.canadensis 0 ALG049 341 Odocoileus.virginianus 14 ALG049 341 Rangifer.tarandus 0 ALG049 341 Canis.latrans 0 ALG049 341 Canis.lupus 0 ALG049 341 Vulpes.vulpes 0 ALG049 341 Lynx.canadensis 0 ALG049 341 Lontra.canadensis 0 ALG049 341 Martes.americana 1 ALG049 341 Ursus.americanus 10 ALG049 341 Lepus.americanus 0 ALG049 341 Oryctolagus.cuniculus 0 ALG049 341 Tamiasciurus.hudsonicus 4 ALG052 590 Alces.alces 13 ALG052 590 Cervus.canadensis 0 ALG052 590 Odocoileus.virginianus 65 ALG052 590 Rangifer.tarandus 1 ALG052 590 Canis.latrans 0 ALG052 590 Canis.lupus 3 ALG052 590 Vulpes.vulpes 0 ALG052 590 Lynx.canadensis 1 ALG052 590 Lontra.canadensis 0 ALG052 590 Martes.americana 1 ALG052 590 Ursus.americanus 17 ALG052 590 Lepus.americanus 9 ALG052 590 Oryctolagus.cuniculus 0 ALG052 590 Tamiasciurus.hudsonicus 0 ALG053 592 Alces.alces 3 ALG053 592 Cervus.canadensis 0 ALG053 592 Odocoileus.virginianus 0 ALG053 592 Rangifer.tarandus 1 ALG053 592 Canis.latrans 0 ALG053 592 Canis.lupus 0 ALG053 592 Vulpes.vulpes 3 ALG053 592 Lynx.canadensis 1 ALG053 592 Lontra.canadensis 0 ALG053 592 Martes.americana 0 ALG053 592 Ursus.americanus 2 ALG053 592 Lepus.americanus 0 ALG053 592 Oryctolagus.cuniculus 0 ALG053 592 Tamiasciurus.hudsonicus 0 ALG054 595 Alces.alces 1 ALG054 595 Cervus.canadensis 0 ALG054 595 Odocoileus.virginianus 28 ALG054 595 Rangifer.tarandus 0 ALG054 595 Canis.latrans 0 ALG054 595 Canis.lupus 1 ALG054 595 Vulpes.vulpes 0 ALG054 595 Lynx.canadensis 3 ALG054 595 Lontra.canadensis 0 ALG054 595 Martes.americana 0 ALG054 595 Ursus.americanus 2 ALG054 595 Lepus.americanus 25 ALG054 595 Oryctolagus.cuniculus 0 ALG054 595 Tamiasciurus.hudsonicus 0 ALG055 592 Alces.alces 3 ALG055 592 Cervus.canadensis 0 ALG055 592 Odocoileus.virginianus 19 ALG055 592 Rangifer.tarandus 0 ALG055 592 Canis.latrans 3 ALG055 592 Canis.lupus 26 ALG055 592 Vulpes.vulpes 2 ALG055 592 Lynx.canadensis 9 ALG055 592 Lontra.canadensis 0 ALG055 592 Martes.americana 1 ALG055 592 Ursus.americanus 10 ALG055 592 Lepus.americanus 29 ALG055 592 Oryctolagus.cuniculus 0 ALG055 592 Tamiasciurus.hudsonicus 2 ALG056 595 Alces.alces 8 ALG056 595 Cervus.canadensis 0 ALG056 595 Odocoileus.virginianus 6 ALG056 595 Rangifer.tarandus 0 ALG056 595 Canis.latrans 0 ALG056 595 Canis.lupus 0 ALG056 595 Vulpes.vulpes 0 ALG056 595 Lynx.canadensis 1 ALG056 595 Lontra.canadensis 0 ALG056 595 Martes.americana 0 ALG056 595 Ursus.americanus 2 ALG056 595 Lepus.americanus 0 ALG056 595 Oryctolagus.cuniculus 0 ALG056 595 Tamiasciurus.hudsonicus 0 ALG057 405 Alces.alces 0 ALG057 405 Cervus.canadensis 0 ALG057 405 Odocoileus.virginianus 13 ALG057 405 Rangifer.tarandus 0 ALG057 405 Canis.latrans 0 ALG057 405 Canis.lupus 1 ALG057 405 Vulpes.vulpes 0 ALG057 405 Lynx.canadensis 0 ALG057 405 Lontra.canadensis 0 ALG057 405 Martes.americana 0 ALG057 405 Ursus.americanus 2 ALG057 405 Lepus.americanus 0 ALG057 405 Oryctolagus.cuniculus 1 ALG057 405 Tamiasciurus.hudsonicus 0 ALG058 595 Alces.alces 2 ALG058 595 Cervus.canadensis 0 ALG058 595 Odocoileus.virginianus 51 ALG058 595 Rangifer.tarandus 1 ALG058 595 Canis.latrans 0 ALG058 595 Canis.lupus 0 ALG058 595 Vulpes.vulpes 0 ALG058 595 Lynx.canadensis 3 ALG058 595 Lontra.canadensis 0 ALG058 595 Martes.americana 0 ALG058 595 Ursus.americanus 11 ALG058 595 Lepus.americanus 58 ALG058 595 Oryctolagus.cuniculus 0 ALG058 595 Tamiasciurus.hudsonicus 0 ALG059 465 Alces.alces 7 ALG059 465 Cervus.canadensis 0 ALG059 465 Odocoileus.virginianus 0 ALG059 465 Rangifer.tarandus 3 ALG059 465 Canis.latrans 0 ALG059 465 Canis.lupus 0 ALG059 465 Vulpes.vulpes 0 ALG059 465 Lynx.canadensis 1 ALG059 465 Lontra.canadensis 0 ALG059 465 Martes.americana 0 ALG059 465 Ursus.americanus 9 ALG059 465 Lepus.americanus 2 ALG059 465 Oryctolagus.cuniculus 0 ALG059 465 Tamiasciurus.hudsonicus 0 ALG060 592 Alces.alces 1 ALG060 592 Cervus.canadensis 0 ALG060 592 Odocoileus.virginianus 0 ALG060 592 Rangifer.tarandus 0 ALG060 592 Canis.latrans 0 ALG060 592 Canis.lupus 0 ALG060 592 Vulpes.vulpes 0 ALG060 592 Lynx.canadensis 1 ALG060 592 Lontra.canadensis 0 ALG060 592 Martes.americana 2 ALG060 592 Ursus.americanus 6 ALG060 592 Lepus.americanus 1 ALG060 592 Oryctolagus.cuniculus 0 ALG060 592 Tamiasciurus.hudsonicus 0 ALG061 590 Alces.alces 0 ALG061 590 Cervus.canadensis 0 ALG061 590 Odocoileus.virginianus 60 ALG061 590 Rangifer.tarandus 0 ALG061 590 Canis.latrans 0 ALG061 590 Canis.lupus 0 ALG061 590 Vulpes.vulpes 0 ALG061 590 Lynx.canadensis 0 ALG061 590 Lontra.canadensis 0 ALG061 590 Martes.americana 4 ALG061 590 Ursus.americanus 5 ALG061 590 Lepus.americanus 0 ALG061 590 Oryctolagus.cuniculus 0 ALG061 590 Tamiasciurus.hudsonicus 0 ALG062 592 Alces.alces 1 ALG062 592 Cervus.canadensis 0 ALG062 592 Odocoileus.virginianus 48 ALG062 592 Rangifer.tarandus 0 ALG062 592 Canis.latrans 0 ALG062 592 Canis.lupus 4 ALG062 592 Vulpes.vulpes 0 ALG062 592 Lynx.canadensis 1 ALG062 592 Lontra.canadensis 0 ALG062 592 Martes.americana 0 ALG062 592 Ursus.americanus 16 ALG062 592 Lepus.americanus 4 ALG062 592 Oryctolagus.cuniculus 0 ALG062 592 Tamiasciurus.hudsonicus 0 ALG063 593 Alces.alces 1 ALG063 593 Cervus.canadensis 0 ALG063 593 Odocoileus.virginianus 23 ALG063 593 Rangifer.tarandus 4 ALG063 593 Canis.latrans 0 ALG063 593 Canis.lupus 1 ALG063 593 Vulpes.vulpes 0 ALG063 593 Lynx.canadensis 0 ALG063 593 Lontra.canadensis 0 ALG063 593 Martes.americana 1 ALG063 593 Ursus.americanus 2 ALG063 593 Lepus.americanus 1 ALG063 593 Oryctolagus.cuniculus 0 ALG063 593 Tamiasciurus.hudsonicus 1 ALG064 592 Alces.alces 2 ALG064 592 Cervus.canadensis 0 ALG064 592 Odocoileus.virginianus 0 ALG064 592 Rangifer.tarandus 10 ALG064 592 Canis.latrans 0 ALG064 592 Canis.lupus 0 ALG064 592 Vulpes.vulpes 0 ALG064 592 Lynx.canadensis 0 ALG064 592 Lontra.canadensis 0 ALG064 592 Martes.americana 0 ALG064 592 Ursus.americanus 0 ALG064 592 Lepus.americanus 0 ALG064 592 Oryctolagus.cuniculus 0 ALG064 592 Tamiasciurus.hudsonicus 0 ALG065 447 Alces.alces 26 ALG065 447 Cervus.canadensis 0 ALG065 447 Odocoileus.virginianus 0 ALG065 447 Rangifer.tarandus 1 ALG065 447 Canis.latrans 0 ALG065 447 Canis.lupus 0 ALG065 447 Vulpes.vulpes 0 ALG065 447 Lynx.canadensis 0 ALG065 447 Lontra.canadensis 0 ALG065 447 Martes.americana 1 ALG065 447 Ursus.americanus 2 ALG065 447 Lepus.americanus 0 ALG065 447 Oryctolagus.cuniculus 0 ALG065 447 Tamiasciurus.hudsonicus 0 ALG066 591 Alces.alces 2 ALG066 591 Cervus.canadensis 0 ALG066 591 Odocoileus.virginianus 1 ALG066 591 Rangifer.tarandus 6 ALG066 591 Canis.latrans 0 ALG066 591 Canis.lupus 0 ALG066 591 Vulpes.vulpes 0 ALG066 591 Lynx.canadensis 1 ALG066 591 Lontra.canadensis 0 ALG066 591 Martes.americana 0 ALG066 591 Ursus.americanus 0 ALG066 591 Lepus.americanus 0 ALG066 591 Oryctolagus.cuniculus 0 ALG066 591 Tamiasciurus.hudsonicus 0 ALG067 391 Alces.alces 2 ALG067 391 Cervus.canadensis 0 ALG067 391 Odocoileus.virginianus 3 ALG067 391 Rangifer.tarandus 0 ALG067 391 Canis.latrans 0 ALG067 391 Canis.lupus 2 ALG067 391 Vulpes.vulpes 0 ALG067 391 Lynx.canadensis 0 ALG067 391 Lontra.canadensis 0 ALG067 391 Martes.americana 0 ALG067 391 Ursus.americanus 0 ALG067 391 Lepus.americanus 0 ALG067 391 Oryctolagus.cuniculus 0 ALG067 391 Tamiasciurus.hudsonicus 0 ALG068 592 Alces.alces 3 ALG068 592 Cervus.canadensis 0 ALG068 592 Odocoileus.virginianus 0 ALG068 592 Rangifer.tarandus 0 ALG068 592 Canis.latrans 0 ALG068 592 Canis.lupus 0 ALG068 592 Vulpes.vulpes 0 ALG068 592 Lynx.canadensis 0 ALG068 592 Lontra.canadensis 0 ALG068 592 Martes.americana 0 ALG068 592 Ursus.americanus 2 ALG068 592 Lepus.americanus 0 ALG068 592 Oryctolagus.cuniculus 0 ALG068 592 Tamiasciurus.hudsonicus 0 ALG069 592 Alces.alces 6 ALG069 592 Cervus.canadensis 0 ALG069 592 Odocoileus.virginianus 64 ALG069 592 Rangifer.tarandus 0 ALG069 592 Canis.latrans 0 ALG069 592 Canis.lupus 0 ALG069 592 Vulpes.vulpes 0 ALG069 592 Lynx.canadensis 4 ALG069 592 Lontra.canadensis 0 ALG069 592 Martes.americana 0 ALG069 592 Ursus.americanus 25 ALG069 592 Lepus.americanus 3 ALG069 592 Oryctolagus.cuniculus 0 ALG069 592 Tamiasciurus.hudsonicus 0 ALG070 408 Alces.alces 2 ALG070 408 Cervus.canadensis 0 ALG070 408 Odocoileus.virginianus 4 ALG070 408 Rangifer.tarandus 3 ALG070 408 Canis.latrans 0 ALG070 408 Canis.lupus 0 ALG070 408 Vulpes.vulpes 0 ALG070 408 Lynx.canadensis 0 ALG070 408 Lontra.canadensis 0 ALG070 408 Martes.americana 1 ALG070 408 Ursus.americanus 1 ALG070 408 Lepus.americanus 0 ALG070 408 Oryctolagus.cuniculus 0 ALG070 408 Tamiasciurus.hudsonicus 0 ALG071 595 Alces.alces 3 ALG071 595 Cervus.canadensis 0 ALG071 595 Odocoileus.virginianus 46 ALG071 595 Rangifer.tarandus 0 ALG071 595 Canis.latrans 0 ALG071 595 Canis.lupus 1 ALG071 595 Vulpes.vulpes 1 ALG071 595 Lynx.canadensis 2 ALG071 595 Lontra.canadensis 0 ALG071 595 Martes.americana 0 ALG071 595 Ursus.americanus 22 ALG071 595 Lepus.americanus 21 ALG071 595 Oryctolagus.cuniculus 0 ALG071 595 Tamiasciurus.hudsonicus 0 ALG072 595 Alces.alces 1 ALG072 595 Cervus.canadensis 0 ALG072 595 Odocoileus.virginianus 5 ALG072 595 Rangifer.tarandus 1 ALG072 595 Canis.latrans 0 ALG072 595 Canis.lupus 0 ALG072 595 Vulpes.vulpes 0 ALG072 595 Lynx.canadensis 4 ALG072 595 Lontra.canadensis 0 ALG072 595 Martes.americana 0 ALG072 595 Ursus.americanus 3 ALG072 595 Lepus.americanus 5 ALG072 595 Oryctolagus.cuniculus 0 ALG072 595 Tamiasciurus.hudsonicus 0 ALG073 593 Alces.alces 1 ALG073 593 Cervus.canadensis 0 ALG073 593 Odocoileus.virginianus 0 ALG073 593 Rangifer.tarandus 23 ALG073 593 Canis.latrans 0 ALG073 593 Canis.lupus 0 ALG073 593 Vulpes.vulpes 0 ALG073 593 Lynx.canadensis 0 ALG073 593 Lontra.canadensis 0 ALG073 593 Martes.americana 0 ALG073 593 Ursus.americanus 0 ALG073 593 Lepus.americanus 0 ALG073 593 Oryctolagus.cuniculus 0 ALG073 593 Tamiasciurus.hudsonicus 0 It is often easier to use this long format to make summaries: # We can them summaries those using dplyr tmp &lt;- long_obs %&gt;% # Take the long observation data frame `long_obs` group_by(sp) %&gt;% # Group by species summarise(count=sum(count)) # Sum all the independent observations # Add it to the sp_summary dataframe sp_summary &lt;- left_join(sp_summary, tmp) 8.2.2 Raw occupancy We can very quickly flip a count to a presence/absence using as.logical this converts all integers to 1 and keeps 0’s as 0! # We use the mutate function to mutate the column total_binary &lt;- total_obs %&gt;% # The total obs dataframe mutate(across(sp_summary$sp, ~+as.logical(.x))) # across all of the species columns, make it binary # Flip the dataframe to longer - as before long_bin &lt;- total_binary %&gt;% pivot_longer(cols=sp_summary$sp, names_to=&quot;sp&quot;, values_to = &quot;count&quot;) # Takes the species names columns, and makes them unique rows with &quot;sp&quot; as the key Now when we do the same calculations, as before, we can calculate the number of sites occupied: # We can now sum the presence/absences and divide by the number of survey locations tmp &lt;- long_bin %&gt;% group_by(sp) %&gt;% summarise(occupancy=sum(count)/nrow(locs)) # divided the sum by the number of sites # add the results to the sp_summary sp_summary &lt;- left_join(sp_summary, tmp) 8.2.3 Comparison plot Then we can use the dataframe created above to summaries the detections and the occupancy patterns. Note - here we weave two plotly graphs together using the subplot() function! # Lets put the dataframes in a sensible order sp_summary &lt;- sp_summary[order(sp_summary$count),] yform &lt;- list(categoryorder = &quot;array&quot;, categoryarray = sp_summary$sp) xform &lt;- list(title=&quot;Captures&quot;) # Capture rate fig1 &lt;- plot_ly(x = sp_summary$count, y = sp_summary$sp, type = &#39;bar&#39;, orientation = &#39;h&#39;) %&gt;% layout(yaxis = yform, xaxis=xform) yform &lt;- list(categoryorder = &quot;array&quot;, categoryarray = sp_summary$sp, showticklabels=F) xform &lt;- list(title=&quot;Occupancy&quot;) # Occupancy fig2 &lt;- plot_ly(x = sp_summary$occupancy, y = sp_summary$sp, type = &#39;bar&#39;, orientation = &#39;h&#39;) %&gt;% layout(yaxis = yform, xaxis=xform) subplot(nrows=1,fig1, fig2, titleX = T) # We could stack them on top of one another using nrows=2 What does this output tell you about species-specific occurrences across the landscape? 8.3 Temporal patterns in capture rates Next lets summaries the temporal patterns in the number of sites (placenames) surveyed, and the total number of animals captured. We will use the monthly dataframes in order to do this, but you could do it at the weekly or daily scale if required! mon_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_monthly_observations.csv&quot;, header=T) We will first count the number of survey nights each location was active, then in the second step add the number of species detections. # Count up the number of stations and the number of camera nights mon_summary &lt;- mon_obs %&gt;% # Use the monthly observations dataframe group_by(date) %&gt;% # Group by the date summarise(locs_active=n(), # Count the number of active cameras cam_days=sum(days)) # And sum the active days # Add in the species specific counts - and join it with the mon_summary dataframe mon_summary &lt;- mon_obs %&gt;% group_by(date) %&gt;% summarise(across(sp_summary$sp, sum, na.rm=TRUE)) %&gt;% # summarise across all of # the species columns left_join(x=mon_summary) # Join with the mon_summary dataframe Now lets use lubridate to convert the timestamp column to a date object and plot the output. Each black dot represents the number of survey nights or average capture rate, respectively. # We first need to convert the date column to a date object mon_summary$date &lt;- ym(mon_summary$date) # Set up a two panel plot (side by side) par(mfrow=c(1,2)) plot(mon_summary$date, mon_summary$locs_active, type=&quot;o&quot;, pch=19, ylim=c(0, max(mon_summary$locs_active)), las=1, ylab=&quot;Number of cameras active&quot;, xlab=&quot;Date&quot;) # Sum all the captures rates for the species columns mon_summary$all.sp &lt;- rowSums(mon_summary[, sp_summary$sp]) # Plot them plot(mon_summary$date, mon_summary$all.sp/(mon_summary$cam_days/100), type=&quot;o&quot;, pch=19, las=1, ylab=&quot;Detections per 100 cam days&quot;, xlab=&quot;Date&quot;) As we saw in the error checking section, survey effort (number of cameras active) drops in early 2018 (left hand panel). The right hand panel shows the overall capture rate (for all species pooled), and you can see it is strongly seasonal - peaks in summers, and drops in the winter. 8.4 Species-specific capture rates We should now split up this overall capture rate, and explore temporal patterns in species-specific detections. We can do this by looping the code with a for() loop. par(mfrow=c(2,2)) i &lt;- 1 for(i in 1:length(sp_summary$sp)) { plot(mon_summary$date, pull(mon_summary, sp_summary$sp[i])/(mon_summary$cam_days/100), # The pull command allows you to grab a specific column in a dataframe and turn it into a vector! type=&quot;o&quot;, pch=19, las=1, ylab=&quot;Detections per 100 cam days&quot;, xlab=&quot;Date&quot;, main=sp_summary$sp[i]) } Can you see any interesting patterns in here? What do black bears do in winter? What time of year do we get the most marten detections? 8.5 Spatial patterns in capture rates We also often want to explore if there are any spatial patterns in capture rates, these can hint at any ecological relationships we might want to explore further. Here we do it for just a single species, the white-tailed deer (Odocoileus virginianus). Here we make use of the ‘total_obs’ data frame we imported earlier. We also use the ‘locs’ dataframe. total_obs &lt;- left_join(total_obs, locs) focal_species &lt;- &quot;Odocoileus.virginianus&quot; focal_cr &lt;- pull(total_obs, focal_species)/(total_obs$days/100) m &lt;- leaflet() %&gt;% addProviderTiles(providers$Esri.WorldTopoMap, group=&quot;Base&quot;) %&gt;% addCircleMarkers(lng=locs$longitude, lat=locs$latitude, # Add a popup of the deployment code popup=paste(locs$placename), radius=(focal_cr/max(focal_cr)*10)+1, stroke=F, fillOpacity=0.6) m Try it for some different species. Can you see any different patterns? 8.6 Species co-occurences Camera trap data are being increasingly used to model multiple species communities. In the same way in which we used the corrplot package in the (analysis covariates section(#covariates), we can use it to explore the co-occurrence patterns of the species in the community. The plot below uses the ‘total_obs’ dataframe, and performs pairwise correlations between the species on the left, and the species on the top row. Blue colors = positive correlation -&gt; at locations where you have high counts of one species, you also have high counts of the paired species. Red colors = negative correlation -&gt; at locations where you have high counts of one species, then you are likely to have low counts of the species pair (or vice-versa). We implement a more nuanced form of this data analysis in the interactions chapter. To make this plot we use the total_obs dataframe. # Reset the plot parameters par(mfrow=c(1,1)) # Pull the data for each of the species from tmp &lt;- total_obs[, sp_summary$sp] M &lt;- cor(tmp) corrplot(M, method=&quot;color&quot;, type=&quot;upper&quot;, order=&quot;hclust&quot;, # addCoef.col = &quot;black&quot;, # We suppress the coefs to make a cleaner plot tl.col=&quot;black&quot;, tl.srt=45, #Text label color and rotation diag=FALSE ) What would you conclude? 8.7 Covariate plots So far we have explored temporal and spatial patterns in species counts - but what about the effects of the covariates we derived in the analysis covariates section? Before embarking on an in depth analysis, it is always sensible to plot your response terms against predictors. Note we are often paranoid about “data dredging” or shopping around for “significant” predictors, as this isn’t good scientific practice. Here, we should only explore covariates for which we have a prior belief in there effects on the response term. We are not looking for significant relationships, rather trying to understand the structure of our data! You should know your data inside out before you start modelling. Final note just because you do not see a strong effect in your raw data, doesn’t mean that it will not have an effect in your final models, particularly if you plan to account for multiple confounding variables or use random effects! Lets have a quick look at what we have available to us in the camera_locations_and_covariates file: str(locs) ## &#39;data.frame&#39;: 38 obs. of 13 variables: ## $ project_id : chr &quot;AlgarRestorationProject&quot; &quot;AlgarRestorationProject&quot; &quot;AlgarRestorationProject&quot; &quot;AlgarRestorationProject&quot; ... ## $ placename : chr &quot;ALG027&quot; &quot;ALG029&quot; &quot;ALG031&quot; &quot;ALG032&quot; ... ## $ longitude : num -112 -113 -112 -112 -112 ... ## $ latitude : num 56.3 56.4 56.3 56.4 56.4 ... ## $ feature_type : Factor w/ 3 levels &quot;HumanUse&quot;,&quot;NatRegen&quot;,..: 1 1 1 1 1 2 1 1 1 2 ... ## $ line_of_sight_m: num 253 207 334 83 440 ... ## $ water_depth_m : num 0 0.385 0 0 0 ... ## $ elevation : num 528 510 526 510 505 ... ## $ elev_units : chr &quot;meters&quot; &quot;meters&quot; &quot;meters&quot; &quot;meters&quot; ... ## $ road_dist_m : num 16077 15647 14618 20980 19146 ... ## $ water_dist_m : num 3981 425 3284 1695 2172 ... ## $ mean_ndvi : num 0.593 0.46 0.529 0.521 0.544 ... ## $ colours : chr &quot;#30123BFF&quot; &quot;#30123BFF&quot; &quot;#30123BFF&quot; &quot;#30123BFF&quot; ... We have - feature type, water_depth_m, line_of_sight_m, elevation, road_dist_m, water_dist_m, lcc_habitats and mean_ndvi as potential covariates. Before we proceed, it is good practice to convert categorical variables (like feature_type and lcc_habitats) to factors. There is a very easy way to do that using the mutate_if() function of dplyr: locs &lt;- locs %&gt;% mutate_if(is.character,as.factor) # If a column is a character string, make it a factor Run str(locs) again to see what has changed! Before we explore patterns, we need to add the covariates to the response dataframes (e.g. total_obs). We will use left_join(). total_obs &lt;- left_join(total_obs, locs) Lets explore two different types of plot we can make for once particular species, then we will challenge you to explore some relationships of your own. 8.7.1 Continuous predictors The best plot for two continuous predictors is a scatter plot. In base R: plot(data=total_obs, Alces.alces ~ # Y variable line_of_sight_m, # X variable pch=19, las=1 ) We can make the same plot in ggplot: ggplot(data=total_obs, aes(x=line_of_sight_m, y=Alces.alces)) + geom_point() + # Specify a scatter plot theme_classic() # A nice theme The great thing about ggplot is that it is easy to add trend lines: ggplot(data=total_obs, aes(x=line_of_sight_m, y=Alces.alces)) + geom_point() + # Specify a scatter plot theme_classic() + geom_smooth(method=lm, se=T, fullrange=TRUE) # A nice theme What do you think? For more ggplot scatterplot examples (with code) see the R graph gallery - Scatterplots. Let’s checkout another predictor: ggplot(data=total_obs, aes(x=mean_ndvi, y=Alces.alces)) + geom_point() + # Specify a scatter plot theme_classic() + geom_smooth(method=lm, se=T, fullrange=TRUE) # A nice theme 8.7.2 Catagorical predictors For categorical predictors boxplots are very useful! Base R: boxplot(data=total_obs, Alces.alces ~ feature_type) Or if ggplot is your thing: ggplot(total_obs, aes(x=feature_type, y=Alces.alces)) + geom_boxplot()+ theme_classic() For more ggplot boxplot examples (with code) see R Graph Galley - Boxplots. There is some cool stuff in there! 8.7.3 Do your own exploration We will now list some potential relationships in the data, you should decide the best way to explore each one: Wolves (Canis lupus) use locations with longer line_of_sight_m more frequently Caribou (Rangifer tarandus) use locations where the water table is close to the surface (low water_depth_m) White-tailed deer (Odocoileus virginianus) use locations with higher vegetation productivity (mean_ndvi) Lynx (Lynx canadensis) select human use feature types over other feature types Lynx (Lynx canadensis) select locations with higher snowshoe hare (Lepus americanus) activity Can you find any evidence to support these? Are there any other things that interest you? Remember we have the following species: ## [1] &quot;Cervus.canadensis&quot; &quot;Lontra.canadensis&quot; ## [3] &quot;Oryctolagus.cuniculus&quot; &quot;Canis.latrans&quot; ## [5] &quot;Vulpes.vulpes&quot; &quot;Martes.americana&quot; ## [7] &quot;Tamiasciurus.hudsonicus&quot; &quot;Lynx.canadensis&quot; ## [9] &quot;Canis.lupus&quot; &quot;Rangifer.tarandus&quot; ## [11] &quot;Alces.alces&quot; &quot;Ursus.americanus&quot; ## [13] &quot;Lepus.americanus&quot; &quot;Odocoileus.virginianus&quot; And the following covariates: ## [1] &quot;feature_type&quot; &quot;line_of_sight_m&quot; &quot;water_depth_m&quot; &quot;elevation&quot; ## [5] &quot;road_dist_m&quot; &quot;water_dist_m&quot; &quot;mean_ndvi&quot; "],["composition.html", "Chapter 9 Community composition 9.1 Observed richness 9.2 Estimated richness 9.3 Sampling-unit-based accumulation curves 9.4 Basic results plot 9.5 Other diversity metrics 9.6 Community structure", " Chapter 9 Community composition By Christopher Beirne and Laura Stewart One of the most fundamental questions researchers and practitioners want to answer is how many species are there in my survey area?. Exploring patterns in species richness can also tell us if we have performed ‘enough’ surveying. Create a new .R script Call it 04_example_richness.R. Load the required packages # Check you have them and load them list.of.packages &lt;- c(&quot;iNEXT&quot;, &quot;kableExtra&quot;, &quot;tidyr&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;dplyr&quot;, &quot;viridis&quot;) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) 9.1 Observed richness The simplest way to quantify species richness is counting the number of species you detect on your camera traps - ‘observed richness’. This is very easy to determine using our species list: sp_summary &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_species_list.csv&quot;, header=T) # Use nrow() to count the number of species nrow(sp_summary) ## [1] 14 In the case of the example data set, this represents 14 mammal species. class order family genus species sp common_name mass_g act_noct act_crep act_diur Mammalia Artiodactyla Cervidae Alces alces Alces.alces moose 356998.16 1 1 0 Mammalia Artiodactyla Cervidae Cervus canadensis Cervus.canadensis elk 165015.85 1 1 0 Mammalia Artiodactyla Cervidae Odocoileus virginianus Odocoileus.virginianus white-tailed deer 55508.56 1 1 0 Mammalia Artiodactyla Cervidae Rangifer tarandus Rangifer.tarandus caribou 86033.98 0 0 1 Mammalia Carnivora Canidae Canis latrans Canis.latrans coyote 13406.33 1 1 0 Mammalia Carnivora Canidae Canis lupus Canis.lupus gray wolf 32183.33 1 1 0 Mammalia Carnivora Canidae Vulpes vulpes Vulpes.vulpes red fox 5476.17 1 1 0 Mammalia Carnivora Felidae Lynx canadensis Lynx.canadensis canada lynx 9373.25 1 0 0 Mammalia Carnivora Mustelidae Lontra canadensis Lontra.canadensis river otter 8087.42 1 1 0 Mammalia Carnivora Mustelidae Martes americana Martes.americana american marten 1250.00 1 0 0 Mammalia Carnivora Ursidae Ursus americanus Ursus.americanus black bear 99949.36 1 0 0 Mammalia Lagomorpha Leporidae Lepus americanus Lepus.americanus snowshoe hare 1710.02 1 0 0 Mammalia Lagomorpha Leporidae Oryctolagus cuniculus Oryctolagus.cuniculus rabbit 1832.22 1 0 0 Mammalia Rodentia Sciuridae Tamiasciurus hudsonicus Tamiasciurus.hudsonicus red squirrel 201.17 0 0 1 It is possible to compare observed richness across different strata of interest, however survey effort must be identical between your comparison strata. This very rarely the case in camera trap studies where cameras break, run out of battery or are deployed for different lengths of time. The number of species you detect is a function of the amount of effort you spent surveying/the number of individuals detected - the longer a camera is active/the more individuals detected, the more species it will detect. What this means is, unless you saturate a landscape with camera traps, observed richness will underestimate true richness. Consequently, We need ways of comparing species richness which accounts in some way for survey effort. 9.2 Estimated richness There are two commonly used ways to account for survey effort when estimating species richness using camera traps: using the incidence of rare species to ‘correct’ observed richness (iNext) using multispecies occupancy models to account for the species present but not observed (occupancy model) 9.2.1 iNext package The iNext package (INterpolation and EXTrapolation of species richness) - is both easy to use and rapid to compute. It also comes with a wealth of plotting functions - see the iNext Quick Introduction for a great walk through tutorial. Its core functionality is based on: Chao, Anne, et al. “Rarefaction and extrapolation with Hill numbers: a framework for sampling and estimation in species diversity studies.” Ecological monographs 84.1 (2014): 45-67. Which has, to date, been cited &gt;2000 times! To run this example code you will need to load the iNEXT , ggplot2, and gridExtra packages. library(iNEXT); library(ggplot2); library(gridExtra) Single strata You may want to see if your camera project has sufficient survey effort to capture the species within the area of interest. To do this we can compute a species accumulation curves across the site as a whole. Species accumulation curves plot the increase in species richness as we add survey units. If the curve plateaus (flattens), then that suggests you have sampled the majority of the species in your survey area. 9.3 Sampling-unit-based accumulation curves In camera trap projects we typically think about our survey effort in terms of the number of camera stations we deploy on the landscape or the units of time they are active (e.g. camera days). Performing our species accumulation curves using survey location allows us to determine if we have enough survey locations in a given strata to detect all of the species present. Repeating the analyses using camera days would also give insight into whether we need more survey effort in a given location. Data formatting The data formatting for a sampling-unit based accumulation curve is as follows: we need to create a list object with each strata as elements in that list. Next we nest a vector of numbers within each element, the first represents the number of sampling units surveyed, then the number of those units where each given species was detected following it. The example that comes with the iNext package looks like this. The yellow number is the total number of survey units in each location, the red numbers are the number of sites in which each species occurs. We can create this format from the total observations file: total_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_total_observations.csv&quot;, header=T) inc_dat &lt;- total_obs %&gt;% mutate(across(sp_summary$sp, ~+as.logical(.x))) # Turn species counts into 0&#39;s and 1&#39;s # Make an empty list to store our data project_level &lt;- list() # # Sum all of the observations of each species (colSums), and then make it an element within the project_level list project_level[[1]] &lt;- c(nrow(inc_dat), # First count the number of stations # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) # # Give it a name names(project_level) &lt;- &quot;project_level&quot; This produces a list object which looks like this: ## $project_level ## Alces.alces Ursus.americanus ## 38 34 31 ## Odocoileus.virginianus Lynx.canadensis Rangifer.tarandus ## 27 21 20 ## Lepus.americanus Canis.lupus Martes.americana ## 19 17 13 ## Vulpes.vulpes Tamiasciurus.hudsonicus Canis.latrans ## 6 6 3 ## Oryctolagus.cuniculus Cervus.canadensis Lontra.canadensis ## 2 1 1 And let’s run our iNext model: out &lt;- iNEXT(project_level, # The data frame q=0, # The type of diversity estimator (see discussion of the options below) datatype=&quot;incidence_freq&quot;, # The type of analysis knots=40, # The number of data points in your line (more = smoother) se=TRUE, # Logical statement if you want confidence intervals conf=0.95, # The level of confidence intervals nboot=50) # The number of replications to perform - this generates your confidence interval - the bigger the number the longer the run time a note on q values The iNEXT package uses the concept of hill numbers to calculate its community indices. The q values reflect traditional diversity estimators: 0 = species richness 1 = Shannon diversity 2 = Simpson diversity They differ in the weighting of rare species. 0 treats the ‘value’ of every species equally, rare or common. As the the q value increases, the influence of rare species becomes weaker and weaker. a note on coverage Whilst many users will be familiar with diversity indices, iNEXT also calculates ‘sample coverage’ - the proportion of the total number of individuals that belong to the species detected in the sample. The way to conceptualize this is - if you add an un-surveyed individual to the surveyed population, what is the likelihood it belongs to the species not already detected? If your sample coverage is high, this probability will be very low! We will start with observed richness. The iNEXT() function returns the “iNEXT” object including three output lists: - $DataInfo for summarizing data information - $iNextEst for showing size- and coverage-based diversity estimates along with related statistics for a series of rarefied and extrapolated samples - $AsyEst for showing asymptotic diversity estimates along with related statistics. out Lets check out each one in turn: $DataInfo is shown below, returns summary data such as the reference sample size (n), observed species richness (S.obs - which is hopefully the same as what we calculated above), sample coverage estimate for the reference sample (SC), and the first ten frequency counts (f1‐f10). $iNextEst output includes two data frames: $size_based and $coverage_based. Let’s first look at $iNextEst$size_based: Next $iNextEst$coverage_based: $AsyEst gives the asymptotic estimates and their related statistics. One of the powerful elements of iNEXT is that it can extrapolate beyond your data, this is very useful when you do not have equal sample sizes. 9.4 Basic results plot p1 &lt;- ggiNEXT(out, type=1)+ theme_classic() + # type 1 = the diversity estimator labs(x = &quot;Survey sites&quot;, y = &quot;Richness&quot;) p2 &lt;- ggiNEXT(out, type=2)+ theme_classic() + # type 2 = the survey coverage labs(x = &quot;Survey sites&quot;) grid.arrange(p1, p2, nrow = 1) Multiple strata The iNEXT package gets really interesting when we start to compare multiple different strata. e.g. different treatment types or species groupings. The code to build a multi-strata comparison is very similar to that of a single strata, except now you separate the observations into their relevant categories/strata. We will compare the different categories using the feature_type column in the covariate file. We match the ‘placenames’ in our locations dataframe with the corresponding capture data in total_obs using the %in% command. # Read in the locations data frame locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations_and_covariates.csv&quot;) # We first want to create a data subset for each of the strata we are interested in: # The treatment types for each Deployment.Location.ID are in the sta file # Make an object containing all of the site ID&#39;s for the &quot;Offline&quot; cameras off &lt;- locs$placename[locs$feature_type==&quot;Offline&quot;] # And &quot;HumanUse&quot; cameras hum &lt;- locs$placename[locs$feature_type==&quot;HumanUse&quot;] # Create a new empty list inc_locations &lt;- list() # Only sum the data for each relvent locations inc_locations[[1]] &lt;- c(length(off), # First count the number of stations # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[inc_dat$placename %in% off, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) inc_locations[[2]] &lt;- c(length(hum), # Count the number of stations # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[inc_dat$placename %in% hum, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) # Give them names names(inc_locations) &lt;- c(&quot;Offline&quot;, &quot;HumanUse&quot;) And let’s run our iNext model: out.inc &lt;- iNEXT(inc_locations, q=0, datatype=&quot;incidence_freq&quot;) # Sample‐size‐based R/E curves ggiNEXT(out.inc, type=1, color.var=&quot;Assemblage&quot;) + labs(y=&quot;Richness&quot;, x = &quot;Locations surveyed&quot;) + theme_classic() So it looks like the human use features are more diverse than the offline features. 9.4.1 Sampling duration example If we want to explore the species accumulation patterns as a function of the number of survey duration, we can make use of the ...weekly_observations dataframes. week_obs&lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_weekly_observations.csv&quot;, header=T) # Turn it into binary incidents inc_dat &lt;- week_obs %&gt;% mutate(across(sp_summary$sp, ~+as.logical(.x))) # Create a new empty list inc_time &lt;- list() # Only sum the data for each relevent strata inc_time[[1]] &lt;- c(nrow(inc_dat[inc_dat$placename %in% off,]), # Count the number of weeks we have data for in each strata # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[inc_dat$placename %in% off, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) inc_time[[2]] &lt;- c(nrow(inc_dat[inc_dat$placename %in% hum,]), # Count the number of stations # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[inc_dat$placename %in% hum, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) # Give them names names(inc_time) &lt;- c(&quot;Offline&quot;, &quot;HumanUse&quot;) And run the model: out.inc &lt;- iNEXT(inc_time, q=0, datatype=&quot;incidence_freq&quot;) ## Warning in Fun(x[[i]], q, names(x)[i]): Insufficient data to provide reliable ## estimators and associated s.e. ## Warning in Fun(x[[i]], q, names(x)[i]): Insufficient data to provide reliable ## estimators and associated s.e. # Sample‐size‐based R/E curves ggiNEXT(out.inc, type=1, color.var=&quot;Assemblage&quot;) + labs(y=&quot;Richness&quot;, x = &quot;Camera weeks&quot;) + theme_classic() Which suggests the same pattern! 9.4.2 On your own Simple: Repeat the comparison for all feature types (NetReg, Offline and HumanUse). Advanced: Compare the species accumulate curves at the site level for small (&lt;10 kg) and large mammals (&gt;10kg) # Create a new empty list inc_time &lt;- list() # The treatment types for each Deployment.Location.ID are in the sta file # Make an object containing all of the site ID&#39;s for the &quot;Offline&quot; cameras off &lt;- locs$placename[locs$feature_type==&quot;Offline&quot;] # And &quot;HumanUse&quot; cameras hum &lt;- locs$placename[locs$feature_type==&quot;HumanUse&quot;] regen &lt;- locs$placename[locs$feature_type==&quot;NatRegen&quot;] # Only sum the data for each relevent strata inc_time[[1]] &lt;- c(nrow(inc_dat[inc_dat$placename %in% off,]), # Count the number of weeks we have data for in each strata # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[inc_dat$placename %in% off, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) inc_time[[2]] &lt;- c(nrow(inc_dat[inc_dat$placename %in% hum,]), # Count the number of stations # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[inc_dat$placename %in% hum, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) inc_time[[3]] &lt;- c(nrow(inc_dat[inc_dat$placename %in% regen,]), # Count the number of stations # Then subset the detections to those stations, sum the columns, and sort the incidents inc_dat[inc_dat$placename %in% regen, sp_summary$sp] %&gt;% colSums() %&gt;% sort(decreasing=T)) # Give them names names(inc_time) &lt;- c(&quot;Offline&quot;, &quot;HumanUse&quot;, &quot;NatRegen&quot;) out.inc &lt;- iNEXT(inc_time, q=0, datatype=&quot;incidence_freq&quot;) ## Warning in Fun(x[[i]], q, names(x)[i]): Insufficient data to provide reliable ## estimators and associated s.e. ## Warning in Fun(x[[i]], q, names(x)[i]): Insufficient data to provide reliable ## estimators and associated s.e. ## Warning in Fun(x[[i]], q, names(x)[i]): Insufficient data to provide reliable ## estimators and associated s.e. # Sample‐size‐based R/E curves ggiNEXT(out.inc, type=1, color.var=&quot;Assemblage&quot;) + labs(y=&quot;Richness&quot;, x = &quot;Camera weeks&quot;) + theme_classic() 9.5 Other diversity metrics 9.5.1 Simpson and Shannon One issue with species richness assessments is that they weight all species equally, thus a community with 12 species all present in equal abundances will give you the same richness value as a high skewed community with one highly abundant species, and 11 very rare ones. Consequently, you might want to estimate species diversity. Luckily, the iNEXT package is well suited for comparisons of diversity indices through the use of hill numbers - of which the ‘q’ value represents the traditional Shannon (q=1) and Simpson (q=2) diversity indices (species richness: q = 0). Note Increasing values of q reduces the influence of rare species on your estimate of community diversity. For example, we might want to compare the species diversity across our two focal strata: # We also introduce the object t -&gt; which reflects the range of values over which you want to predict species richness out &lt;- iNEXT(inc_time, q=c(0,1,2) ,datatype=&quot;incidence_freq&quot; ) ggiNEXT(out, type=1, facet.var=&quot;Order.q&quot;, color.var=&quot;Assemblage&quot;) + theme_classic() The plot above shows that the differences between our two strata remain across increasing q values (suggesting that the differences between sites are being driven by several rarely encountered species). Point estimates and their confidence intervals can also be extracted from iNEXT model objects - but it does require a little data wrangling. For example, if we wanted to directly compare the diversity estimates of our strata at 1000 survey units: # To generate predictions for specific amounts of survey effort, we make use of the variable t # T specifies the values you want iNEXt to calculate diversity for out &lt;- iNEXT(inc_time, q=c(0,1,2) ,datatype=&quot;incidence_freq&quot;, size=c(1000)) # The lapply function applies the same logic across elements in a list point_estimate &lt;- out$iNextEst$size_based[out$iNextEst$size_based$t==1000,] point_estimate ## Assemblage t Method Order.q qD qD.LCL qD.UCL ## 1 Offline 1000 Rarefaction 0 9.908397 9.214173 10.602621 ## 5 Offline 1000 Rarefaction 1 5.199616 4.660471 5.738760 ## 9 Offline 1000 Rarefaction 2 3.770656 3.214241 4.327070 ## 13 HumanUse 1000 Rarefaction 0 13.700080 11.834701 15.565459 ## 17 HumanUse 1000 Rarefaction 1 8.526882 7.901720 9.152044 ## 21 HumanUse 1000 Rarefaction 2 7.468354 6.965544 7.971164 ## 28 NatRegen 1000 Extrapolation 0 11.175535 9.552559 12.798511 ## 32 NatRegen 1000 Extrapolation 1 5.404507 4.925905 5.883109 ## 36 NatRegen 1000 Extrapolation 2 4.139836 3.722624 4.557049 ## SC SC.LCL SC.UCL ## 1 0.9942029 0.9911733 0.9972325 ## 5 0.9942029 0.9911733 0.9972325 ## 9 0.9942029 0.9911733 0.9972325 ## 13 0.9925184 0.9872783 0.9977585 ## 17 0.9925184 0.9872783 0.9977585 ## 21 0.9925184 0.9872783 0.9977585 ## 28 0.9982324 0.9944519 1.0000000 ## 32 0.9982324 0.9944519 1.0000000 ## 36 0.9982324 0.9944519 1.0000000 # Make a nice ggplot! ggplot(point_estimate, aes(x=c(-0.2,0.8, 1.8, 0,1,2, 0.2, 1.2, 2.2), y=qD, colour=Assemblage)) + theme_classic() + #scale_x_discrete(breaks=c(&quot;1&quot;,&quot;2&quot;),labels= c(&quot;1&quot;,&quot;2&quot;)) + geom_errorbar(aes(ymin=qD.LCL, ymax=qD.UCL), width=.01) + labs(y=&quot;Diversity&quot;, x = &quot;Diversity at 1000 survey days&quot;) + geom_point() 9.5.2 More examples in the literature Some examples of using iNEXT with camera trap data: Cusack et al. 2015 Random versus Game Trail-Based Camera Trap Placement Strategy for Monitoring Terrestrial Mammal Communities Kays et al. 2020 An empirical evaluation of camera trap study design: How many, how long and when? Semper-Pascual et a. 2018 Mapping extinction debt highlights conservation opportunities for birds and mammals in the South American Chaco Publishing note If you publish your work based on the results from the iNEXT package, you should make references to the following methodology paper (Chao et al. 2014) and the application paper (Hsieh, Ma &amp; Chao, 2016): Chao A, Gotelli NJ, Hsieh TC, Sande EL, Ma KH, Colwell RK, Ellison AM (2014). “Rarefaction and extrapolation with Hill numbers: a framework for sampling and estimation in species diversity studies.” Ecological Monographs, 84, 45–67. Hsieh TC, Ma KH, Chao A (2022). iNEXT: Interpolation and Extrapolation for Species Diversity. R package version 3.0.0, http://chao.stat.nthu.edu.tw/wordpress/software_download/. 9.5.3 Multispecies occupancy model It is also possible to estimate species richness in a given area/strata using multispecies occupancy models. For an example with code in the appendices see: Tobler, M. et al. Spatiotemporal hierarchical modelling of species richness and occupancy using camera trap data. J. Appl. Ecol. (2015). 9.6 Community structure One of the shortfalls in the diversity index approaches is that you can compare two sites with completely different mammal assemblages, but identical diversity estimates! So we would conclude that the two are the same, however,in reality their compositions are totally different. Another way to assess community structure is with ordination methods (e.g non-metric multidimensional scaling or NMDS). For a fantastic (although now somewhat dated) blog on NMDS methods see: Sample(ecology)’s NMDS tutorial in R. Luckily a basic NMDS is very easy to run from our ...total_observations dataframe: #install.packages(&quot;vegan&quot;) library(vegan) # Import your count data total_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_total_observations.csv&quot;, header=T) #Import the location and covariate data locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations_and_covariates.csv&quot;) # Add the covariates to your total_obs dataframe dat &lt;- left_join(total_obs, locs) # Convert to categorical factors dat &lt;- dat %&gt;% mutate_if(is.character,as.factor) # Subset to just the count columns counts &lt;- dat[,sp_summary$sp] # Covert it into a matrix m_counts &lt;- as.matrix(counts) We are now ready to run our NMDS model: set.seed(123) # To make sure we all get the same result # run metaMDS on the count matrix using the &quot; Bray-Curtis dissimilarity&quot; note others are available nmds = metaMDS(m_counts, # The count matrix distance = &quot;bray&quot;, # The method of solving trace=0) # Supress the output - trace=1 is more informative And check the output: nmds ## ## Call: ## metaMDS(comm = m_counts, distance = &quot;bray&quot;, trace = 0) ## ## global Multidimensional Scaling using monoMDS ## ## Data: wisconsin(sqrt(m_counts)) ## Distance: bray ## ## Dimensions: 2 ## Stress: 0.2235068 ## Stress type 1, weak ties ## Best solution was not repeated after 20 tries ## The best solution was from try 0 (metric scaling or null solution) ## Scaling: centring, PC rotation, halfchange scaling ## Species: expanded scores based on &#39;wisconsin(sqrt(m_counts))&#39; 9.6.1 Extracting data for plotting To make a nice plot of the NMDS data we need to learn how to extract the data from it: # Make a dataframe out of the x and Y scores site.scores &lt;- as.data.frame(scores(nmds)$sites) species.scores &lt;- as.data.frame(scores(nmds)$species) # Add in the covariate data #add covariate columns to data frame site.scores$placename &lt;- dat$placename site.scores$feature_type &lt;- dat$feature_type # Assign colors to our feature_types using viridis # then use the turbo() function to assign each level a color col.cat &lt;- cividis(length(levels(dat$feature_type))) # then we apply it to the dataframe dat$colours &lt;- col.cat[dat$feature_type] Lets make a plot in base R using the default plotting functions: par(mfrow=c(1,1)) # Make an empty plot type=&quot;n ordiplot(nmds,type=&quot;n&quot;, las=1, xlim=c(-1.5,1.2)) # Add an elipse corresponding to each site ordiellipse(nmds, groups=dat$feature_type, col=col.cat, lwd=2) # Add the species loadings orditorp(nmds,display=&quot;species&quot;,col=&quot;red&quot;,air=0.5) # Add the site loadings points(site.scores$NMDS1, site.scores$NMDS2, col=dat$colours, pch=19) # Add a legend legend(&quot;topleft&quot;, levels(dat$feature_type), col=col.cat, pch=19 ) The different feature_types to not differ majorly in their species compositions - there is a huge degree of overlap between sites. The NMDS framework is flexible - we can also add environmental covariates using envfit to explain differences we might find. Checkout a great blog on this by Jackie Zorz for more information! 9.6.2 On your own Repeat the comparison for NetRegen and HumanUse feature types. 9.6.3 Examples in the literature Haysom, J. K., Deere, N. J., Wearn, O. R., Mahyudin, A., Jami, J. B., Reynolds, G., &amp; Struebig, M. J. (2021). Life in the Canopy: Using Camera-Traps to Inventory Arboreal Rainforest Mammals in Borneo. Frontiers in Forests and Global Change, 83 Note - they also use iNext! Give that paper a look! "],["habitat-use.html", "Chapter 10 Habitat use 10.1 Calculating capture rate 10.2 Single-species models 10.3 Multispecies models", " Chapter 10 Habitat use Camera traps are well suited for the quantification of habitat use across multiple species. To assess habitat use, we typically quantify the detection rate - the number of detections divided by the time interval of interest. As detection rates are fairly simple to estimate and conceptually simple to understand, thus their use is widespread in the camera trap literature. In its simplest form habitat use represents the number of independent events of a given species at a given camera, divided by the number of days that camera was active during that period of interest. This ‘detection rate’ is thought to reflect the habitat use of a species at a given location. Extreme care should be taken if you want to equate use with abundance or density - something we discuss a little more in the density chapter. Detection rates are typically analysed in a linear modelling framework, and come in single species and multi-species versions (see below). Create a new .R script Call it 05_example_habitat_use.R. Load the required packages # Check you have them and load them list.of.packages &lt;- c(&quot;kableExtra&quot;, &quot;tidyr&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;lme4&quot;, &quot;dplyr&quot;, &quot;Hmsc&quot;, &quot;jtools&quot;, &quot;lubridate&quot;, &quot;corrplot&quot;, &quot;MuMIn&quot;) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) 10.1 Calculating capture rate We will start by using the total_obs dataframe we have used in previous chapters: # Import the total observations dataset total_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_total_observations.csv&quot;, header=T) # Import your species list sp_summary &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_species_list.csv&quot;, header=T) Which, as a quick reminder, looks like this: placename days Alces.alces Canis.latrans Canis.lupus Cervus.canadensis Lepus.americanus Lontra.canadensis Lynx.canadensis Martes.americana Odocoileus.virginianus Oryctolagus.cuniculus Rangifer.tarandus Tamiasciurus.hudsonicus Ursus.americanus Vulpes.vulpes ALG027 358 4 4 13 1 0 0 10 0 3 0 0 0 32 0 ALG029 593 17 0 2 0 0 0 1 0 35 0 0 0 9 0 ALG031 593 11 0 8 0 1 1 0 6 1 1 21 0 2 1 ALG032 592 2 0 0 0 0 0 1 0 2 0 5 0 0 0 ALG035 594 11 0 2 0 7 0 1 0 1 0 6 3 2 0 ALG036 417 1 0 0 0 42 0 1 0 5 0 0 15 0 0 So within each row, we have the location (placename), the survey effort at each given location (camera days), and the number of independent records of each species. This is very close to the format which most linear model analysis packages require. Easy! Our next step to create the capture rate - our proxy for habitat use. We will divide each count by the number of days cameras were active in that location, then multiply by the a standardized number of days - often people use 100. In R this would look like: # Create a dataframe to store these detection rates total_cr &lt;- total_obs # Divide the species abundances (which start in column four), by the amount of camera effort total_cr[ ,sp_summary$sp ] &lt;- (total_cr[ , sp_summary$sp]/total_cr$days)*100 We can then examine the relationship between raw counts (on the x-axis) with our detection rate (on the y-axis), using Odocoileus virginianus as an example. In the plot below each black dot represents a placename where camera trapping has occurred. plot(total_cr$Odocoileus.virginianus ~ total_obs$Odocoileus.virginianus, las=1, pch=19, ylab=&quot;Capture rate per 100 days&quot;, xlab=&quot;Number of independent records&quot;) As you can see they are not a perfect match as the capture rate accounts for the variation in effort between different sites. 10.1.1 Examples from the literature Palmer, Meredith S., et al. “Evaluating relative abundance indices for terrestrial herbivores from large‐scale camera trap surveys.” African journal of ecology 56.4 (2018): 791-803. 10.2 Single-species models The most common way to analyse habitat-use data is through linear models. Linear models typically relate a continuous response variable - in our case capture rate - to a set of one or more discrete or continuous predictor variables. In this simple example we will explore the relationship between the capture rate of a species with the categorical ‘feature_type’ variable and the continuous line_of_sight_m variables. There are a variety if different frameworks to fit and compare different linear models to address a host of different hypotheses, but if you are just starting out you should investigate two widely used packages: lme4 -&gt; frequentest and information theoretic approaches brms -&gt; Bayesian approaches There is no right or wrong about which package and which approach you use to test your hypotheses. Some packages have functionalities that others don’t, which may force your hand. Just make sure you understand the implications of your choices when it comes to reporting your results! 10.2.1 Simple linear model We will start by analyzing a frequentest linear model with a single observation for each camera location. In this worked example we will analyse how habitat use varies using a linear model lm(). The model takes the form: Response term (y) ~ fixed effect 1 (x1) + fixed effect 2 (x2), data frame (data=) It is beyond the scope of this course to test the model assumptions or interrogate the findings, there are better resources to allow you to do that (e.g. we highly recommend reading Gałecki, Andrzej, and Tomasz Burzykowski. “Linear mixed-effects model.” Linear mixed-effects models using R. Springer, New York, NY, 2013. 245-273). In this example we will explore if the habitat use of Odocoileus virginianus varies based on the `feature_type’ the line of sight where it is found. Preparing our data Recall that the information about each location is recorded in the file: locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations_and_covariates.csv&quot;, header=T) # Convert to categorical factors locs &lt;- locs %&gt;% mutate_if(is.character,as.factor) # You should also standardize your covariates - it helps models coverage an facillitates comparison of effects sizes library(MuMIn) z_locs &lt;- stdize(locs) Take a look at it to see what it has done! project_id placename z.longitude z.latitude feature_type z.line_of_sight_m z.water_depth_m z.elevation elev_units z.road_dist_m z.water_dist_m z.mean_ndvi AlgarRestorationProject ALG027 0.3171100 -0.1159006 HumanUse 0.7873181 -0.5761446 1.2075230 meters 0.5055986 1.8459913 1.6001759 AlgarRestorationProject ALG029 -0.7126159 0.6055647 HumanUse 0.4532476 1.2284296 -0.5624619 meters 0.3856670 -0.9846438 -1.4486272 AlgarRestorationProject ALG031 0.2005078 -0.3931695 HumanUse 1.3724162 -0.5761446 1.0108580 meters 0.0984961 1.2910165 0.1278701 AlgarRestorationProject ALG032 1.3727683 0.6897410 HumanUse -0.4355419 -0.5761446 -0.5624619 meters 1.8728210 0.0264221 -0.0569976 AlgarRestorationProject ALG035 0.2815234 0.4837491 HumanUse 2.1302956 -0.5761446 -1.0541243 meters 1.3613604 0.4057329 0.4638086 AlgarRestorationProject ALG036 1.2487630 -1.2925188 NatRegen -0.5816415 -0.4041281 0.6175281 meters -0.0500987 -0.8088957 1.4050838 Each location (placename) has a single row in the dataframe. We will now add our covariates to our capture rate dataframe: mod_dat &lt;- left_join(total_cr, z_locs) # from the dplyr package 10.2.2 Catagorical predictor So we start by exploring the influence of ‘feature_type’ on our response term. What are the categories we have in our feature type variable? table(z_locs$feature_type) ## ## HumanUse NatRegen Offline ## 14 11 13 feature_type is a a categorical variable which reflects strata where the camera trap was deployed: HumanUse = a camera on a seismic line used and maintained in an “open” state by humans Offline = a camera in contiguous forest &gt;200m from a seismic line NatRegen = a seismic line which is naturally regenerating Lets do a quick raw data plot to see what results we might expect: boxplot(mod_dat$Odocoileus.virginianus~mod_dat$feature_type, las=1, xlab=&quot;feature_type&quot;, ylab=&quot;Habitat use&quot;) It looks like white-tailed deer habitat use may be higher in naturally regenerating areas, but there is a lot of overlap between sites. Next we will fit a simple linear model using the `lm()’ function in base R. # model results &lt;- lm( Y data ~ x Data, data= dataframe source) lm_cat &lt;- lm(Odocoileus.virginianus ~ feature_type, data = mod_dat) Lets looks at the model summary: summary(lm_cat) ## ## Call: ## lm(formula = Odocoileus.virginianus ~ feature_type, data = mod_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8377 -2.5572 -0.9958 1.4862 7.4681 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1645 0.8838 1.318 0.1962 ## feature_typeNatRegen 2.6732 1.3324 2.006 0.0526 . ## feature_typeOffline 2.1782 1.2737 1.710 0.0961 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.307 on 35 degrees of freedom ## Multiple R-squared: 0.1208, Adjusted R-squared: 0.07061 ## F-statistic: 2.405 on 2 and 35 DF, p-value: 0.105 Categorical covariates are show as contrasts from the reference level (in this case HumanUse), and the p-value relate to testing whether the other categories are significantly different from the reference level. Other things to note are that our R-squared value (how much variation the model explains) is fairly low - but that is common in camera trap models. We can take a quick look at the predictions using the jtools package. More examples of its use are can be found in the `Visualizing regression model predictions vignette associated with the package. effect_plot(lm_cat, # The model object pred = feature_type, # The variable you want to predict interval = TRUE, # Whether you want confidence intervals (default = 0.95) partial.residuals = T, # Show the residual variation -after accounting for fixed effects y.label = &quot;Habitat use&quot;) # Change the y axis label 10.2.3 Continuous predictor Let’s also explore a continuous predictor `line_of_sight_m’: plot(mod_dat$Odocoileus.virginianus~mod_dat$z.line_of_sight_m, las=1, xlab=&quot;line_of_sight_m&quot;, ylab=&quot;Habitat use&quot;) Next we will fit a simple linear model using the `lm()’ function in base R. # model results &lt;- lm( Y data ~ x Data, data= dataframe source) lm_con &lt;- lm(Odocoileus.virginianus ~ z.line_of_sight_m, data = mod_dat) Lets looks at the model summary: summary(lm_con) ## ## Call: ## lm(formula = Odocoileus.virginianus ~ z.line_of_sight_m, data = mod_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9177 -2.7046 -0.5994 1.6422 7.0377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.6835 0.5157 5.203 8.06e-06 *** ## z.line_of_sight_m -1.3898 0.5227 -2.659 0.0116 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.179 on 36 degrees of freedom ## Multiple R-squared: 0.1642, Adjusted R-squared: 0.141 ## F-statistic: 7.071 on 1 and 36 DF, p-value: 0.01162 Here the effect represents the gradient of the relationship between line_of_sight_m and the habitat use of white-tailed deer. The effect is negative, and the p-value is below the arbitrary 0.05 threshold, which suggests it my be an important predictor of white-tailed deer habitat use. It will make more sense if we plot it - again using jtools effect_plot(lm_con, # The model object pred = z.line_of_sight_m, # The variable you want to predict interval = TRUE, # Whether you want confidence intervals (default = 0.95) partial.residuals = T, # Show the residual variation -after accounting for fixed effects y.label = &quot;Habitat use&quot;) # Change the y axis label 10.2.4 Model comparisons There are times when we may want to compare which model is “the best”, or which model is the most parsimonious. One way to do this is through the use of Information Theory - we can compare which model explains the most amount of variation after applying a penalty for how complex it is (more complex models will always explain more variation, even if just by chance). One useful package for this is the MuMIn package and the function model.sel() for model selection: library(MuMIn) # Lets also create a &quot;null model&quot; something without any predictors in at all, to compare these models to: lm_null &lt;- lm(Odocoileus.virginianus ~ 1, data = mod_dat) Now compare the three alternative models: model.sel(lm_null, lm_cat, lm_con) ## Model selection table ## (Int) ftr_typ z.lin_of_sgh_m df logLik AICc delta weight ## lm_con 2.683 -1.39 3 -96.844 200.4 0.00 0.821 ## lm_cat 1.164 + 4 -97.805 204.8 4.43 0.090 ## lm_null 2.683 2 -100.252 204.8 4.45 0.089 ## Models ranked by AICc(x) Whilst both models improve on the null model, there is stronger support for line_of_sight_m than for our feature types in influencing white-tailed deer habitat use. Cool! 10.2.5 Problems with these models But can you see any problems with this type of model? We probably should be concerned about the fact that: There are negative predictions for both sets of confidence intervals - but you can’t get a negative capture rate! We do not account for seasonality - we saw species detection rates change with time of year in the data exploration section And more besides! 10.2.6 Mixed-effects models Let’s build a more robust habitat-use model which addresses some of the issues highlighted here. To do this we will take advantage of a type of analysis called ‘mixed effects modelling’. Mixed effects models allow us to perform robust analysis of populations which have been repeatedly sampled through time. As such, we can break our data set down into months without violating the assumptions of the models. If you are new to mixed effects models you must try this fantastic interactive aid to help you understand how they work: Michael Freeman’s ‘An Introduction to Hierarchical Modeling’ And for a deep-dive into the inner workings of mixed effects models and their assumptions, see the following paper: Harrison, Xavier A., et al. “A brief introduction to mixed effects modelling and multi-model inference in ecology.” PeerJ 6 (2018): e4794. First we must install the packages we require: ‘lme4’ and `tidyr’: library(lme4); library(tidyr) The lme4 package requires a dataframe format (as above), with the response term and the predictor variables all included in the same location. Second, lets create our monthly analysis dataframe: # Import the total observations dataset monthly_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_monthly_observations.csv&quot;, header=T) Let’s join this dataframe with the location data, as before: mod_dat &lt;- left_join(monthly_obs, z_locs) And let’s do another raw data check: boxplot(mod_dat$Odocoileus.virginianus~mod_dat$feature_type, las=1, xlab=&quot;feature_type&quot;, ylab=&quot;Habitat use&quot;) The patterns are similar to the total_obs dataset, however there is more noise! Now that we have monthly data, we might also want to control for some element of seasonality in our models. We can extract the month from our date column. mod_dat$date &lt;- ym(mod_dat$date) mod_dat$month&lt;- month(mod_dat$date, label=T) Next we will fit a mixed effects model to this data set using lme4. You may have noticed that we haven’t calculated a separate capture rate dataframe as we did in the simple example! That is because we can create a relative abundance index within the model itself by providing an offset() term . An offset term serves to scale the response term based on the amount of survey effort, and preserves the original units of the observations (counts). The model takes the form: Response term ~ fixed effect + offset() + (1|random intercept), data frame, distribution We include placename as the random intercept, as camera locations are repeatedly sampled at monthly intervals and thus our data (rows in the dataframe) are not independent. We use the poisson family, as our response term is a count. glmm_cat &lt;- glmer(Odocoileus.virginianus ~ feature_type + month + offset(log(days)) + (1|placename) , data=mod_dat, family=&quot;poisson&quot;) Oh that warning doesn’t look friendly. Convergence errors often arise as when we fit a model which is too complicated! It basically says the the model hasn’t completely “solved” the parameters within it. In it’s current form the model is estimating and effect for every single month of the year, that’s a lot of thing to estimate with such a small dataset. Let’s simplify things to summer and winter! # Lets create a new column and give it the value summer mod_dat$season &lt;- &quot;summer&quot; mod_dat$season[month(mod_dat$date) %in% c(10,11,12,1,2,3)] &lt;- &quot;winter&quot; # make it a factor factors mod_dat &lt;- mod_dat %&gt;% mutate_if(is.character,as.factor) And re-run our model - also this time with a negative binomial distribution to account for excess zeros: glmm_cat &lt;- glmer.nb(Odocoileus.virginianus ~ feature_type + season + offset(log(days)) + (1|placename) , data=mod_dat) We can view a summary of the model fit using: summary(glmm_cat) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: Negative Binomial(0.8362) ( log ) ## Formula: Odocoileus.virginianus ~ feature_type + season + offset(log(days)) + ## (1 | placename) ## Data: mod_dat ## ## AIC BIC logLik deviance df.resid ## 1337.6 1364.9 -662.8 1325.6 691 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.8336 -0.4267 -0.1919 -0.1210 6.5197 ## ## Random effects: ## Groups Name Variance Std.Dev. ## placename (Intercept) 3.758 1.939 ## Number of obs: 697, groups: placename, 38 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.7986 0.6205 -9.345 &lt;2e-16 *** ## feature_typeNatRegen 1.9662 0.8612 2.283 0.0224 * ## feature_typeOffline 1.0860 0.8379 1.296 0.1950 ## seasonwinter -0.4427 0.1566 -2.827 0.0047 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ftr_NR ftr_tO ## ftr_typNtRg -0.695 ## ftr_typOffl -0.694 0.497 ## seasonwintr -0.073 -0.011 -0.008 We can plot the predictions from these models using the jtools package. First lets look at the effects of feature_type: effect_plot(glmm_cat, pred = feature_type, interval = TRUE, y.label = &quot;Habitat use&quot;, , data=mod_dat) ## Outcome is based on a total of 1 exposures ## Confidence intervals for merMod models is an experimental feature. The ## intervals reflect only the variance of the fixed effects, not the random ## effects. As with our simple linear model, the mixed effects model also suggests a difference between the different feature_type strata for white-tailed deer. Lets also look at the effect of our new season variable: effect_plot(glmm_cat, pred = season, interval = TRUE, y.label = &quot;Habitat use&quot;, , data=mod_dat) Which suggests habitat use is slightly lower habitat use in winter then in summer. 10.2.7 On your own Using this code as a scaffold, explore some more of the patterns we explored in the data exploration chapter. Remember we have the following species: ## [1] &quot;Alces.alces&quot; &quot;Cervus.canadensis&quot; ## [3] &quot;Odocoileus.virginianus&quot; &quot;Rangifer.tarandus&quot; ## [5] &quot;Canis.latrans&quot; &quot;Canis.lupus&quot; ## [7] &quot;Vulpes.vulpes&quot; &quot;Lynx.canadensis&quot; ## [9] &quot;Lontra.canadensis&quot; &quot;Martes.americana&quot; ## [11] &quot;Ursus.americanus&quot; &quot;Lepus.americanus&quot; ## [13] &quot;Oryctolagus.cuniculus&quot; &quot;Tamiasciurus.hudsonicus&quot; And the following covariates: ## [1] &quot;feature_type&quot; &quot;z.line_of_sight_m&quot; &quot;z.water_depth_m&quot; ## [4] &quot;z.elevation&quot; &quot;z.road_dist_m&quot; &quot;z.water_dist_m&quot; ## [7] &quot;z.mean_ndvi&quot; A note of caution As stated at the start of this guide, we are not focusing on whether the models we apply are appropriate or finding “the best” models for this datasheet, so do not spend too much time trying to interpret this information! 10.2.8 Advanced mixed-model predictions Tools such as jtools are great for generating simple predictions from mixed models, however the more complex the models get, the more you may want to specify your own prediction dataframes. If you want more applied examples of generating predictions from mixed effects models, check out Ben Bolkers workbook. There is also some great discussion about model selection and r-squared values. 10.2.9 Examples in the literature Tattersall, E. R., Burgar, J. M., Fisher, J. T., &amp; Burton, A. C. (2020). Mammal seismic line use varies with restoration: Applying habitat restoration to species at risk conservation in a working landscape. Biological Conservation, 241, 108295. 10.3 Multispecies models In the above examples, we analyse each individual species separately. This is great if you only care about one species, however we often want a more holistic understanding of wildlife communities! Recent advances in computer power and analytic approaches mean it is becoming increasingly popular to model multiple species within the same framework! This opens up a variety of things not previously possible. A note of caution In experimenting with single species models you may have realized it can sometimes be hard to build a sensible and robust model. Now do this for &gt;10 species in the same model, and the potential to get silly results increases. Tread carefully! As with single species linear models, there are many choices available for modeling multiple species in the same framework. Two notable options are: GJAM HMSc In this example we will use the Hmsc package. library(Hmsc) Preparing our data The format of data required for joint species distribution models is very similar to the data required for single species models. However, rather than storing the response term and fixed effects within the the same data frame (as with mod_dat above), we need a separate Y matrix of site_time x species, and a separate Xdata dataframe containing the fixed and random effects. # Pull the count data into its own matrix Y &lt;- as.matrix(monthly_obs[,sp_summary$sp]) # Give the row names a useful label, in this case the site_date values # (just in case you want to check things) row.names(Y) &lt;- paste(monthly_obs$placename, monthly_obs$date, sep=&quot;_&quot;) Which looks like this: Alces.alces Cervus.canadensis Odocoileus.virginianus Rangifer.tarandus Canis.latrans Canis.lupus Vulpes.vulpes Lynx.canadensis Lontra.canadensis Martes.americana Ursus.americanus Lepus.americanus Oryctolagus.cuniculus Tamiasciurus.hudsonicus ALG027_2018-04 0 0 0 0 0 2 0 2 0 0 1 0 0 0 ALG027_2018-05 0 0 0 0 0 2 0 0 0 0 9 0 0 0 ALG027_2018-06 1 0 1 0 0 0 0 2 0 0 12 0 0 0 ALG027_2018-07 1 0 1 0 0 1 0 1 0 0 5 0 0 0 ALG027_2018-08 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ALG027_2018-09 0 0 0 0 0 1 0 1 0 0 4 0 0 0 We then create the XData in a similar way to before, but this time dropping the species information: Xdat &lt;- left_join(monthly_obs[c(&quot;placename&quot;, &quot;date&quot;, &quot;days&quot;)], z_locs) # All XData must be numeric or factors, so lets check what we have Whose output looks like this: placename date days project_id z.longitude z.latitude feature_type z.line_of_sight_m z.water_depth_m z.elevation elev_units z.road_dist_m z.water_dist_m z.mean_ndvi ALG027 2018-04 20 AlgarRestorationProject 0.31711 -0.1159006 HumanUse 0.7873181 -0.5761446 1.207523 meters 0.5055986 1.845991 1.600176 ALG027 2018-05 31 AlgarRestorationProject 0.31711 -0.1159006 HumanUse 0.7873181 -0.5761446 1.207523 meters 0.5055986 1.845991 1.600176 ALG027 2018-06 30 AlgarRestorationProject 0.31711 -0.1159006 HumanUse 0.7873181 -0.5761446 1.207523 meters 0.5055986 1.845991 1.600176 ALG027 2018-07 31 AlgarRestorationProject 0.31711 -0.1159006 HumanUse 0.7873181 -0.5761446 1.207523 meters 0.5055986 1.845991 1.600176 ALG027 2018-08 31 AlgarRestorationProject 0.31711 -0.1159006 HumanUse 0.7873181 -0.5761446 1.207523 meters 0.5055986 1.845991 1.600176 ALG027 2018-09 30 AlgarRestorationProject 0.31711 -0.1159006 HumanUse 0.7873181 -0.5761446 1.207523 meters 0.5055986 1.845991 1.600176 With Bayesian approaches we need to set up our sampling conditions nChains = 2 # How many total repeats to run thin = 5 # How often to thin the samples samples = 100 # How many samples to take transient = 1000 # How long should the &quot;warm up&quot; be verbose = T # Give reports on model progress Setup our random effect: # Add a station-level random effect (for the co-variances) studyDesign = data.frame(station = as.factor(Xdat$placename)) rL = HmscRandomLevel(units = studyDesign$station) Specify our model”” # Model specification mod &lt;- Hmsc(Y = Y, XData = Xdat[,c(&quot;z.line_of_sight_m&quot;, &quot;z.water_depth_m&quot;, &quot;days&quot;)], XFormula = ~z.line_of_sight_m + z.water_depth_m + log(days), studyDesign = studyDesign, ranLevels = list(station = rL), distr=&quot;poisson&quot;) And fit the model: out &lt;- sampleMcmc(mod, thin = thin, samples = samples, transient = transient, nChains = nChains, verbose = verbose) We can plot a basic summary of the modeled effects using the following code. postBeta = getPostEstimate(out, parName = &quot;Beta&quot;) par(mar=c(8,12,1,1)) plotBeta(out, post = postBeta, param = &quot;Support&quot;, supportLevel = 0.95) We the colors denote the size and magnitude of the effect of proportion of lowland habitat. NOTE treat these results with caution as the number of model runs is very low (to increase speed) and the model assumptions have not been interrogated. OmegaCor = computeAssociations(out) supportLevel = 0.0 toPlot = ((OmegaCor[[1]]$support&gt;supportLevel) + (OmegaCor[[1]]$support&lt;(1-supportLevel))&gt;0)*OmegaCor[[1]]$mean corrplot(toPlot, method = &quot;color&quot;, type=&quot;upper&quot;, order = &quot;FPC&quot;, col = colorRampPalette(c(&quot;blue&quot;,&quot;white&quot;,&quot;red&quot;))(200), title = paste(&quot;random effect level:&quot;, mod$rLNames[1]), mar=c(0,0,1,0)) 10.3.1 Potential dangers The analysis has worked and we have some really stylish output! But - take screenshots of the output and run it again. Compare your screen shots. Bayesian solvers don’t work the same way as frequentist approaches. With frequentist approaches you get the same result every time, with bayesian approaches a solver explores the parameter space to “find” the right solution. If you do not give time for the solver to coverage on the right solution, you will get a result that is not in the slightest bit reliable! For a nice overview on assessing Bayesian model convergence see Michael Clark’s bayseian model diagnostics page. Let’s have a look at our traceplots - these are plots which show the Bayesian solvers efforts to converge on the answer for each parameter with each iteration of the model (red and black done the different runs). If they have converged on a solution they should be steady and stable, the coloured lines on the left should overlap and the density plot on the right should be uni-modal. First for the fixed effects in the model: mpost = convertToCodaObject(out) plot(mpost$Beta) What do you think? These sampling chains will have to be much longer for these models to converge! 10.3.2 Further reading The best place for examples of HMSC analyses right now are package vignettes: Getting started with HMSC-R: univariate models Getting started with HMSC-R: low-dimensional multivariate models Getting started with HMSC-R: high-dimensional multivariate models Getting started with HMSC-R: spatial models 10.3.3 Examples in the literature Carvalho Jr, Elildo AR, et al. “Effects of illegal logging on Amazonian medium and large-sized terrestrial vertebrates.” Forest Ecology and Management 466 (2020): 118105. Beirne, Christopher, et al. “Multispecies modelling reveals potential for habitat restoration to re‐establish boreal vertebrate community dynamics.” Journal of Applied Ecology 58.12 (2021): 2821-2832. "],["occupancy.html", "Chapter 11 Occupancy 11.1 Single species occupancy model 11.2 Spatial occupancy model: spOccupancy", " Chapter 11 Occupancy Occupancy modelling has been one of the mainstays of camera traps data analysis for many years, so learning how to wangle our data into occupancy-style formats is essential. When we survey wild and free ranging populations using any sampling methodology, the probability of detecting a given individual or species if it is actually present on the landscape at the time of sampling is typically less than one. This is because wild animals are often hard to see! This issue is termed “imperfect detection”. In order to deal with the imperfect detection issue - occupancy models separate our the counts of a given species at a site into two processes: occupancy (ψ) - which is the probability of a species occurring within a spatial unit (or “site”) during the sampling session detection probability (p) - the probability that the species will be detected given that it already occurs at a site In order to separate out the occupancy process from the detection process, surveys need to occur at replicated ‘sites’ and we need repeated ‘visits’ to the same site. It is important to know that in camera trap studies, practitioners typically treat individual locations as sites and rather than repeated return to a location to survey it at different times, they divide the continuous camera activity data into block of time (e.g. 1 to 7 day windows). Occupancy models were not developed specifically for camera traps - thus there are a suite of assumptions we need to make about the populations we survey when applying occupancy models. We do not adress these here. However, below we provide a list introductory resources for you to dig into the occupancy models to decide if they are appropriate for your situation: Burton, A. Cole, et al. “Wildlife camera trapping: a review and recommendations for linking surveys to ecological processes.” Journal of Applied Ecology 52.3 (2015): 675-685. MacKenzie, Darryl I., et al. Occupancy estimation and modeling: inferring patterns and dynamics of species occurrence. Elsevier, 2017. Let’s focus our time on getting our data into the right formt, and applying some occupancy models! # Check you have them and load them list.of.packages &lt;- c(&quot;kableExtra&quot;, &quot;tidyr&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;dplyr&quot;, &quot;unmarked&quot;, &quot;lubridate&quot;, &quot;tibble&quot;, &quot;sf&quot;, &quot;gfcanalysis&quot;, &quot;MuMIn&quot;, &quot;spOccupancy&quot;) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) 11.1 Single species occupancy model In this example we will use the ...weekly_observations dataframe we created in the data creation section. We do this because 7 days is a time interval which occupancy models are often devided into for occupancy analyses. # Import the weekly observations data set week_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_weekly_observations.csv&quot;, header=T) Which, as a quick reminder, looks like this: placename date days Alces.alces Canis.latrans Canis.lupus Cervus.canadensis Lepus.americanus Lontra.canadensis Lynx.canadensis Martes.americana Odocoileus.virginianus Oryctolagus.cuniculus Rangifer.tarandus Tamiasciurus.hudsonicus Ursus.americanus Vulpes.vulpes ALG027 2018-W14 4 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG027 2018-W15 7 0 0 1 0 0 0 1 0 0 0 0 0 0 0 ALG027 2018-W16 7 0 0 1 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W17 7 0 0 1 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 3 0 ALG027 2018-W21 7 0 0 1 0 0 0 0 0 0 0 0 0 8 0 ALG027 2018-W22 7 0 0 0 0 0 0 1 0 0 0 0 0 3 0 ALG027 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG027 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W25 7 1 0 0 0 0 0 1 0 1 0 0 0 1 0 ALG027 2018-W26 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG027 2018-W27 7 0 0 1 0 0 0 1 0 0 0 0 0 1 0 ALG027 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG027 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG027 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W34 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG027 2018-W37 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG027 2018-W38 7 0 0 1 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W39 7 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG027 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W41 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W42 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W45 7 0 0 1 0 0 0 0 0 1 0 0 0 0 0 ALG027 2018-W46 7 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W47 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W49 7 0 0 2 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W01 7 0 2 1 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W04 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W06 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W07 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG027 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG027 2019-W11 7 0 0 0 0 0 0 2 0 0 0 0 0 0 0 ALG027 2019-W12 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG027 2019-W13 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W16 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W18 7 3 0 0 0 0 0 0 0 2 0 0 0 1 0 ALG029 2018-W19 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W20 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W25 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG029 2018-W26 7 2 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG029 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W37 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W39 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W44 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W45 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W46 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG029 2018-W47 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W48 7 0 0 0 0 0 0 1 0 1 0 0 0 0 0 ALG029 2018-W49 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W50 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W06 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W14 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG029 2019-W15 7 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG029 2019-W16 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W18 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W19 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W20 7 0 0 0 0 0 0 0 0 4 0 0 0 0 0 ALG029 2019-W21 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG029 2019-W23 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG029 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W33 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG029 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W36 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG029 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG029 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG029 2019-W42 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W43 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG029 2019-W44 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W45 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG029 2019-W46 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W14 7 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ALG031 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W16 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W17 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W18 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W20 7 1 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG031 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W24 7 1 0 0 0 0 0 0 1 0 0 2 0 0 0 ALG031 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W26 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W27 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W37 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG031 2018-W38 7 0 0 1 0 0 0 0 0 0 0 1 0 0 0 ALG031 2018-W39 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG031 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ALG031 2018-W41 7 1 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG031 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W51 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W11 7 0 0 0 0 0 0 0 2 0 0 0 0 0 0 ALG031 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W13 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W15 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W16 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W17 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W21 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG031 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W23 7 1 0 0 0 0 0 0 0 1 0 1 0 0 0 ALG031 2019-W24 7 0 0 1 0 0 0 0 0 0 1 1 0 0 0 ALG031 2019-W25 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG031 2019-W26 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG031 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W28 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG031 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG031 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W31 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG031 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W36 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG031 2019-W37 7 0 0 0 0 0 0 0 0 0 0 3 0 0 0 ALG031 2019-W38 7 0 0 0 0 0 0 0 0 0 0 3 0 0 0 ALG031 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W41 7 0 0 0 0 0 0 0 0 0 0 1 0 1 0 ALG031 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG031 2019-W46 5 0 0 0 0 1 0 0 1 0 0 0 0 0 0 ALG032 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W22 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG032 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W26 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W28 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG032 2018-W29 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG032 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W44 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG032 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W46 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG032 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W29 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG032 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W36 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG032 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W43 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG032 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 2019-W46 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W15 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG035 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W17 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W18 7 2 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W23 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W24 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W38 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG035 2018-W39 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG035 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W49 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG035 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2018-W52 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG035 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W10 7 0 0 0 0 4 0 0 0 0 0 0 0 0 0 ALG035 2019-W11 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG035 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W15 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W21 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W22 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG035 2019-W23 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W25 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W27 7 1 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG035 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG035 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W31 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 3 1 0 ALG035 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W37 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG035 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W39 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG035 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W42 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG035 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 2019-W46 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W14 6 0 0 0 0 1 0 0 0 0 0 0 1 0 0 ALG036 2018-W15 7 0 0 0 0 5 0 0 0 0 0 0 0 0 0 ALG036 2018-W16 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG036 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W20 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W45 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W46 7 0 0 0 0 2 0 0 0 1 0 0 0 0 0 ALG036 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W49 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG036 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W02 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG036 2019-W03 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG036 2019-W04 7 0 0 0 0 1 0 0 0 0 0 0 1 0 0 ALG036 2019-W05 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG036 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W10 7 0 0 0 0 2 0 0 0 0 0 0 1 0 0 ALG036 2019-W11 7 0 0 0 0 10 0 0 0 0 0 0 2 0 0 ALG036 2019-W12 7 0 0 0 0 5 0 0 0 0 0 0 1 0 0 ALG036 2019-W13 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG036 2019-W14 7 0 0 0 0 3 0 0 0 0 0 0 2 0 0 ALG036 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W16 7 0 0 0 0 3 0 0 0 0 0 0 0 0 0 ALG036 2019-W17 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG036 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W20 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG036 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG036 2019-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG036 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG036 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W31 7 0 0 0 0 0 0 0 0 1 0 0 1 0 0 ALG036 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG036 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 2 0 0 ALG036 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG036 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG036 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 2019-W46 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W14 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W41 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W43 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W47 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG037 2018-W48 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG037 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W19 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG037 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG037 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG037 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 2019-W46 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG038 2018-W22 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W35 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2018-W36 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2018-W51 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG038 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W16 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W20 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W26 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2019-W27 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2019-W28 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2019-W29 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG038 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W32 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG038 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 2019-W46 5 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG039 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W21 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG039 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W36 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W45 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W49 7 0 0 2 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W50 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W11 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG039 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG039 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W22 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG039 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W35 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG039 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W40 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG039 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W45 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 2019-W46 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W16 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W45 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2018-W46 7 0 0 0 0 0 0 0 0 5 0 0 0 0 0 ALG043 2018-W47 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2018-W51 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG043 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W05 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG043 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W11 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG043 2019-W12 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG043 2019-W13 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W15 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W16 7 1 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG043 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W18 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG043 2019-W19 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG043 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W21 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W22 7 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG043 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W24 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG043 2019-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W31 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W33 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG043 2019-W34 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W36 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG043 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W39 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 2019-W42 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W43 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W44 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG043 2019-W45 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG043 2019-W46 4 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG044 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W14 7 1 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG044 2018-W15 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG044 2018-W16 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W45 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W46 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG044 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W05 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W11 7 0 0 2 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ALG044 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W19 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W20 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG044 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W22 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG044 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG044 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG044 2019-W28 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W29 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W30 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG044 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W32 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W34 7 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG044 2019-W35 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W36 7 2 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG044 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG044 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W39 7 1 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG044 2019-W40 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W41 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W42 7 1 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG044 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG044 2019-W44 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG044 2019-W45 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG044 2019-W46 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG045 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W15 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG045 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W17 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W18 7 0 0 2 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W20 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W45 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W47 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG045 2018-W48 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG045 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W01 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W02 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W03 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W08 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG045 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W13 7 1 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG045 2019-W14 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG045 2019-W15 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG045 2019-W16 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG045 2019-W17 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG045 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W19 7 0 0 1 0 0 0 0 0 0 0 1 0 2 0 ALG045 2019-W20 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG045 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG045 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG045 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W31 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W33 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG045 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG045 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG045 2019-W46 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W14 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG046 2018-W15 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG046 2018-W16 7 0 0 0 0 1 0 0 0 0 0 0 0 0 1 ALG046 2018-W17 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W18 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W22 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2018-W27 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W28 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG046 2018-W29 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG046 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W31 7 2 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2018-W32 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W10 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG046 2019-W11 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG046 2019-W12 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG046 2019-W13 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG046 2019-W14 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG046 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W16 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG046 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W18 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2019-W19 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2019-W20 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W26 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2019-W28 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W29 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W30 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG046 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W32 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG046 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W38 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W43 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG046 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 2019-W46 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W16 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W17 7 0 0 3 0 0 0 0 0 0 0 0 0 1 0 ALG047 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 3 0 ALG047 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG047 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG047 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG047 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W26 7 0 0 0 0 0 0 0 0 3 0 0 0 2 0 ALG047 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG047 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W31 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG047 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W34 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG047 2018-W35 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG047 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W37 7 0 0 0 0 0 0 0 0 0 0 3 0 0 0 ALG047 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W47 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W51 7 0 1 3 0 0 0 0 0 0 0 0 0 0 0 ALG047 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W02 7 0 0 2 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W04 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W07 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG047 2019-W08 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W12 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG047 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W17 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG047 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG047 2019-W19 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG047 2019-W20 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W25 7 1 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG047 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W28 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG047 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W32 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG047 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 2019-W34 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W25 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG048 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG048 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG048 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W19 7 0 0 0 0 0 0 0 0 0 0 1 0 1 0 ALG048 2019-W20 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG048 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG048 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W29 7 0 0 0 0 0 0 0 0 0 0 3 0 0 0 ALG048 2019-W30 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG048 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W37 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG048 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W40 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W44 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG048 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG048 2019-W46 5 0 0 1 0 0 0 0 0 1 0 0 0 0 0 ALG049 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W16 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W45 3 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG049 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W00 5 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W03 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W06 7 0 0 0 0 0 0 0 1 1 0 0 0 0 0 ALG049 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W09 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG049 2019-W11 7 0 0 0 0 0 0 0 0 1 0 0 1 0 0 ALG049 2019-W12 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W13 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W14 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W17 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W18 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG049 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG049 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 1 1 0 ALG049 2019-W21 7 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG049 2019-W22 7 0 0 0 0 0 0 0 0 2 0 0 0 4 0 ALG049 2019-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG049 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG049 2019-W38 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG049 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W14 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG052 2018-W17 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W21 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG052 2018-W22 7 1 0 0 0 0 0 0 0 2 0 0 0 1 0 ALG052 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W24 7 0 0 0 0 0 0 0 0 3 0 1 0 1 0 ALG052 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG052 2018-W27 7 0 0 0 0 0 0 0 0 2 0 0 0 2 0 ALG052 2018-W28 7 1 0 0 0 0 0 0 0 1 0 0 0 2 0 ALG052 2018-W29 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2018-W30 7 2 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG052 2018-W31 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W33 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2018-W34 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG052 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W36 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2018-W37 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2018-W38 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2018-W39 7 0 0 0 0 0 0 0 0 4 0 0 0 0 0 ALG052 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W41 7 0 0 0 0 0 0 1 0 2 0 0 0 0 0 ALG052 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W43 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG052 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W49 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W11 7 1 0 0 0 2 0 0 1 0 0 0 0 0 0 ALG052 2019-W12 7 0 0 0 0 3 0 0 0 0 0 0 0 0 0 ALG052 2019-W13 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W15 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG052 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W18 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG052 2019-W19 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W20 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG052 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG052 2019-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W25 7 0 0 0 0 0 0 0 0 5 0 0 0 2 0 ALG052 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W27 7 1 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG052 2019-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG052 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W30 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG052 2019-W31 7 1 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG052 2019-W32 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W33 7 2 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W34 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG052 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W36 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG052 2019-W37 7 0 0 2 0 0 0 0 0 0 0 0 0 1 0 ALG052 2019-W38 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W40 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG052 2019-W41 7 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG052 2019-W42 7 0 0 0 0 1 0 0 0 3 0 0 0 0 0 ALG052 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W44 7 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG052 2019-W45 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG052 2019-W46 5 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG053 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG053 2018-W22 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG053 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W26 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ALG053 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W29 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ALG053 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W14 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG053 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W25 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG053 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG053 2019-W46 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W14 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG054 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W16 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG054 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W19 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG054 2018-W20 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG054 2018-W21 7 0 0 0 0 2 0 0 0 1 0 0 0 0 0 ALG054 2018-W22 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG054 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W24 7 0 0 1 0 0 0 0 0 1 0 0 0 0 0 ALG054 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG054 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W29 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W46 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W48 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG054 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W50 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG054 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W00 5 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG054 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W02 7 0 0 0 0 4 0 1 0 0 0 0 0 0 0 ALG054 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W04 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG054 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W11 7 0 0 0 0 3 0 1 0 0 0 0 0 0 0 ALG054 2019-W12 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG054 2019-W13 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG054 2019-W14 7 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG054 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W21 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG054 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG054 2019-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2019-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W29 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W33 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG054 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W37 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG054 2019-W38 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2019-W39 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2019-W40 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG054 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 2019-W44 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG054 2019-W45 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG054 2019-W46 6 0 0 0 0 0 0 0 0 4 0 0 0 0 0 ALG055 2018-W14 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W15 7 0 0 1 0 1 0 0 0 0 0 0 1 0 0 ALG055 2018-W16 7 0 0 1 0 1 0 0 0 0 0 0 0 0 0 ALG055 2018-W17 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG055 2018-W18 7 0 0 1 0 3 0 0 0 0 0 0 0 0 0 ALG055 2018-W19 7 0 0 1 0 4 0 0 0 0 0 0 0 0 0 ALG055 2018-W20 7 0 0 1 0 0 0 0 0 0 0 0 0 1 0 ALG055 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W22 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W23 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG055 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG055 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG055 2018-W29 7 0 0 3 0 0 0 0 0 1 0 0 0 0 0 ALG055 2018-W30 7 0 0 0 0 0 0 0 0 4 0 0 0 1 0 ALG055 2018-W31 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W35 7 0 0 1 0 0 0 0 0 2 0 0 0 1 0 ALG055 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W37 7 0 0 0 0 1 0 0 0 0 0 0 0 0 1 ALG055 2018-W38 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG055 2018-W39 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG055 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W41 7 0 0 0 0 0 0 1 0 0 0 0 0 0 1 ALG055 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W43 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W44 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W46 7 0 0 2 0 0 0 1 0 0 0 0 0 0 0 ALG055 2018-W47 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG055 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W49 7 0 0 1 0 1 0 1 0 0 0 0 0 0 0 ALG055 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W51 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG055 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W01 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W02 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG055 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W04 7 0 1 0 0 1 0 0 0 0 0 0 0 0 0 ALG055 2019-W05 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W06 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG055 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W11 7 1 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG055 2019-W12 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG055 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W14 7 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG055 2019-W15 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W16 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG055 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W18 7 0 0 0 0 4 0 0 0 1 0 0 0 0 0 ALG055 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG055 2019-W20 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG055 2019-W21 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG055 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W23 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG055 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG055 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG055 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG055 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG055 2019-W30 7 0 0 0 0 0 0 1 0 1 0 0 0 0 0 ALG055 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG055 2019-W33 7 0 0 1 0 0 0 0 0 0 0 0 0 2 0 ALG055 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG055 2019-W36 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG055 2019-W37 7 0 0 2 0 1 0 1 0 0 0 0 0 0 0 ALG055 2019-W38 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG055 2019-W39 7 0 0 1 0 1 0 0 0 0 0 0 0 0 0 ALG055 2019-W40 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG055 2019-W41 7 0 0 1 0 1 0 0 0 0 0 0 0 0 0 ALG055 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W44 7 0 0 3 0 0 0 0 0 1 0 0 0 0 0 ALG055 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG055 2019-W46 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W18 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W19 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W26 7 2 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG056 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W28 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W29 7 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG056 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG056 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W37 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W11 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG056 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W14 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG056 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG056 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W33 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG056 2019-W34 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG056 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W38 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG056 2019-W46 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W14 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W19 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W45 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W46 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG057 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W01 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG057 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG057 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W20 7 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ALG057 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG057 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG057 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W34 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG057 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG057 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W40 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 2019-W44 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG057 2019-W45 7 0 0 0 0 0 0 0 0 5 0 0 0 0 0 ALG057 2019-W46 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG058 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W15 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG058 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W17 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W20 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2018-W21 7 0 0 0 0 1 0 0 0 0 0 0 0 2 0 ALG058 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W23 7 1 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG058 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG058 2018-W30 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG058 2018-W31 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2018-W32 7 0 0 0 0 3 0 0 0 0 0 0 0 0 0 ALG058 2018-W33 7 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG058 2018-W34 7 0 0 0 0 3 0 0 0 1 0 0 0 0 0 ALG058 2018-W35 7 0 0 0 0 8 0 0 0 0 0 0 0 0 0 ALG058 2018-W36 7 0 0 0 0 2 0 0 0 1 0 0 0 0 0 ALG058 2018-W37 7 0 0 0 0 5 0 0 0 0 0 0 0 1 0 ALG058 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W39 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W41 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2018-W42 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2018-W43 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG058 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W45 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG058 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W02 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W10 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W12 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W16 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG058 2019-W17 7 0 0 0 0 0 0 1 0 1 0 0 0 0 0 ALG058 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W21 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG058 2019-W22 7 0 0 0 0 1 0 1 0 1 0 0 0 0 0 ALG058 2019-W23 7 0 0 0 0 0 0 0 0 4 0 0 0 0 0 ALG058 2019-W24 7 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG058 2019-W25 7 0 0 0 0 0 0 0 0 2 0 0 0 1 0 ALG058 2019-W26 7 0 0 0 0 0 0 0 0 4 0 0 0 0 0 ALG058 2019-W27 7 0 0 0 0 0 0 0 0 3 0 0 0 1 0 ALG058 2019-W28 7 0 0 0 0 0 0 0 0 2 0 0 0 1 0 ALG058 2019-W29 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG058 2019-W30 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG058 2019-W31 7 0 0 0 0 1 0 0 0 3 0 0 0 0 0 ALG058 2019-W32 7 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG058 2019-W33 7 0 0 0 0 2 0 0 0 4 0 0 0 0 0 ALG058 2019-W34 7 0 0 0 0 4 0 0 0 6 0 0 0 1 0 ALG058 2019-W35 7 0 0 0 0 2 0 0 0 4 0 0 0 0 0 ALG058 2019-W36 7 0 0 0 0 3 0 0 0 0 0 0 0 1 0 ALG058 2019-W37 7 0 0 0 0 2 0 0 0 2 0 0 0 1 0 ALG058 2019-W38 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2019-W39 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2019-W40 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W43 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG058 2019-W44 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG058 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG058 2019-W46 6 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG059 2018-W14 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG059 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W16 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG059 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG059 2018-W25 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W26 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG059 2018-W27 3 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG059 2018-W45 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W02 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W11 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG059 2019-W12 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG059 2019-W13 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W25 7 1 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG059 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W27 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG059 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG059 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG059 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG059 2019-W31 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG059 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG059 2019-W46 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG060 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG060 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W45 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG060 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2018-W52 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG060 2019-W00 5 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG060 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG060 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W25 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG060 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG060 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W38 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG060 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 2019-W46 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W14 6 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2018-W15 7 0 0 0 0 0 0 0 1 2 0 0 0 0 0 ALG061 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W21 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W23 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2018-W24 7 0 0 0 0 0 0 0 0 5 0 0 0 0 0 ALG061 2018-W25 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG061 2018-W26 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2018-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2018-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG061 2018-W37 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG061 2018-W38 7 0 0 0 0 0 0 0 0 1 0 0 0 2 0 ALG061 2018-W39 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2018-W40 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG061 2018-W41 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W44 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2018-W45 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2018-W46 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG061 2018-W47 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W50 7 0 0 0 0 0 0 0 0 5 0 0 0 0 0 ALG061 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W01 7 0 0 0 0 0 0 0 1 1 0 0 0 0 0 ALG061 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W04 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W05 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2019-W06 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W07 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W08 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W09 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W11 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W22 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG061 2019-W25 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2019-W26 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W29 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W31 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W33 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG061 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W43 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG061 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 2019-W45 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG061 2019-W46 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2018-W14 6 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG062 2018-W15 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG062 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W17 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG062 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W21 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG062 2018-W22 7 1 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG062 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG062 2018-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG062 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG062 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W30 7 0 0 0 0 0 0 0 0 2 0 0 0 1 0 ALG062 2018-W31 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2018-W32 7 0 0 1 0 0 0 0 0 0 0 0 0 1 0 ALG062 2018-W33 7 0 0 0 0 0 0 0 0 3 0 0 0 1 0 ALG062 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG062 2018-W36 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG062 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2018-W51 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2018-W52 2 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG062 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W01 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG062 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W03 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W05 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W14 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W15 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W16 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W17 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG062 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W20 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG062 2019-W21 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W22 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG062 2019-W23 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG062 2019-W24 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG062 2019-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 2 0 ALG062 2019-W26 7 0 0 2 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG062 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W32 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W33 7 0 0 0 0 0 0 0 0 5 0 0 0 0 0 ALG062 2019-W34 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG062 2019-W36 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG062 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG062 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG062 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W40 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG062 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W44 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG062 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG062 2019-W46 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ALG063 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W22 7 1 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG063 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG063 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W32 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG063 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W46 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG063 2018-W47 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG063 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W49 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG063 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2018-W52 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W02 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W06 7 0 0 0 0 0 0 0 1 1 0 0 0 0 0 ALG063 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W10 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG063 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W14 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W16 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W18 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG063 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W22 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W26 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG063 2019-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W29 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG063 2019-W30 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG063 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W33 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG063 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG063 2019-W46 5 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG064 2018-W14 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W20 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG064 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W28 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG064 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W20 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W22 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W25 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG064 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W28 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG064 2019-W29 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG064 2019-W30 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG064 2019-W31 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG064 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG064 2019-W46 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG065 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG065 2018-W18 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W42 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG065 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W45 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W46 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W47 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W48 7 5 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W49 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W51 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W00 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W03 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W04 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W05 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W06 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W07 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W10 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG065 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W14 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W16 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 2019-W25 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W14 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG066 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W17 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W20 7 1 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG066 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W22 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG066 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W27 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG066 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W29 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG066 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W18 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG066 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W25 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG066 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG066 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG066 2019-W46 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W14 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W16 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W45 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W18 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG067 2019-W19 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W22 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG067 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W28 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG067 2019-W29 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W44 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 2019-W46 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W16 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG068 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W45 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W48 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG068 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 2019-W46 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W14 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG069 2018-W15 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG069 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG069 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG069 2018-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG069 2018-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG069 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W32 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG069 2018-W33 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG069 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W41 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2018-W42 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2018-W43 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG069 2018-W44 7 0 0 0 0 0 0 0 0 5 0 0 0 0 0 ALG069 2018-W45 7 0 0 0 0 0 0 1 0 7 0 0 0 0 0 ALG069 2018-W46 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG069 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W48 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG069 2018-W49 7 0 0 0 0 0 0 1 0 2 0 0 0 0 0 ALG069 2018-W50 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W13 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W15 7 0 0 0 0 0 0 0 0 3 0 0 0 1 0 ALG069 2019-W16 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG069 2019-W18 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W19 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W20 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG069 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG069 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 3 0 ALG069 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 3 0 ALG069 2019-W25 7 0 0 0 0 0 0 0 0 2 0 0 0 1 0 ALG069 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG069 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG069 2019-W28 7 0 0 0 0 0 0 0 0 3 0 0 0 1 0 ALG069 2019-W29 7 0 0 0 0 0 0 0 0 4 0 0 0 1 0 ALG069 2019-W30 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W32 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG069 2019-W33 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W34 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG069 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG069 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 2019-W38 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W39 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W40 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W41 7 0 0 0 0 0 0 0 0 4 0 0 0 0 0 ALG069 2019-W42 7 0 0 0 0 0 0 0 0 5 0 0 0 0 0 ALG069 2019-W43 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG069 2019-W44 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG069 2019-W45 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG069 2019-W46 3 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG070 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W18 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W19 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W45 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W08 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG070 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W18 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ALG070 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W20 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG070 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W24 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG070 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG070 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W30 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG070 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W32 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG070 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG070 2019-W46 3 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG071 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W14 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG071 2018-W15 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG071 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W18 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG071 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2018-W20 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2018-W24 7 0 0 0 0 0 0 0 0 2 0 0 0 1 0 ALG071 2018-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 2 0 ALG071 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W27 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2018-W28 7 0 0 0 0 0 0 0 0 3 0 0 0 0 0 ALG071 2018-W29 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2018-W30 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG071 2018-W31 7 0 0 0 0 0 0 0 0 2 0 0 0 2 0 ALG071 2018-W32 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG071 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W36 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2018-W37 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG071 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W40 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG071 2018-W41 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W44 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG071 2018-W45 7 0 0 0 0 0 0 0 0 2 0 0 0 0 1 ALG071 2018-W46 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG071 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W10 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG071 2019-W11 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG071 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W14 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2019-W15 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2019-W16 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG071 2019-W17 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2019-W18 7 0 0 0 0 1 0 0 0 0 0 0 0 1 0 ALG071 2019-W19 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2019-W20 7 0 0 0 0 4 0 0 0 2 0 0 0 0 0 ALG071 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2019-W23 7 0 0 0 0 0 0 0 0 1 0 0 0 2 0 ALG071 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2019-W26 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG071 2019-W27 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG071 2019-W28 7 1 0 0 0 0 0 0 0 0 0 0 0 2 0 ALG071 2019-W29 7 1 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W31 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG071 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG071 2019-W33 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG071 2019-W34 7 0 0 0 0 0 0 0 0 4 0 0 0 0 0 ALG071 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2019-W36 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG071 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W38 7 0 0 0 0 0 0 1 0 1 0 0 0 0 0 ALG071 2019-W39 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG071 2019-W40 7 1 0 0 0 0 0 0 0 0 0 0 0 3 0 ALG071 2019-W41 7 0 0 1 0 0 0 0 0 2 0 0 0 0 0 ALG071 2019-W42 7 0 0 0 0 0 0 0 0 2 0 0 0 0 0 ALG071 2019-W43 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG071 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 2019-W46 6 0 0 0 0 1 0 0 0 1 0 0 0 0 0 ALG072 2018-W13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W14 7 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ALG072 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W16 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG072 2018-W17 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG072 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG072 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W29 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG072 2018-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ALG072 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W34 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG072 2018-W35 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG072 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W00 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W10 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG072 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W17 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG072 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W19 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W21 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W22 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W24 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W25 7 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ALG072 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W30 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W34 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG072 2019-W35 7 0 0 0 0 0 0 0 0 1 0 0 0 1 0 ALG072 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W38 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG072 2019-W39 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ALG072 2019-W40 7 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ALG072 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG072 2019-W46 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W15 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W19 7 1 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2018-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W22 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG073 2018-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W24 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2018-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W26 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2018-W27 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W28 7 0 0 0 0 0 0 0 0 0 0 4 0 0 0 ALG073 2018-W29 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2018-W30 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2018-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W38 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W45 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W46 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W47 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W48 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W49 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W50 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W51 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2018-W52 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W00 5 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W01 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W02 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W03 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W04 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W05 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W06 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W07 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W08 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W09 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W10 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W11 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W12 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W13 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W14 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W15 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W16 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W17 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W18 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W19 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W20 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W21 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W22 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W23 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W24 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W25 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W26 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W27 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W28 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W29 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W30 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W31 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W32 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W33 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W34 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W35 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W36 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W37 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W38 7 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ALG073 2019-W39 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W40 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W41 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W42 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W43 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W44 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG073 2019-W45 7 0 0 0 0 0 0 0 0 0 0 2 0 0 0 ALG073 2019-W46 5 0 0 0 0 0 0 0 0 0 0 1 0 0 0 As with previous chapters, we will start by focusing on the white-tailed deer (Odocoileus virginianus). We first need to create a site by occasion matrix for our focal species, using a 7-day occasion length. This means we need to break our camera data into seven day bins. We can create the detection histories using the following code: # Use white-tailed deer focal_sp&lt;- &quot;Odocoileus.virginianus&quot; # subset to 2019 tmp_week &lt;- week_obs[substr(week_obs$date,1,4)==2019,] # Create the Y data y_dat &lt;- tmp_week[,c(&quot;placename&quot;, &quot;date&quot;, focal_sp)] %&gt;% # Subset to just white-tailed deer pivot_wider(names_from = date, values_from = focal_sp) # Shift to wide format # Convert it to a matrix - but only keep the date values y_mat &lt;- as.matrix(y_dat[,unique(tmp_week$date)]) # Update the row names row.names(y_mat) &lt;- y_dat$placename The resulting data frame looks like this: 2019-W00 2019-W01 2019-W02 2019-W03 2019-W04 2019-W05 2019-W06 2019-W07 2019-W08 2019-W09 2019-W10 2019-W11 2019-W12 2019-W13 2019-W14 2019-W15 2019-W16 2019-W17 2019-W18 2019-W19 2019-W20 2019-W21 2019-W22 2019-W23 2019-W24 2019-W25 2019-W26 2019-W27 2019-W28 2019-W29 2019-W30 2019-W31 2019-W32 2019-W33 2019-W34 2019-W35 2019-W36 2019-W37 2019-W38 2019-W39 2019-W40 2019-W41 2019-W42 2019-W43 2019-W44 2019-W45 2019-W46 ALG027 0 0 0 0 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ALG029 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 1 1 4 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 1 3 1 1 0 ALG031 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG032 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG035 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG036 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG037 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG038 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG039 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG043 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 2 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 ALG044 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 ALG045 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG046 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG047 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 NA NA NA NA NA NA NA NA NA NA NA NA ALG048 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ALG049 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 NA NA NA NA NA NA NA ALG052 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 3 1 2 0 0 1 1 5 0 0 1 0 2 3 1 1 1 1 1 0 0 0 2 1 3 0 1 0 1 ALG053 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG054 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 2 0 0 0 2 1 1 1 0 0 0 2 2 4 ALG055 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 2 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 ALG056 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 ALG057 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 5 1 ALG058 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 1 1 4 1 2 4 3 2 2 2 3 1 4 6 4 0 2 0 0 0 0 0 0 0 0 0 ALG059 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG060 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG061 0 1 0 0 1 2 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 2 1 ALG062 0 3 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 2 0 0 3 1 3 2 3 1 0 1 0 0 0 0 0 5 1 1 1 0 0 0 0 0 0 0 0 0 0 ALG063 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 1 1 1 0 0 1 1 0 1 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 1 ALG064 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG065 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ALG066 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG067 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG068 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG069 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3 1 0 1 0 0 0 0 0 0 2 0 0 3 4 1 0 2 1 3 1 0 0 1 1 1 4 5 0 0 1 2 ALG070 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ALG071 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 2 0 0 1 0 0 1 0 0 1 0 2 0 0 4 1 0 0 1 0 0 2 2 1 0 0 1 ALG072 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ALG073 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 It is a matrix of all the weeks the cameras were active, and whether the count of the independent detections in that interval. The fill = NA command puts a zero where there is data for a given day. You can see that in some columns we have values &gt; 1 - this is because we had more than one independent observation in that week. Occupancy analyses (typically) require this data to be in detection/non-dection (0 or 1) format. So lets change that here. # Where y_mat is &gt; 1, and where y_mat isn&#39;t NA - give it the value 1 y_mat[y_mat&gt;1 &amp; is.na(y_mat)==F] &lt;- 1 However, we have lost our effort information - the number of days each camera was active in a given time period. So we need another data frame! To get that information we need to create an effort history eff_mat: # To create the effort matrix - inst of the Focal Species bring in the effort eff_mat &lt;- tmp_week[,c(&quot;placename&quot;, &quot;date&quot;, &quot;days&quot;)] eff_mat &lt;- eff_mat %&gt;% # Create a matrix based on dates and effort spread(date,days, fill = NA) %&gt;% # group by deloyment Location ID, then make that the row.namesd group_by(placename) %&gt;% column_to_rownames( var = &quot;placename&quot;) eff_mat &lt;- as.matrix(eff_mat) Check that it looks sensible: 2019-W00 2019-W01 2019-W02 2019-W03 2019-W04 2019-W05 2019-W06 2019-W07 2019-W08 2019-W09 2019-W10 2019-W11 2019-W12 2019-W13 2019-W14 2019-W15 2019-W16 2019-W17 2019-W18 2019-W19 2019-W20 2019-W21 2019-W22 2019-W23 2019-W24 2019-W25 2019-W26 2019-W27 2019-W28 2019-W29 2019-W30 2019-W31 2019-W32 2019-W33 2019-W34 2019-W35 2019-W36 2019-W37 2019-W38 2019-W39 2019-W40 2019-W41 2019-W42 2019-W43 2019-W44 2019-W45 2019-W46 ALG027 5 7 7 7 7 7 7 7 7 7 7 7 7 4 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ALG029 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG031 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG032 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG035 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 ALG036 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG037 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG038 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG039 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG043 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG044 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG045 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG046 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG047 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 NA NA NA NA NA NA NA NA NA NA NA NA ALG048 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG049 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 NA NA NA NA NA NA NA ALG052 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG053 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG054 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 ALG055 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG056 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 ALG057 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG058 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 ALG059 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG060 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG061 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG062 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG063 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG064 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 ALG065 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ALG066 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG067 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG068 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 ALG069 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG070 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 ALG071 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 ALG072 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 ALG073 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We might want to remove all of the data from the weeks where we did not get a complete sample: y_mat[eff_mat!=7] &lt;- NA Now we are ready to feed this into the unmarked package. 11.1.1 Unmarked package One of the hurdles in using the unmarked package is it uses a different style of dataframe called an unmarked dataframe. It is essentially a compillation of the different dataframes we need for the analysis (y data and covariate data). We asemmbled the Y data above, so now lets make the covariates: locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations_and_covariates.csv&quot;) # Unmarked wants your detection history, effort data and site covariates as matrices. But the order is important! # Check the order of your matrices and covariates files matches... or you will get nonsense! table(locs$placename == row.names(y_mat)) ## ## TRUE ## 38 Data standardization Unmarked models benefit from standardizing your covariates - it helps the solving algorithms converge on an appropriate solution. To do this we use the MuMIn package. library(MuMIn) z_locs &lt;- stdize(locs) Take a look at it to see what it has done! We then need to build an ‘unmarked’ data frame. You don’t really need to know why they are different or how to use one (although it helps), knowing how to use one is sufficient. # Build an unmarkedFramOccu un_dat &lt;- unmarkedFrameOccu(y = y_mat, # your occupancy data siteCovs = z_locs) # Your site covariates ## Warning: siteCovs contains characters. Converting them to factors. We can then fit the occupancy model, lets start with a “null” model with no predictors on detection or occupancy. # Fit general model all variables m0 &lt;- occu(formula = ~1 # detection formula first ~1, # occupancy formula second, data = un_dat) Then view the results. summary(m0) ## ## Call: ## occu(formula = ~1 ~ 1, data = un_dat) ## ## Occupancy (logit-scale): ## Estimate SE z P(&gt;|z|) ## 0.67 0.345 1.94 0.0522 ## ## Detection (logit-scale): ## Estimate SE z P(&gt;|z|) ## -1.39 0.0752 -18.4 1.15e-75 ## ## AIC: 1162.108 ## Number of sites: 38 ## optim convergence code: 0 ## optim iterations: 18 ## Bootstrap iterations: 0 The estimate you see for both occupancy and detection probability is on the log-link scale. If we want to calculate the occupancy probability, we can use the backTransform() function: backTransform(m0, type = &quot;state&quot;) ## Backtransformed linear combination(s) of Occupancy estimate(s) ## ## Estimate SE LinComb (Intercept) ## 0.662 0.0773 0.67 1 ## ## Transformation: logistic So the probability that a white-tailed deer occupies one of the survey locations is ~0.66. For the detection probability we specify “det”: backTransform(m0, type = &quot;det&quot;) ## Backtransformed linear combination(s) of Detection estimate(s) ## ## Estimate SE LinComb (Intercept) ## 0.2 0.012 -1.39 1 ## ## Transformation: logistic The probability that we detect a white-tailed deer in a given unit of time (7-days), given that it is there to be detected, is ~0.2. Let’s fit a couple of other models! First with a continuous covariate on the occupancy probability, then a categorical one too: # Occupancy is influence by line of sight m1 &lt;- occu(formula = ~1 # detection formula first ~z.line_of_sight_m, # occupancy formula second, data = un_dat) # Occupancy is influenced by the feature_type a camera is deployed on m2 &lt;- occu(formula = ~1 # detection formula first ~feature_type, # occupancy formula second, data = un_dat) We can perform model selection on these different scenarios in the same way as in the habitat use chapter - using the MuMIn package: model.sel(m0,m1,m2) ## Model selection table ## psi(Int) p(Int) psi(z.lin_of_sgh_m) psi(ftr_typ) df logLik AICc delta ## m1 0.70590 -1.384 -0.5664 3 -577.711 1162.1 0.00 ## m0 0.67020 -1.385 2 -579.054 1162.5 0.32 ## m2 0.01868 -1.384 + 4 -577.642 1164.5 2.37 ## weight ## m1 0.464 ## m0 0.394 ## m2 0.142 ## Models ranked by AICc(x) The best supported model contains z.line_of_sight_m, although the improvement on the null model is minimal. 11.1.2 Plotting predictions We can observe the relationship between our covariates and our occupancy probabilities through the use of a dummy dataframes (which we will call new_dat). A dummy dataframe is essential just a dataframe built up of dummy data - which lies within the upper and lower limits of the covariates we already have. We wouldn’t want to extrapolate beyond our data! We can then plot the results: # Generate new data to predict from new_dat &lt;- cbind(expand.grid( z.line_of_sight_m=seq(min(z_locs$z.line_of_sight_m),max(z_locs$z.line_of_sight_m), # add more covariates here if the model is more complex length.out=25))) # Make the predicted values for the data you supplied new_dat &lt;- predict(m1, type=&quot;state&quot;, newdata = new_dat, appendData=TRUE) #Plot the results p1 &lt;- ggplot(new_dat, aes(x = z.line_of_sight_m, y = Predicted)) + # mean line geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5, linetype = &quot;dashed&quot;) + #Confidence intervals geom_path(linewidth = 1) + labs(x = &quot;Line of sight&quot;, y = &quot;Occupancy probability&quot;) + # axis labels theme_classic() + coord_cartesian(ylim = c(0,1)) p1 As with our habitat use model, white-tailed deer (Odocoileus virginianus) occupancy appears to decrease with increasing line of sight. 11.1.3 On your own Let’s explore some of the models we fit in the habitat use chapter in the occupancy framework. We have not included any detection covariates in this example dataset, so hold that constand for now! NOT IN R Tobler, M. et al. Spatiotemporal hierarchical modelling of species richness and occupancy using camera trap data. J. Appl. Ecol. (2015). 11.2 Spatial occupancy model: spOccupancy COMING SOON Whilst unmarked has been the workhorse for implementing occupancy models in R, there is a new kid on the block - spOccupancy. One of the benefits of spOccupancy is that they have a huge amount of resources to help you learn and fit models too: the spOccupancy website 11.2.1 Multi species model COMING SOON "],["activity.html", "Chapter 12 Activity 12.1 Independent detections or raw data? 12.2 Data formatting 12.3 Species comparisons 12.4 Treatment comparisons 12.5 On your own 12.6 Selected further reading", " Chapter 12 Activity Given that camera traps operate 24 hours a day, 7 days a week, and can record animal motion down to second-level precision, they represent a powerful tool to explore and contrast the activity patterns of the species they detect! Such analyses can give insight into competition, predation and coexistence. Characterizing the “activity level” - the proportion of the day which animals are active - is also increasingly important for new estimators of animal density (see the density chapter for more info). Consequently, understanding how to derive and use activity data is very important for people using camera traps. Must read Frey, Sandra, et al. “Investigating animal activity patterns and temporal niche partitioning using camera‐trap data: Challenges and opportunities.” Remote Sensing in Ecology and Conservation 3.3 (2017): 123-132. Two key packages overlap https://cran.r-project.org/web/packages/overlap/index.html activity https://cran.r-project.org/web/packages/activity/index.html They each use the timestamps in camera trap detetions to derive activity indices which can be compared between different strata of interest (e.g. species, treatments etc.). Here we will use the activity package. 12.1 Independent detections or raw data? A recent paper has highlighted that we need to carefully consider our data source for activity analyses: Christopher Peral, Marietjie Landman, Graham I. H. Kerley The inappropriate use of time-to-independence biases estimates of activity patterns of free-ranging mammals derived from camera traps Ecology and Evolution Whilst we typically use “independent data” for most of our camera trap analysis, doing so may throw away useful data on activity. Both in terms of the number of data points (power) but also the activity patterns they generate. Peral et.al show that 70% of papers published to date use independent data to derive their indices. They actually state:“We conclude that the application of time-to-independence data filters in camera trap-based estimates of activity patterns is not valid and should not be used.” So we will use the raw data to derive our indices! Load your packages 12.2 Data formatting First, lets import the processed raw data file. # Import the data img &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_raw_detections.csv&quot;, header=T) Which looks like this: project_id deployment_id image_id filename location is_blank identified_by wi_taxon_id class order family genus species uncertainty timestamp number_of_objects age sex animal_recognizable individual_id individual_animal_notes behavior highlighted markings cv_confidence license placename group_size temperature sp common_name animal_count event_id event_duration event_groupsize event_observations AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-01.JPG Algar27__2018-04-13__13-51-01.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA 2018-04-13 13:51:01 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA Lynx.canadensis canada lynx 1 E000001 8 1 6 AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-02.JPG Algar27__2018-04-13__13-51-02.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA 2018-04-13 13:51:02 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA Lynx.canadensis canada lynx 1 E000001 8 1 6 AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-03.JPG Algar27__2018-04-13__13-51-03.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA 2018-04-13 13:51:03 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA Lynx.canadensis canada lynx 1 E000001 8 1 6 AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-06.JPG Algar27__2018-04-13__13-51-06.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA 2018-04-13 13:51:06 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA Lynx.canadensis canada lynx 1 E000001 8 1 6 AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-07.JPG Algar27__2018-04-13__13-51-07.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA 2018-04-13 13:51:07 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA Lynx.canadensis canada lynx 1 E000001 8 1 6 AlgarRestorationProject ALG027_2018-04-11 Algar27__2018-04-13__13-51-09.JPG Algar27__2018-04-13__13-51-09.JPG NA 0 ERT NA Mammalia Carnivora Felidae Lynx canadensis NA 2018-04-13 13:51:09 1 Adult NA NA NA NA NA NA NA NA ALG027 1 NA Lynx.canadensis canada lynx 1 E000001 8 1 6 Then load the activity package: # Load the package library(activity) If your cameras correct for daylight savings use the correct code, if they do not, use UTC. img$timestamp &lt;- ymd_hms(img$timestamp, tz=&quot;UTC&quot;) Note - find your timezone code for the tz= call here. 12.2.1 Accounting for sunrise and sunset A recent paper highlighted the challenges in trying to understand animal activity patterns at high latitudes - as sunrise/sunset timings vary substantially through the calender year. See: Vazquez, Carmen, et al. “Comparing diel activity patterns of wildlife across latitudes and seasons: Time transformations using day length.” Methods in Ecology and Evolution 10.12 (2019): 2057-2066. If we want to compare activity patterns between two different locations, or different seasons, the day length at the time the detection occurred can have a huge impact on our estimates of wildlife activity. For example, if we wanted to compare day/night activity between winter and summer periods, in winter animal activity is constrained to a much shorter day length. Fortunately, the authors have a solution! The average anchoring method Instead of using the ‘human’ 24h clock, we can instead express animal activity relative to an important anchor point in the day (e.g. sunrise). NOTE -the transformation is not necessary at latitudes below 20°, or in studies with a duration of less than a month (below 40° latitude), as day length doesn’t chnage substantially. # We need to add latitude and longitude to our observations # import our station locations (and other covariates) locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations_and_covariates.csv&quot;) # Add them to our data frame img_locs &lt;- left_join(img, locs) ## Joining with `by = join_by(project_id, placename)` # calculate solar time tmp &lt;- solartime ( img_locs$timestamp, # the date time column img_locs$latitude, # Latitude img_locs$longitude, # Longitude tz=-6, # an offset in numeric hours to UTC (Alberta is 6 hours behind) format=&quot;%Y-%m-%d %H:%M:%S&quot;) # Although we want to use solar time, let&#39;s add both incase you want to explore the implications img_locs$solar &lt;- tmp$solar img_locs$clock &lt;- tmp$clock Let’s check out the relationship between these two indices: plot(img_locs$solar, img_locs$clock) We are now ready to fit some models! 12.3 Species comparisons Let’s start with a white-tailed deer and caribou example. Note we are reducing the number of replicates to 100 to speed up the process - typically people use 1000. # Fit an activity model m1 &lt;- fitact(img_locs$solar[img_locs$sp==&quot;Odocoileus.virginianus&quot;], sample=&quot;model&quot;, reps=100) plot(m1) Take a look at the raw data if you want. m1 And repeat it for Caribou # Fit an activity model m2 &lt;- fitact(img_locs$solar[img_locs$sp==&quot;Rangifer.tarandus&quot;], sample=&quot;model&quot;, reps=100) plot(m2) We can plot both on the same axis as follows: plot(m2, yunit=&quot;density&quot;, data=&quot;none&quot;, las=1, lwd=2, tline=list(lwd=2), # Thick line cline=list(lty=0)) # Supress confidence intervals plot(m1, yunit=&quot;density&quot;, data=&quot;none&quot;, add=TRUE, tline=list(col=&quot;red&quot;, lwd=2), cline=list(lty=0)) legend(&quot;topright&quot;, c(&quot;Caribou&quot;, &quot;Deer&quot;), col=1:2, lty=1, lwd=2) We can compare different activity patterns using coefficient of overlap (∆) - developed by Ridout and Linkie: Ridout, Martin S., and Matthew Linkie. “Estimating overlap of daily activity patterns from camera trap data.” Journal of Agricultural, Biological, and Environmental Statistics 14.3 (2009): 322-337. The coefficient ranges from 0 (no overlap) to 1 (complete overlap). We can implement for a two species comparison as follows: # Note reps reduced to speed up running time - people typically use 1000. compareCkern(m1, m2, reps = 100) ## obs null seNull pNull ## 0.784652408 0.955259792 0.008606608 0.000000000 The output above represents: 0 = no overlap and 1 = high overlap! obs = observed overlap index; null = mean null overlap index; seNull = standard error of the null distribution; pNull = probability observed index arose by chance. Which suggests there is reasonably high overlap between the two species - and that it did not come about by chance. 12.4 Treatment comparisons We can also compare patterns within a species across different strata of interest. For example, perhaps white-tailed deer change their activity patterns in response to the feature_type they are using - perhaps they will be more nocturnal on HumanUse lines relative to Offline strata. Lets try it: White-tail deer on HumanUse feature #Fit an activity model m1 &lt;- fitact(img_locs$solar[img_locs$sp==&quot;Odocoileus.virginianus&quot; &amp; img_locs$feature_type==&quot;HumanUse&quot;], sample=&quot;model&quot;, reps=100) White-tail deer on Offline feature m2 &lt;- fitact(img_locs$solar[img_locs$sp==&quot;Odocoileus.virginianus&quot; &amp; img_locs$feature_type==&quot;Offline&quot;], sample=&quot;model&quot;, reps=100) plot(m2, yunit=&quot;density&quot;, data=&quot;none&quot;, las=1, lwd=2, tline=list(lwd=2), # Thick line cline=list(lty=0)) # Supress confidence intervals plot(m1, yunit=&quot;density&quot;, data=&quot;none&quot;, add=TRUE, tline=list(col=&quot;red&quot;, lwd=2), cline=list(lty=0)) legend(&quot;topright&quot;, c(&quot;Offline&quot;, &quot;HumanUse&quot;), col=1:2, lty=1, lwd=2) # Note reps reduced to speed up running time - people typically use 1000. compareCkern(m1, m2, reps = 100) ## obs null seNull pNull ## 0.82674255 0.96140086 0.01041054 0.00000000 There is very high overlap for these comparisons, and it is unlikely to have arisen by chance! So it seems the edidence for changes in temporal activity in response to feature_type is weak - at least for the white-tiled deer! 12.4.1 Seasonal comparison img_locs$month &lt;- month(img_locs$timestamp, label=T) #Fit an activity model m1 &lt;- fitact(img_locs$solar[img_locs$sp==&quot;Rangifer.tarandus&quot; &amp; img_locs$month %in% c(&quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;)], sample=&quot;model&quot;, reps=100) m2 &lt;- fitact(img_locs$solar[img_locs$sp==&quot;Rangifer.tarandus&quot; &amp; img_locs$month %in% c(&quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;, &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;)], sample=&quot;model&quot;, reps=100) plot(m2, yunit=&quot;density&quot;, data=&quot;none&quot;, las=1, lwd=2, tline=list(lwd=2), # Thick line cline=list(lty=0)) # Supress confidence intervals plot(m1, yunit=&quot;density&quot;, data=&quot;none&quot;, add=TRUE, tline=list(col=&quot;red&quot;, lwd=2), cline=list(lty=0)) legend(&quot;topright&quot;, c(&quot;Winter&quot;, &quot;Summer&quot;), col=1:2, lty=1, lwd=2) 12.5 On your own Try your own species comparisons. Remember we have the following species: ## [1] &quot;Lynx.canadensis&quot; &quot;Canis.lupus&quot; ## [3] &quot;Ursus.americanus&quot; &quot;Alces.alces&quot; ## [5] &quot;Odocoileus.virginianus&quot; &quot;Canis.latrans&quot; ## [7] &quot;Cervus.canadensis&quot; &quot;Lontra.canadensis&quot; ## [9] &quot;Martes.americana&quot; &quot;Rangifer.tarandus&quot; ## [11] &quot;Vulpes.vulpes&quot; &quot;Oryctolagus.cuniculus&quot; ## [13] &quot;Lepus.americanus&quot; &quot;Tamiasciurus.hudsonicus&quot; You can also try other categorical strata comparisons, we have: feature_type ## ## HumanUse NatRegen Offline ## 14 11 13 12.6 Selected further reading Houngbégnon, Fructueux GA, et al. “Daily Activity Patterns and Co-Occurrence of Duikers Revealed by an Intensive Camera Trap Survey across Central African Rainforests.” Animals 10.12 (2020): 2200. Ross J, Hearn AJ, Johnson PJ, Macdonald DW (2013). Activity patterns and temporal avoidance by prey in response to Sunda clouded leopard predation risk. Journal of Zoology, 290(2), 96,106. Azevedo FC, Lemos FG, Freitas-Junior MC, Rocha DG, Azevedo FCC (2018). Puma activity patterns and temporal overlap with prey in a human-modifed landscape at Southeastern Brazil.” Journal of Zoology "],["density.html", "Chapter 13 Density 13.1 Individually identifiable individuals 13.2 Unmarked animals 13.3 Future directions", " Chapter 13 Density Precise and unbiased population density estimates are fundamental to conserve rare and vulnerable species… But it is complicated, especially with camera traps! There we said it, estimating density from camera trap data is not easy, often isn’t precise, and can require you to move beyond “just” R (typically JAGS or Nimble). Also - over the last few years the number of approaches has grown rapidly, consequently it is impossible to cover all of them here. There have been several great review papers published recently, and we recommend you check each one out: Morin, Dana J., et al. “Comparison of methods for estimating density and population trends for low-density Asian bears.” Global Ecology and Conservation 35 (2022): e02058. This paper is great for overview of the different methods available with a specific species in mind - Asian black bear. They discuss the methods, and do not directly compare them. Gilbert, N. A., Clare, J. D., Stenglein, J. L., &amp; Zuckerberg, B. (2021). Abundance estimation of unmarked animals based on camera‐trap data. Conservation Biology, 35(1), 88-100. This paper provides a nice overview of the state of the art of density estimation of unmarked animals. They discuss the methods, and do not directly compare them. Palencia, P., Rowcliffe, J. M., Vicente, J., &amp; Acevedo, P. (2021). Assessing the camera trap methodologies used to estimate density of unmarked populations. Journal of Applied Ecology, 58(8), 1583-1592. A quantitative comparison of different unmarked density estimators - focusing on REM, REST and Distance methods. 13.1 Individually identifiable individuals If you are dealing with a project with individually identifiable animals, you are in luck as there are some great resources created by Ian Durbach and David Borchers: https://www.scr-cameratrap.com/ and an amazing tool for helping you design your surveys called secrdesign. Spatial capture recapture is considered the gold standard in density estimation, you cannot get much better than this! 13.2 Unmarked animals Unfortunately, it is not possible to identify individual animals for the vast majority of species detected by camera traps. There are a growing number of analysis frameworks for unmarked animals, including: 13.2.1 Random encounter model The random encounter model is based on modelling the ‘random encounters’ between moving animals and stationary camera traps. It takes into account key variables that affect the encounter rate: the camera detection zone, defined by its radius and angle, the daily distance travelled by an animal in the population (a.k.a. day range) Parameters required [and how you might get them]: y = number of independent photo events [from our independent detections file] t - total survey effort [from our deployment data] v = average speed of animal movement [We could use telemetry, we could use speed derived from cameras] r = radius of camera detection zone [Could use field trials as these parameters can vary station to station] theta = angle of camera detection zone [Could use field trials as these parameters can vary station to station] We have speed data for moose and wolf in both winter and summer (derived from telemetery studies which occured close to our region of interest). So let’s compare density estimates between winter (Oct-Mar) and summer (Apr-Sep) between these two species. In our project we didn’t empirically measure the detection zone, so we will assume that these remain constant through time time and space. First, install the remBoot package to help us fit the REM model. #devtools::install_github(&quot;arcaravaggi/remBoot&quot;) library(remBoot); library(dplyr); library(lubridate) 13.2.1.1 Formatting The formulation of the REM included in the ‘remBoot’ package is fairly simple, a dataframe consisting of rows, there each row is an independent detection, and with columns reflecting the strata of interest (confusingly labeled site), location_id (labelled cam), the group_size (labelled count), the viewshed radius (in km) and the viewshed angle (width) in radians. Note: in this instance, we don’t have empirically derived measures of the viewshed radius and angle, so we will use the values assessed by TrailCameraPro by Reconyx for the Hyperfire 2 (the camera model used in this study): angle = 42.9; and half the maximum distance for its range = 25m . We can easily derive this from our independent detections data frame. Moose first! 13.2.2 Moose From existing literature, we are expecting moose densities of roughly 0.2-0.5 individuals per km2. ind &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_detections.csv&quot;, header=T) # Setup a winter and summer months variable summer &lt;- c(&quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;) winter &lt;- c(&quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;) # Subset to moose (alces alces) and the summer months moose_summer &lt;- ind[ind$sp==&quot;Alces.alces&quot; &amp; substr(ind$timestamp,6,7) %in% summer,c(&quot;placename&quot;, &quot;event_groupsize&quot;)] # rename the columns moose_summer &lt;- moose_summer %&gt;% rename(&quot;cam&quot;=placename, &quot;count&quot;=event_groupsize) moose_summer$dist &lt;- 0.024 # Our detection distance in km moose_summer$theta &lt;- 42.9*(pi/180) # Our viewshed angle # Subset to moose (alces alces) and the winter months moose_winter &lt;- ind[ind$sp==&quot;Alces.alces&quot; &amp; substr(ind$timestamp,6,7) %in% winter,c(&quot;placename&quot;, &quot;event_groupsize&quot;)] # rename the columns moose_winter &lt;- moose_winter %&gt;% rename(&quot;cam&quot;=placename, &quot;count&quot;=event_groupsize) # Add the detection zone covariates moose_winter$dist &lt;- 0.024 moose_winter$theta &lt;- 42.9*(pi/180) Finally we need two constants: the effort in each given strata of interest (days), and 2) the average day range of the focal animal. To get the number of days per season, we can use the monthly_observations dataframe: mon_obs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_monthly_observations.csv&quot;, header=T) summer_effort &lt;- sum(mon_obs$days[substr(mon_obs$date, 6,7) %in% summer]) winter_effort &lt;- sum(mon_obs$days[substr(mon_obs$date, 6,7) %in% winter]) Here I will use the average daily distance for summer and winter derived from telemetry datasets on Movebank from locations close to where this camera trap study was conducted: Moose winter = 1.07 km per day Moose summer = 1.15 km per day Now lets fit the REM model. nboots &lt;- 1000 summer_rem &lt;- rem(dat = moose_summer, tm=summer_effort, v=1.15) winter_rem &lt;- rem(dat = moose_winter, tm=winter_effort, v=1.07) moose_res &lt;- data.frame(season=c(&quot;summer&quot;, &quot;winter&quot;), species=&quot;moose&quot;, density=c(summer_rem,winter_rem), sd=NA) # Add the sd tm &lt;- summer_effort v &lt;- 1.15 moose_res$sd[moose_res$season==&quot;summer&quot;] &lt;- sd(boot_sd(moose_summer)) tm &lt;- winter_effort v &lt;- 1.07 moose_res$sd[moose_res$season==&quot;winter&quot;] &lt;- sd(boot_sd(moose_winter)) Take a look at your results: moose_res ## season species density sd ## 1 summer moose 0.5275170 0.01966975 ## 2 winter moose 0.4094503 0.02852886 Remember, we were expecting a density of around 0.2 to 0.5 individuals per km2… not bad! Lets make a plot of the estimated densities and their standard deviations: library(ggplot2) p&lt;-ggplot(moose_res, aes(x=season, y=density)) + geom_point()+ geom_errorbar(aes(ymin=density-sd, ymax=density+sd), width=.2, position=position_dodge(0.05)) + theme_classic() p 13.2.3 Wolf From existing literature we would expect wolf densities of 0.001 - 0.025 individuals per km2. Lets repeat our analysis for wolves: # Subset to moose (alces alces) and the summer months wolf_summer &lt;- ind[ind$sp==&quot;Canis.lupus&quot; &amp; substr(ind$timestamp,6,7) %in% summer,c(&quot;placename&quot;, &quot;event_groupsize&quot;)] # rename the columns wolf_summer &lt;- wolf_summer %&gt;% rename(&quot;cam&quot;=placename, &quot;count&quot;=event_groupsize) wolf_summer$dist &lt;- 0.024 # Our detection distance in km wolf_summer$theta &lt;- 42.9*(pi/180) # Our viewshed angle # Subset to wolf (alces alces) and the winter months wolf_winter &lt;- ind[ind$sp==&quot;Canis.lupus&quot; &amp; substr(ind$timestamp,6,7) %in% winter,c(&quot;placename&quot;, &quot;event_groupsize&quot;)] # rename the columns wolf_winter &lt;- wolf_winter %&gt;% rename(&quot;cam&quot;=placename, &quot;count&quot;=event_groupsize) # Add the detection zone covariates wolf_winter$dist &lt;- 0.024 wolf_winter$theta &lt;- 42.9*(pi/180) We have the following values for wolf movement Wolf winter = 11.8 km per day Wolf summer = 11.9 km per day summer_rem &lt;- rem(dat = wolf_summer, tm=summer_effort, v=11.9) winter_rem &lt;- rem(dat = wolf_winter, tm=winter_effort, v=11.8) wolf_res &lt;- data.frame(season=c(&quot;summer&quot;, &quot;winter&quot;), species=&quot;wolf&quot;, density=c(summer_rem,winter_rem), sd=NA) # Add the sd tm &lt;- summer_effort v &lt;- 11.9 wolf_res$sd[wolf_res$season==&quot;summer&quot;] &lt;- sd(boot_sd(wolf_summer)) tm &lt;- winter_effort v &lt;- 11.8 wolf_res$sd[wolf_res$season==&quot;winter&quot;] &lt;- sd(boot_sd(wolf_winter)) Which again, is fairly close to our expected densities, if a little on the high side. Again let’s plot them: library(ggplot2) p&lt;-ggplot(wolf_res, aes(x=season, y=density)) + geom_point()+ geom_errorbar(aes(ymin=density-sd, ymax=density+sd), width=.2, position=position_dodge(0.05)) + theme_classic() p These estimates are in the right ball park - and suggesting that wolf densities are higher in winter. The reference for the movement data use to calculate the movement speeds are: Wolves: Latham ADM, Boutin S. 2019. Data from: Wolf ecology and caribou-primary prey-wolf spatial relationships in low productivity peatland complexes in northeastern Alberta. Movebank Data Repository. Moose: Bohm H, Neilson E, de la Mare C, Boutin S (2014) Wildlife habitat effectiveness and connectivity: moose ecology project summary report 2010–2012: Final report. 41 p. 13.2.3.1 Further reading about REM Palencia, Pablo, et al. “Random encounter model is a reliable method for estimating population density of multiple species using camera traps.” Remote Sensing in Ecology and Conservation (2022). Palencia, Pablo, et al. “Assessing the camera trap methodologies used to estimate density of unmarked populations.” Journal of Applied Ecology 58.8 (2021): 1583-1592. 13.2.4 Time to event / Space to event density estimates NIT YET IMPLEMENTED ROBUSTLY - WILL BE UPDATED SOON Whist the REM has been around for a while, there are a couple of new density formulations on the block! Time to event (TTE) and Space to event (STE). Both TTE and STE models use the mathematical relationship between the poisson and exponential distributions to estimate animal density. Conceptually, TTE and STE rely on the basic idea that greater abundance in an area leads to greater detection rates at cameras. The first of these, TTE, estimates abundance from the amount of time that elapses before an animal enters the viewshed of a given camera. TTE requires an independent estimate of animal movement rate. TTE is the only method of the three that requires an estimate of mean animal movement rate, defined across all animal behaviors, including rest (Moeller et al. 2018) Conceptually, STE is similar to TTE with space substituted for time… In contrast to TTE, the STE model uses instantaneous sampling occasions, and therefore it does not depend on movement rate. For both methods, cameras should be randomly or systematically deployed across the sampling frame. Practices to increase detections, such as targeting high-use trails, should be avoided as they can bias the abundance estimate. Second, the authors note that animals should have no behavioral response to cameras or camera sites. This precludes the use of bait or lures to increase encounter rates. It also means that cameras should be unobtrusive and not repel animals with bright flashes or human scent. Finally, the area viewed by each camera should be known across time and measured accurately. If camera area is not measured accurately, abundance estimates will be biased. For STE, motion-sensor detection probability is defined by four conditions: the animal is present in the camera’s viewshed, the motion sensor detects the animal, the camera takes a picture with the animal still in view, and the user correctly identifies the species. Sampling effort is difficult to quantify from motion-sensor photography, as the outcome (no picture) is the same whether the camera stops working, the motion-sensor doesn’t detect the animal, or the animal is absent. Time-lapse photography can help define motion-sensor effort if the two are used in conjunction. For example, time-lapse photos throughout the day will show that the batteries are functioning, the lens is clear of snow and debris, and the camera is pointed in the intended direction, which can help give confidence that the motion sensor is working as intended. STE and TTE use camera data in a particularly unique way that may be unfamiliar to many users. Rather than using counts of individual animals or independent detection events, STE uses the amount of space sampled by cameras until an animal detection at a given time, while TTE uses the time elapsed from an arbitrary starting point to the first detection of the species of interest. Note The details above represent an abridged form of the information in: Moeller, Anna K., and Paul M. Lukacs. “spaceNtime: an R package for estimating abundance of unmarked animals using camera-trap photographs.” Mammalian Biology (2021): 1-10. Again, lets attempt to repeat these methods for moose and wolves. 13.2.4.1 Moose #library(remotes) #remotes::install_github(&quot;annam21/spaceNtime&quot;, build_vignettes=TRUE) library(spaceNtime) The first thing we need is a dataframe of all the detections of our focal species with three columns: cam: the placename (camera location) where surveys occured datetime: the ‘timestamp’ count: the group count img &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_raw_detections.csv&quot;, header=T) # Make ind timestamp a date object img$timestamp &lt;- ymd_hms(img$timestamp) moose_dat &lt;- img %&gt;% filter(sp==&quot;Alces.alces&quot;) %&gt;% dplyr::select(placename, timestamp, event_groupsize) %&gt;% rename(cam = placename, datetime = timestamp, count = event_groupsize) spaceNtime also needs camera deployment data. Luckily, the deployment data it requires is identical to how we store deployment data - each row represents a block of continuous activity, and if the camera ever goes off then comes back online, that is represented as a new row. The deployment data needs the following columns: cam: the placename (camera location) where surveys occured start: deployment start date end: deployment end date area: area in sq meters (we will use the same definition as in the REM ecample 24m depth and radius of 42.9 -&gt; 196m2) dep &lt;- read.csv(&quot;data/raw_data/example_data/dep.csv&quot;, header=T) dep$start_date &lt;- ymd_hms(paste(dep$start_date, &quot;12:00:00&quot;)) dep$end_date &lt;- ymd_hms(paste(dep$end_date, &quot;11:59:00&quot;)) snt_dep &lt;- dep %&gt;% dplyr::select(placename, start_date, end_date) %&gt;% rename(cam = placename, start = start_date, end = end_date) snt_dep$area &lt;- 196 # Remove the NA snt_dep &lt;- snt_dep[is.na(snt_dep$end)==F,] After defining the detection dataframe and deployment data we need to specify the sampling occasions. This can be done manually or with the function build_occ(). The sampling occasions should be in a data.frame or tibble with the following structure: summer_dates &lt;- as.POSIXct(c(&quot;2018-04-07 00:00:00&quot;, &quot;2018-09-30 23:59:59&quot;), tz = &quot;GMT&quot;) summer_occ &lt;- build_occ(samp_freq = 30, # seconds between the start of each sampling occasion samp_length = 2, # duration of each sampling occasion (seconds) study_start = study_dates[1], study_end = study_dates[2]) # Then build our dataframe ste_summer &lt;- ste_build_eh(moose_dat, snt_dep, summer_occ) winter_dates &lt;- as.POSIXct(c(&quot;2018-10-01 00:00:00&quot;, &quot;2019-03-31 23:59:59&quot;), tz = &quot;GMT&quot;) winter_occ &lt;- build_occ(samp_freq = 30, # seconds between the start of each sampling occasion samp_length = 2, # duration of each sampling occasion (seconds) study_start = study_dates[1], study_end = study_dates[2]) # Then build our dataframe ste_winter &lt;- ste_build_eh(moose_dat, snt_dep, winter_occ) # Run the models moose_summer_res &lt;- ste_estN_fn(ste_summer, study_area = 1e6) # specify 1 sq kilometer moose_wnter_res &lt;- ste_estN_fn(ste_winter, study_area = 1e6) # specify 1 sq kilometer An application of this method on wolves can be seen here: Ausband, David E., et al. “Estimating wolf abundance from cameras.” Ecosphere 13.2 (2022): e3933. Ausband, D. E., Luk They didn’t estimate viewshed size either: “we did not estimate viewshed area during camera deployment; thus, we used a viewshed area based on expected performance of motion-triggered cameras. We assumed the cameras detected wolves within a 106-m2, pie-slice shape area in front of the camera derived from standard field protocols and that the motion trigger detected all wolves that passed through the viewshed.” 13.2.5 N-mixture model in unmarked Example coming soon! Example: Ribeiro, Fernando S., et al. “Disturbance or propagule pressure? Unravelling the drivers and mapping the intensity of invasion of free‐ranging dogs across the Atlantic forest hotspot.” Diversity and Distributions 25.2 (2019): 191-204. 13.2.6 ABMI Method Becker, Marcus, et al. “Applying and testing a novel method to estimate animal density from motion‐triggered cameras.” Ecosphere 13.4 (2022: e4005. 13.2.7 Unmarked spatial capture recapture (uSCR) Here you have to currently have to go beyond R - but there are some good options and exciting new developments. See Gilbert et al. 2021 for more details. 13.3 Future directions Direct comparisons of the different methods are starting to appear: Santini, Giacomo, et al. “Population assessment without individual identification using camera-traps: A comparison of four methods.” Basic and Applied Ecology 61 (2022): 68-81. “Further, while unmarked methods require less information and model parameters, there is far greater risk of bias in estimates resulting from model assumptions that are difficult to validate. The inconsistencies in precision of unmarked empirical estimates, even within the same study designs, sites, and species (Table 3), likely demonstrate unaccounted assumption violations pertaining to animal movement and we would expect these issues to extend to most Asian bear populations.” The future of determining viewshed area: Moeller, Anna K., et al. “Best practices to account for capture probability and viewable area in camera‐based abundance estimation.” Remote Sensing in Ecology and Conservation (2022). Nice way of estimating “day range” from camera traps which is equivalent to telemetry data Palencia, Pablo, et al. “Estimating day range from camera‐trap data: the animals’ behaviour as a key parameter.” Journal of Zoology 309.3 (2019): 182-190. And Pablo Palencia has a nice package to help you integrate speed data derived from cameras with activity data derived from camera traps in the activity package. See the Activity chapter! trappingmotion "],["behavior.html", "Chapter 14 Behavior 14.1 Behavioural designations 14.2 Event duration 14.3 Animal speed and day range 14.4 Experimental manipulations 14.5 Interactions 14.6 Worked examples", " Chapter 14 Behavior Camera traps are being used in increasingly creative ways to understand species behaviours, including: activity patterns, foraging tactics, social interactions and predation. Here we will summarise some of the current approaches, and give a quick example using event duration in our example dataset. Camera traps are thought to have some advantages over studies which directly observing animals in the wild, including reduce presence in the field (hopefully leading to the capture of “more natural” behaviors), the ability to be deployed in high numbers are for long periods in time, and the potential to capture standardised observation of behaviour comparable across multiple studies. See Caravaggi, A., et al. “A review of factors to consider when using camera traps to study animal behavior to inform wildlife ecology and conservation. Conservat Sci and Prac. 2020; 2.” (2020). for a balanced consideration of the benefits and potential pitfalls in using cameras for behaviour studies. 14.1 Behavioural designations Vigilant vs. non-vigilant Example Schuttler, Stephanie G., et al. “Deer on the lookout: how hunting, hiking and coyotes affect white‐tailed deer vigilance.” Journal of Zoology 301.4 (2017): 320-327 14.2 Event duration One behavior parameter which is simple to derive from existing camera datasets is the length of the event - which put simply is the interval between the start of a detection event and the end. For this value to be meaningful, it is important that the cameras are on ‘motion’ trigger (as opposed to time lapse) and that the quiet period between detections is very short - so that we have a good idea of when the animal arrived and departed from the frame. What this ‘event duration’ means very much depends on the context of your study region. For example, if some of your cameras are located in rugged and dense terrain, whereas other are in wide open habitat, ‘event duration’ could simply represent the resistance to movement of the habitat. However, if your stations are situated in locations which are very similar in ruggedness or vegetation type, and simply differ in terms of some other experimental manipulation - then event duration could mean something very different. Our Wildlife Coexistance Laboratory recently publish a paper on using ‘event duration’ to explore the responses of ungulates to predation risk. Burton, A. Cole, et al. “Behavioral “bycatch” from camera trap surveys yields insights on prey responses to human‐mediated predation risk.” Ecology and evolution 12.7 (2022): e9108. Typically when we analyse camera trap data we analyse it in units of site_time - for example, the number of detections per week. However in this instance we will be analyzing the individual detection events. load the packages Let’s read in the independent detection data for this project: ind &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_30min_independent_detections.csv&quot;) And the location information: locs &lt;- read.csv(&quot;data/processed_data/AlgarRestorationProject_camera_locations.csv&quot;) When we created this dataset, we calculated a parameter called event_duration - this reflects the interval in seconds between the first detection in an independent event, and the last. Lets look at the distribution of independent events across all of our species: fig &lt;- plot_ly(data=ind,y = ~event_duration, type = &quot;box&quot;,split = ~sp) fig Let’s simpify things, and explore the data for a single species at two of the feature_types (treatment strata): sub_locs &lt;- locs$placename[locs$feature_type %in% c(&quot;HumanUse&quot;, &quot;Offline&quot;)] sub_ind &lt;- ind[ind$sp==&quot;Odocoileus virginianus&quot; &amp; ind$placename %in% sub_locs,] Let’s check out that boxplot again! fig &lt;- plot_ly(data=sub_ind,y = ~event_duration, type = &quot;box&quot;,split = ~sp) fig 14.3 Animal speed and day range Coming soon Palencia, Pablo, et al. “Innovations in movement and behavioural ecology from camera traps: day range as model parameter.” Methods in Ecology and Evolution. 14.4 Experimental manipulations Coming soon 14.5 Interactions Niedballa, Jürgen, et al. “Assessing analytical methods for detecting spatiotemporal interactions between species from camera trapping data.” Remote Sensing in Ecology and Conservation 5.3 (2019): 272-285. # &quot;One approach estimates spatiotemporal avoidance, that is,to what extent site visitation by species A (the ‘primary’species, hereafter) influences subsequent visitations by spe-cies B (the ‘secondary’ species, hereafter, e.g. Harmsenet al. 2009; Parsons et al. 2016; Karanth et al. 2017). Suchavoidance behaviour can be mediated by olfactory (Apfel-bach et al. 2005; Ferrero et al. 2011), visual (Blumsteinet al. 2000; Stankowich and Coss 2007) or acoustic cues(Hauser and Wrangham 1990). The second, more com-monly used approach assesses temporal segregationbetween species. Here, the temporal overlap in activitybetween two species is estimated to assess whether dailyactivity patterns may have shifted in response to the pres-ence of the other species (Ridout and Linkie 2009; Linkieand Ridout 2011; Foster et al. 2013; Lynam et al. 2013;Ross et al. 2013; Farris et al. 2015; Sunarto et al. 2015).Often, camera trap stations are pooled for this analysis,thus omitting spatial information.&quot; ## [1] &quot;One approach estimates spatiotemporal avoidance, that is,to what extent site visitation by species A (the ‘primary’species, hereafter) influences subsequent visitations by spe-cies B (the ‘secondary’ species, hereafter, e.g. Harmsenet al. 2009; Parsons et al. 2016; Karanth et al. 2017). Suchavoidance behaviour can be mediated by olfactory (Apfel-bach et al. 2005; Ferrero et al. 2011), visual (Blumsteinet al. 2000; Stankowich and Coss 2007) or acoustic cues(Hauser and Wrangham 1990). The second, more com-monly used approach assesses temporal segregationbetween species. Here, the temporal overlap in activitybetween two species is estimated to assess whether dailyactivity patterns may have shifted in response to the pres-ence of the other species (Ridout and Linkie 2009; Linkieand Ridout 2011; Foster et al. 2013; Lynam et al. 2013;Ross et al. 2013; Farris et al. 2015; Sunarto et al. 2015).Often, camera trap stations are pooled for this analysis,thus omitting spatial information.&quot; # See file on desktop for code 14.5.1 Activity overlap 14.5.2 One species as a predictor of another Tattersall, E. R., Burgar, J. M., Fisher, J. T. &amp; Burton, A. C. Boreal predator co-occurrences reveal shared use of seismic lines in a working landscape. Ecol. Evol. 10, 1678–1691 (2020). 14.5.3 Residual co-occurence models See: Ovaskainen, O. et al. How to make more out of community data? A conceptual framework and its implementation as models and software. Ecol. Lett. 20, 561–576 (2017). Linear models: Tikhonov, G. et al. Joint species distribution modelling with the r-package Hmsc. Methods Ecol. Evol. 11, 442–447 (2020). Occupancy: Tobler, Mathias W., et al. “Joint species distribution models with species correlations and imperfect detection.” Ecology 100.8 (2019): e02754. 14.5.4 Attractance-Avoidance Ratios (AAR) For a given species of interest (e.g. humans) we can record the period of time until the next detection of an animal species of interest, as well a the period of since it was last detected before the human detection. The ratio between these times is known as the attractace avoidance ratio. See: Parsons, A. W., et al. “The ecological impact of humans and dogs on wildlife in protected areas in eastern North America.” Biological Conservation 203 (2016): 75-88. Naidoo, R. &amp; Burton, A. C. Relative effects of recreational activities on a temperate terrestrial wildlife assemblage. Conserv. Sci. Pract. (2020) Niedballa, Jürgen, et al. “Assessing analytical methods for detecting spatiotemporal interactions between species from camera trapping data.” Remote Sensing in Ecology and Conservation 5.3 (2019): 272-285. Manipulative experiments Measuring species feeding responses to anthropogenic and natural sounds. 14.6 Worked examples Coming soon. Check the wildCo github page for updates "],["on-your-own-5.html", "Chapter 15 On your own 15.1 Working with a new dataset 15.2 Add your covariates 15.3 Spatial data 15.4 Data exploration", " Chapter 15 On your own Congratulations! You made it to the end of the planned content in one peice. You now have the opportunity to apply some of the skills you have learnt to a completely new database. Cut and paste your code from previous chapters to check the data, write the analysis date frames and explore the dataset. I will help you read in the data - and correct a mistake I made. Then you are on your own! 15.1 Working with a new dataset 15.1.1 Read in the data and packages # Load your data pro &lt;- read.csv(&quot;data/raw_data/your_data/proj.csv&quot;, header=T) # I missed of the project ID from all the dtaframes so we need to update those pro$project_id &lt;- pro$project_name img &lt;- read.csv(&quot;data/raw_data/your_data/img.csv&quot;, header=T) img$project_id &lt;- pro$project_name dep &lt;- read.csv(&quot;data/raw_data/your_data/dep.csv&quot;, header=T) dep$project_id &lt;- pro$project_name cam &lt;- read.csv(&quot;data/raw_data/your_data/cam.csv&quot;, header=T) cam$project_id &lt;- pro$project_name # A list of the required packages list.of.packages &lt;- c(&quot;activity&quot;, &quot;corrplot&quot;, &quot;cowplot&quot;, &quot;dplyr&quot;, &quot;elevatr&quot;, &quot;gfcanalysis&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;iNEXT&quot;, &quot;kableExtra&quot;, &quot;Hmsc&quot;, &quot;leaflet&quot;, &quot;lme4&quot;, &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;MCMCvis&quot;, &quot;MODISTools&quot;, &quot;osmdata&quot;, &quot;pals&quot;, &quot;plotly&quot;, &quot;remotes&quot;, &quot;rmarkdown&quot;, &quot;sf&quot;, &quot;spOccupancy&quot;, &quot;stars&quot;, &quot;stringr&quot;, &quot;terra&quot;, &quot;tibble&quot;, &quot;tidyr&quot;, &quot;unmarked&quot;, &quot;viridis&quot;, &quot;jtools&quot;, &quot;vegan&quot;, &quot;MuMIn&quot;) # A check to see which ones you have and which are missing new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # Code which tells R to install the missing packages if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) 15.1.2 Format the dates # Deployment dates # start dates dep$start_date &lt;- ymd(dep$start_date) # end dates dep$end_date &lt;- ymd(dep$end_date) # camera days dep$days &lt;- interval(dep$start_date, dep$end_date)/ddays(1) # Image dates # Image timestamp img$timestamp &lt;- ymd_hms(img$timestamp) 15.1.3 Plot and correct spatial co-ordinates # Pick the category you want to color category &lt;- &quot;feature_type&quot; # We first convert this category to a factor with discrete levels dep[,category] &lt;- factor(dep[,category]) # then use the turbo() function to assign each level a color col.cat &lt;- turbo(length(levels(dep[,category]))) # then we apply it to the dataframe dep$colours &lt;- col.cat[dep[,category]] m &lt;- leaflet() %&gt;% addProviderTiles(providers$Esri.WorldImagery, group=&quot;Satellite&quot;) %&gt;% addTiles(group=&quot;Base&quot;) %&gt;% # Include a basemap option too addCircleMarkers(lng=dep$longitude, lat=dep$latitude, # Co lour the markers depending on the &#39;feature type&#39; color=dep$colours, # Add a popup of the placename and feature_type together popup=paste(dep$placename, dep[,category])) %&gt;% # Add a legend explaining what is going on addLegend(&quot;bottomleft&quot;, colors = col.cat, labels = levels(dep[,category]), title = category, labFormat = labelFormat(prefix = &quot;$&quot;), opacity = 1) %&gt;% # add a layer control box to toggle between the layers addLayersControl( baseGroups = c(&quot;Satellite&quot;, &quot;Base&quot;)) m QUESTION What is the survey design? Hint: zoom in. 15.1.4 Check camera spacing # Distance between traps camera_locs &lt;- dep %&gt;% dplyr::select(placename, latitude, longitude) %&gt;% unique() %&gt;% # remove duplicated rows (rows where the placename and coordinates match) st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = &quot;+proj=longlat&quot;) # Convert to `sf` format # distance matrix for all cameras camera_dist &lt;- st_distance(camera_locs) %&gt;% as.dist() %&gt;% usedist::dist_setNames(as.character(camera_locs$placename)) %&gt;% as.matrix() camera_dist_list &lt;- list() i &lt;- 1 for(i in 1:nrow(camera_dist)) { camera_dist_list[[i]]&lt;- data.frame(placename=row.names(camera_dist)[i], dist= min(camera_dist[i,][camera_dist[i,]!=0])) } camera_dist_list &lt;- bind_rows(camera_dist_list) summary(camera_dist_list$dist) Do you have anything to be worried about here? 15.1.5 Check all deployments have image data # Do all deployments have images table(unique(dep$placename) %in% unique(img$placename)) 15.1.6 5b. Check all images have deployment data # Do all deployments have images table(unique(img$placename) %in% unique(dep$placename)) 15.1.7 5c. Check deployments occur where you expect # Call the plot p &lt;- plot_ly() # We want a separate row for each &#39;placename&#39; - so lets turn it into a factor dep$placename &lt;- as.factor(dep$placename) # loop through each place name for(i in seq_along(levels(dep$placename))) { #Subset the data to just that placename tmp &lt;- dep[dep$placename==levels(dep$placename)[i],] # Order by date tmp &lt;- tmp[order(tmp$start_date),] # Loop through each deployment at that placename for(j in 1:nrow(tmp)) { # Add a line to &#39;p&#39; p &lt;- add_trace(p, #Use the start and end date as x coordinates x = c(tmp$start_date[j], tmp$end_date[j]), #Use the counter for the y coordinates y = c(i,i), # State the type of chart type=&quot;scatter&quot;, # make a line that also has points mode = &quot;lines+markers&quot;, # Add the deployment ID as hover text hovertext=tmp$deployment_id[j], # Color it all black color=I(&quot;black&quot;), # Suppress the legend showlegend = FALSE) } } # Add a categorical y axis p &lt;- p %&gt;% layout(yaxis = list( ticktext = as.list(levels(dep$placename)), tickvals = as.list(1:length(levels(dep$placename))), tickmode = &quot;array&quot;)) p QUESTION What do you think about the camera deployments? 15.1.8 Check images occur within deployments #Camera activity plot # Make a separate plot for each 20 stations For each 20 stations # Order by placename to make the graphs easier to follow dep &lt;- dep[order(dep$placename),] # To do this make a plot dataframe tmp &lt;- data.frame(&quot;deployment_id&quot;=unique(dep$deployment_id), &quot;plot_group&quot;=ceiling(1:length(unique(dep$deployment_id))/20)) dep_tmp &lt;- left_join(dep,tmp, by=&quot;deployment_id&quot;) for(i in 1:max(dep_tmp$plot_group)) { # Call the plot #Subset the data to just that placename tmp &lt;- dep_tmp[dep_tmp$plot_group==i,] # Order by placename tmp &lt;- tmp[order(tmp$placename),] p &lt;- plot_ly() # Loop through each deployment at that placename for(j in 1:nrow(tmp)) { #Subset the image data tmp_img &lt;- img[img$deployment_id==tmp$deployment_id[j],] if(nrow(tmp_img)&gt;0) { p &lt;- add_trace(p, #Use the start and end date as x coordinates x = c(ymd_hms(tmp_img$timestamp)), #Use the counter for the y coordinates y = rep(j, nrow(tmp_img)), # State the type of chart type=&quot;scatter&quot;, # make a line that also has points mode = &quot;markers&quot;, # Add the deployment ID as hover text hovertext=paste(tmp_img$genus,tmp_img$species), # Color it all black marker = list(color = &quot;red&quot;), # Suppress the legend showlegend = FALSE) } # Add a line to &#39;p&#39; p &lt;- add_trace(p, #Use the start and end date as x coordinates x = c(tmp$start_date[j], tmp$end_date[j]+days(1)), #Use the counter for the y coordinates y = c(j,j), # State the type of chart type=&quot;scatter&quot;, # make a line that also has points mode = &quot;lines&quot;, # Add the deployment ID as hover text hovertext=tmp$deployment_id[j], # Color it all black color=I(&quot;black&quot;), # Suppress the legend showlegend = FALSE) } # Add custom y axis labels p &lt;- p %&gt;% layout(yaxis = list( ticktext = as.list(tmp$deployment_id), tickvals = as.list(1:nrow(tmp)), tickmode = &quot;array&quot;)) print(p) } Note - there are three panels to check! QUESTION Do you need to make any edits? Note - there are three plot windows to check. The camera failed for LAC061 - try to seem in and find out the last day it worked. Then update the deployment so the camera stops the day after the last correct date. # The camera failed for LAC061 dep$end_date[dep$deployment_id==&quot;LAC061_C186_010921&quot;] &lt;- ymd(&quot;2021-09-09&quot;) LAC064 also looks somewhat strange, that would be worth checking what happened. Lets leave it in for now. We could maybe remove OFF067 and OFF072? Personal choice! 15.1.9 Check species taxonomy # Create a species list # add an sp column to the img dataframe - remember the genus and species columns are not pasted together yet img$sp &lt;- paste(img$genus, img$species, sep=&quot;.&quot;) ### Species lists # First define vector of the headings you want to see (we will use this trick a lot later on) taxonomy_headings &lt;- c(&quot;class&quot;, &quot;order&quot;, &quot;family&quot;, &quot;genus&quot;, &quot;species&quot;, &quot;common_name&quot;) # Subset the image data to just those columns tmp&lt;- img[,colnames(img)%in% taxonomy_headings] # Remove duplicates tmp &lt;- tmp[duplicated(tmp)==F,] # Create an ordered species list sp_list &lt;- tmp[order(tmp$class, tmp$order, tmp$family, tmp$genus, tmp$species),] sp_list$sp &lt;- paste(sp_list$genus, sp_list$species, sep=&quot;.&quot;) write.csv(sp_list, paste0(&quot;data/raw_data/&quot;,pro$project_id[1],&quot;_raw_species_list.csv&quot;)) QUESTION Are there any classifications you will remove when we create our analysis dataframes? 15.1.10 Check species activity # Diel activity check # First lets convert our timestamp to decimal hours img$hours &lt;- hour(img$timestamp) + minute(img$timestamp)/60 + second(img$timestamp)/(60*60) # Count all of the captures tmp &lt;- img %&gt;% group_by(common_name) %&gt;% summarize(count=n()) yform &lt;- list(categoryorder = &quot;array&quot;, categoryarray = tmp$common_name) fig &lt;- plot_ly(x = img$hours, y = img$common_name,type=&quot;scatter&quot;, height=1000, text=img$deployment_id, hoverinfo=&#39;text&#39;, mode = &#39;markers&#39;, marker = list(size = 5, color = &#39;rgba(50, 100, 255, .2)&#39;, line = list(color = &#39;rgba(0, 0, 0, 0)&#39;, width = 0))) %&gt;% layout(yaxis = yform) fig # Remove the column img$hours &lt;- NULL QUESTION Are there any images you would like to check? Note - Paca’s are nocturnal but they have some diurnal detections here. Check which station they come from. It might be that they are the same station as the one we excluded previously! 15.1.11 Filter to target species # Create your processed dataframe folder dir.create(&quot;data/processed_data&quot;) # Filter your species # Remove observations without animals detected, where we don&#39;t know the species, and non-mammals img_sub &lt;- img %&gt;% filter(is_blank==0, # Remove the blanks is.na(img$species)==FALSE, # Remove classifications which don&#39;t have species class==&quot;Mammalia&quot;, # Subset to mammals species!=&quot;sapiens&quot;, species!=&quot;&quot;) # remove instances without species labels table(img_sub$common_name) QUESTION Are there any other classifications you would like to remove? 15.1.12 Make effort look_up ########################################## # Create your daily lookup # Remove any deployments without end dates tmp &lt;- dep[is.na(dep$end_date)==F,] # Create an empty list to store our days daily_lookup &lt;- list() # Loop through the deployment dataframe and create a row for every day the camera is active for(i in 1:nrow(tmp)) { if(ymd(tmp$start_date[i])!=ymd(tmp$end_date[i])) { daily_lookup[[i]] &lt;- data.frame(&quot;date&quot;=seq(ymd(tmp$start_date[i]), ymd(tmp$end_date[i]), by=&quot;days&quot;), &quot;placename&quot;=tmp$placename[i]) } } # Merge the lists into a dataframe row_lookup &lt;- bind_rows(daily_lookup) # Remove duplicates - when start and end days are the same for successive deployments row_lookup &lt;- row_lookup[duplicated(row_lookup)==F,] 15.1.13 Create independent data ################################### # Create your independent detections independent &lt;- 30 # Check for a `group_size` variable? table(img_sub$group_size) # Check for a &#39;number_of_objects&#39; variable table(img_sub$number_of_objects) QUESTION WHich variable will you use for your animal_count? img_sub$animal_count &lt;- ###INSERT YOUR SELECTION HERE### # Create your independent data img_tmp &lt;- img_sub %&gt;% arrange(deployment_id) %&gt;% # Order by deployment_id group_by(deployment_id, sp) %&gt;% # Group species together mutate(duration = int_length(timestamp %--% lag(timestamp))) # Calculate the gap bet library(stringr) # Give a random value to all cells img_tmp$event_id &lt;- 9999 # Create a counter counter &lt;- 1 # Make a unique code that has one more zero than rows in your dataframe num_code &lt;- as.numeric(paste0(nrow(img_sub),0)) # Loop through img_tmp - if gap is greater than the threshold -&gt; give it a new event ID for (i in 2:nrow(img_tmp)) { img_tmp$event_id[i-1] &lt;- paste0(&quot;E&quot;, str_pad(counter, nchar(num_code), pad = &quot;0&quot;)) if(is.na(img_tmp$duration[i]) | abs(img_tmp$duration[i]) &gt; (independent * 60)) { counter &lt;- counter + 1 } } # Update the information for the last row - the loop above always updates the previous row... leaving the last row unchanged # group ID for the last row if(img_tmp$duration[nrow(img_tmp)] &lt; (independent * 60)| is.na(img_tmp$duration[nrow(img_tmp)])){ img_tmp$event_id[nrow(img_tmp)] &lt;- img_tmp$event_id[nrow(img_tmp)-1] } else{ counter &lt;- counter + 1 img_tmp$event_id[nrow(img_tmp)] &lt;- paste0(&quot;E&quot;, str_pad(counter, nchar(num_code), pad = &quot;0&quot;)) } # remove the duration column img_tmp$duration &lt;- NULL # find out the last and the first of the time in the group top &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% top_n(1,timestamp) %&gt;% dplyr::select(event_id, timestamp) bot &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% top_n(-1,timestamp) %&gt;% dplyr::select(event_id, timestamp) names(bot)[2] &lt;- c(&quot;timestamp_end&quot;) img_num &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% summarise(event_observations=n()) # number of images in the event event_grp &lt;- img_tmp %&gt;% group_by(event_id) %&gt;% summarise(event_groupsize=max(animal_count)) # calculate the duration and add the other elements diff &lt;- top %&gt;% left_join(bot, by=&quot;event_id&quot;) %&gt;% mutate(event_duration=abs(int_length(timestamp %--% timestamp_end))) %&gt;% left_join(event_grp, by=&quot;event_id&quot;)%&gt;% left_join(img_num, by=&quot;event_id&quot;) # Remove columns you don&#39;t need diff$timestamp &lt;-NULL diff$timestamp_end &lt;-NULL diff &lt;- diff[duplicated(diff)==F,] # Merge the img_tmp with the event data img_tmp &lt;- left_join(img_tmp,diff,by=&quot;event_id&quot;) # Remove duplicates ind_dat &lt;- img_tmp[duplicated(img_tmp$event_id)==F,] # Make a unique code for ever day and deployment where cameras were functioning tmp &lt;- paste(row_lookup$date, row_lookup$placename) #Subset ind_dat to data that matches the unique codes ind_dat &lt;- ind_dat[paste(substr(ind_dat$timestamp,1,10), ind_dat$placename) %in% tmp, ] # Convert your species names to factors ind_dat$sp &lt;- as.factor(ind_dat$sp) # Export your data frames write.csv(ind_dat, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_detections.csv&quot;), row.names = F) # also write the cleaned all detections file (some activity analyses require it) write.csv(img_tmp, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_raw_detections.csv&quot;), row.names = F) write.csv(row_lookup, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_daily_lookup.csv&quot;), row.names = F) #Subset the columns tmp &lt;- dep[, c(&quot;project_id&quot;, &quot;placename&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;feature_type&quot;)] # Remove duplicated rows tmp&lt;- tmp[duplicated(tmp)==F,] # write the file write.csv(tmp, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_camera_locations.csv&quot;), row.names = F) tmp &lt;- sp_list[sp_list$sp %in% ind_dat$sp,] write.csv(tmp, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_species_list.csv&quot;), row.names = F) 15.1.14 Create analysis dataframes In the next step, we won’t create the daily dataframe as it takes too long! # Total counts # Station / Month / deport / Species tmp &lt;- row_lookup # Calculate the number of days at each site total_obs &lt;- tmp %&gt;% group_by(placename) %&gt;% summarise(days = n()) # Convert to a data frame total_obs &lt;- as.data.frame(total_obs) # Add columns for each species total_obs[, levels(ind_dat$sp)] &lt;- NA # Duplicate for counts total_count &lt;- total_obs # Test counter i &lt;-1 # For each station, count the number of individuals/observations for(i in 1:nrow(total_obs)) { tmp &lt;- ind_dat[ind_dat$placename==total_obs$placename[i],] tmp_stats &lt;- tmp %&gt;% group_by(sp, .drop=F) %&gt;% summarise(obs=n(), count=sum(animal_count)) total_obs[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$obs total_count[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$count } # Save them write.csv(total_obs, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_total_observations.csv&quot;), row.names = F) write.csv(total_count, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_total_counts.csv&quot;), row.names = F) # Monthly counts # Station / Month / days / Covariates / Species tmp &lt;- row_lookup # Simplify the date to monthly tmp$date &lt;- substr(tmp$date,1,7) # Calculate the number of days in each month mon_obs &lt;- tmp %&gt;% group_by(placename,date ) %&gt;% summarise(days = n()) # Convert to a data frame mon_obs &lt;- as.data.frame(mon_obs) mon_obs[, levels(ind_dat$sp)] &lt;- NA mon_count &lt;- mon_obs # For each month, count the number of individuals/observations for(i in 1:nrow(mon_obs)) { tmp &lt;- ind_dat[ind_dat$placename==mon_obs$placename[i] &amp; substr(ind_dat$timestamp,1,7)== mon_obs$date[i],] tmp_stats &lt;- tmp %&gt;% group_by(sp, .drop=F) %&gt;% summarise(obs=n(), count=sum(animal_count)) mon_obs[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$obs mon_count[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$count } write.csv(mon_obs, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_monthly_observations.csv&quot;), row.names = F) write.csv(mon_count, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_monthly_counts.csv&quot;), row.names = F) ### Weekly # Weekly format # Station / Month / days / Covariates / Species tmp &lt;- row_lookup # Simplify the date to year-week tmp$date &lt;- strftime(tmp$date, format = &quot;%Y-W%U&quot;) # The way this is coded is the counter W01 starts at the first Sunday of the year, everything before that is W00. Weeks do not roll across years. # Calculate the number of days in each week week_obs &lt;- tmp %&gt;% group_by(placename,date ) %&gt;% summarise(days = n()) # Convert to a data frame week_obs &lt;- as.data.frame(week_obs) # Add species columns week_obs[, levels(ind_dat$sp)] &lt;- NA # Duplicate for counts week_count &lt;- week_obs # For each week, count the number of individuals/observations for(i in 1:nrow(week_obs)) { tmp &lt;- ind_dat[ind_dat$placename==week_obs$placename[i] &amp; strftime(ind_dat$timestamp, format = &quot;%Y-W%U&quot;)== week_obs$date[i],] tmp_stats &lt;- tmp %&gt;% group_by(sp, .drop=F) %&gt;% summarise(obs=n(), count=sum(animal_count)) week_obs[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$obs week_count[i,as.character(tmp_stats$sp)] &lt;- tmp_stats$count } write.csv(week_obs, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_weekly_observations.csv&quot;), row.names = F) write.csv(week_count, paste0(&quot;data/processed_data/&quot;,ind_dat$project_id[1], &quot;_&quot;,independent ,&quot;min_independent_weekly_counts.csv&quot;), row.names = F) 15.1.15 Check analysis dataframe counts # Do an final check of the counts tmp &lt;- cbind(data.frame(&quot;Time&quot;=c(&quot;Total&quot;, &quot;Monthly&quot;, &quot;Weekly&quot;)), rbind(colSums(total_obs[,2:ncol(total_obs)]), colSums(mon_obs[,3:ncol(mon_obs)]), colSums(week_obs[,3:ncol(week_obs)]))) tmp %&gt;% kbl() %&gt;% kable_styling(full_width = T) %&gt;% column_spec(1, bold = T, border_right = T)%&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) 15.2 Add your covariates # LOad the packages library(kableExtra);library(dplyr); library(sf); library(modisfast); library(lubridate); library(corrplot); library(traitdata); library(terra); library(osmdata); library(elevatr) 15.2.1 Species Traits # Start by reading in your species list sp_summary &lt;- read.csv(paste0(&quot;data/processed_data/&quot;,pro$project_id[1],&quot;_species_list.csv&quot;), header=T) locs &lt;- read.csv(paste0(&quot;data/processed_data/&quot;,pro$project_id[1],&quot;_camera_locations.csv&quot;), header=T) # Species traits library(traitdata) data(&quot;elton_mammals&quot;) elton_mammals$sp &lt;- paste0(elton_mammals$Genus,&quot;.&quot; ,elton_mammals$Species) tmp &lt;- elton_mammals[, c(&quot;sp&quot;,&quot;BodyMass.Value&quot;, &quot;Activity.Nocturnal&quot;, &quot;Activity.Crepuscular&quot;, &quot;Activity.Diurnal&quot;)] # Lets rename the columns to make them more usable tmp &lt;- tmp %&gt;% rename( mass_g = BodyMass.Value, act_noct = Activity.Nocturnal, act_crep = Activity.Crepuscular, act_diur = Activity.Diurnal) sp_summary &lt;- left_join(sp_summary, tmp) QUESTION Are there any missing values you need to fill in? Dont worry about adding it right now - carry on! write.csv(sp_summary, paste0(&quot;data/processed_data/&quot;, locs$project_id[1],&quot;_species_list.csv&quot;), row.names = F) 15.3 Spatial data 15.3.1 Convert to simple features # Spatial covariates locs_sf &lt;- st_as_sf(locs, # We specify the dataframe coords=c(&quot;longitude&quot;, &quot;latitude&quot;), # The XY coordinates crs=4326) # And the projection code 15.3.2 Get elevation data ################### # Elevation library(elevatr) locs_sf &lt;- get_elev_point(locs_sf, src=&quot;aws&quot;, #Amazon Web Service Terrain Tiles - available globally z = 12) # z specifies the zoom level, the lower the value the faster the code runs, but the coarser the elevation values are boxplot(locs_sf$elevation) 15.3.3 Open street map data ####### # Distance to water (there are no roads here) library(osmdata) aoi &lt;- st_bbox(st_buffer(locs_sf, 10000)) # Units are in meters water &lt;- opq(aoi) %&gt;% add_osm_feature(key = &#39;natural&#39;, value = &#39;water&#39;) %&gt;% osmdata_sf() # THERE IS NO OPEN STREET MAP DATA FOR THIS AREA - ARGH 15.3.4 NDVI ####### # NDVI library(modisfast) log &lt;- mf_login(credentials = c(&quot;cameratrap_course&quot;, &quot;CMI_Revelstoke_2025&quot;)) # set your own EOSDIS username and password # Create aoi if sf format aoi_sf &lt;- st_as_sf(data.frame(id = &quot;los_amigos&quot;, geom = st_as_sfc(aoi), wkt = &quot;geom&quot;, crs = 4326)) # a ROI of interest, format sf polygon # MODIS collections and variables (bands) of interest collection &lt;- &quot;MOD13Q1.061&quot; # run mf_list_collections() for an exhaustive list of collections available variables &lt;- c(&quot;_250m_16_days_NDVI&quot;) # run mf_list_variables(&quot;MOD13Q1.061&quot;) for an exhaustive list of variables available for the collection &quot;MOD13A3.061&quot; # Specify the time range - we will use a winter-summer transition time_range &lt;- as.Date(c(&quot;2022-01-01&quot;, &quot;2022-02-28&quot;)) # a time range of interest ## Get the URLs of the data urls &lt;- mf_get_url( collection = collection, variables = variables, roi = aoi_sf, time_range = time_range ) ## Download the data. By default the data is downloaded in a temporary directory, but you can specify a folder res_dl &lt;- mf_download_data(urls, parallel = TRUE) # Import it into R r &lt;- mf_import_data( path = dirname(res_dl$destfile[1]), collection = collection, proj_epsg = 4326 ) # Plot the resulting NDVI data terra::plot(r, col = rev(terrain.colors(20)), range=c(0,1)) # Specify the range of values you want to plot # Extract the data to camera locations tmp &lt;- terra::extract(r, locs_sf, fun=mean, ID=F) # Extract - each layer will become its own column locs_sf$mean_ndvi &lt;- rowMeans(tmp) # take the mean of each column ####################################### # Convert it back to a dataframe locs_sf$geometry &lt;- NULL locs &lt;- left_join(locs, locs_sf) # Write the dataset write.csv(locs, paste0(&quot;data/processed_data/&quot;, locs$project_id[1],&quot;_camera_locations_and_covariates.csv&quot;), row.names=F) 15.4 Data exploration 15.4.1 Final map list.of.packages &lt;- c(&quot;kableExtra&quot;, &quot;tidyr&quot;, &quot;leaflet&quot;, &quot;dplyr&quot;, &quot;viridis&quot;, &quot;corrplot&quot;, &quot;lubridate&quot;, &quot;plotly&quot;) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) # Final locations plot locs &lt;- read.csv(paste0(&quot;data/processed_data/&quot;,pro$project_id[1],&quot;_camera_locations_and_covariates.csv&quot;)) # If you want to color by a category do it here: category &lt;- &quot;feature_type&quot; # First lets choose a category to color locs[,category] &lt;- factor(locs[,category]) col.cat &lt;- turbo(length(levels(locs[,category]))) # Add it to the dataframe locs$colours &lt;- col.cat[locs[,category]] m &lt;- leaflet() %&gt;% # Add a satellite image layer addProviderTiles(providers$Esri.WorldImagery, group=&quot;Satellite&quot;) %&gt;% addProviderTiles(providers$Esri.WorldTopoMap, group=&quot;Base&quot;) %&gt;% addCircleMarkers(lng=locs$longitude, lat=locs$latitude, # Color the markers depending on the &#39;feature type&#39; color=locs$colours, # Add a popup of the deployment code popup=paste(locs$placename, locs[,category])) %&gt;% # Add a legend explaining what is going on addLegend(&quot;bottomleft&quot;, colors = col.cat, labels = levels(locs[,category]), title = category, labFormat = labelFormat(prefix = &quot;$&quot;), opacity = 1 ) %&gt;% # add a layer control box to toggle between the layers addLayersControl( baseGroups = c(&quot;Satellite&quot;, &quot;Base&quot;), options = layersControlOptions(collapsed = FALSE) ) m 15.4.2 Detection summary sp_summary &lt;- read.csv(paste0(&quot;data/processed_data/&quot;,pro$project_id[1],&quot;_species_list.csv&quot;), header=T) total_obs &lt;- read.csv(paste0(&quot;data/processed_data/&quot;,pro$project_id[1],&quot;_30min_independent_total_observations.csv&quot;), header=T) # Convert to long long_obs &lt;- total_obs %&gt;% pivot_longer(cols=sp_summary$sp, # The columns we want to create into rows - species names_to=&quot;sp&quot;, # What we what the number column to be called values_to = &quot;count&quot;) # Takes the values in the species columns and calls them `count` # We can them summaries those using dplyr tmp &lt;- long_obs %&gt;% # Take the long observation data frame `long_obs` group_by(sp) %&gt;% # Group by species summarise(count=sum(count)) # Sum all the independent observations # Add it to the sp_summary dataframe sp_summary &lt;- left_join(sp_summary, tmp) ## Occupancy # We use the mutate function to mutate the column total_binary &lt;- total_obs %&gt;% # The total obs dataframe mutate(across(sp_summary$sp, ~+as.logical(.x))) # across all of the species columns, make it binary # Flip the dataframe to longer - as before long_bin &lt;- total_binary %&gt;% pivot_longer(cols=sp_summary$sp, names_to=&quot;sp&quot;, values_to = &quot;count&quot;) # Takes the species names columns, and makes them unique rows with &quot;sp&quot; as the key # We can now sum the presence/absences and divide by the number of survey locations tmp &lt;- long_bin %&gt;% group_by(sp) %&gt;% summarise(occupancy=sum(count)/nrow(locs)) # divided the sum by the number of sites # add the results to the sp_summary sp_summary &lt;- left_join(sp_summary, tmp) ########################### # Comparison plot # Lets put the dataframes in a sensible order sp_summary &lt;- sp_summary[order(sp_summary$count),] yform &lt;- list(categoryorder = &quot;array&quot;, categoryarray = sp_summary$sp) xform &lt;- list(title=&quot;Captures&quot;) # Capture rate fig1 &lt;- plot_ly(x = sp_summary$count, y = sp_summary$common_name, type = &#39;bar&#39;, orientation = &#39;h&#39;) %&gt;% layout(yaxis = yform, xaxis=xform) yform &lt;- list(categoryorder = &quot;array&quot;, categoryarray = sp_summary$sp, showticklabels=F) xform &lt;- list(title=&quot;Occupancy&quot;) # Occupancy fig2 &lt;- plot_ly(x = sp_summary$occupancy, y = sp_summary$common_name, type = &#39;bar&#39;, orientation = &#39;h&#39;) %&gt;% layout(yaxis = yform, xaxis=xform) subplot(nrows=1,fig1, fig2, titleX = T) # We could stack them on top of one another using nrows=2 QUESTION What does this tell you about the behavior of the target species? 15.4.3 Temporal patterns par(mar=c(5,4,1,1)) # Temporal patterns mon_obs &lt;- read.csv(paste0(&quot;data/processed_data/&quot;,pro$project_id[1],&quot;_30min_independent_monthly_observations.csv&quot;), header=T) # Count up the number of stations and the number of camera nights mon_summary &lt;- mon_obs %&gt;% # Use the monthly observations dataframe group_by(date) %&gt;% # Group by the date summarise(locs_active=n(), # Count the number of active cameras cam_days=sum(days)) # And sum the active days # Add in the species specific counts - and join it with the mon_summary dataframe mon_summary &lt;- mon_obs %&gt;% group_by(date) %&gt;% summarise(across(sp_summary$sp, sum, na.rm=TRUE)) %&gt;% # summarise across all of # the species columns left_join(x=mon_summary) # Join with the mon_summary dataframe # We first need to convert the date column to a date object mon_summary$date &lt;- ym(mon_summary$date) # Set up a two panel plot (side by side) par(mfrow=c(1,2)) plot(mon_summary$date, mon_summary$locs_active, type=&quot;o&quot;, pch=19, ylim=c(0, max(mon_summary$locs_active)), las=1, ylab=&quot;Number of cameras active&quot;, xlab=&quot;Date&quot;) # Sum all the captures rates for the species columns mon_summary$all.sp &lt;- rowSums(mon_summary[, sp_summary$sp]) # Plot them plot(mon_summary$date, mon_summary$all.sp/(mon_summary$cam_days/100), type=&quot;o&quot;, pch=19, las=1, ylab=&quot;Detections per 100 cam days&quot;, xlab=&quot;Date&quot;) Question What is going on in these plots? # Species specific plots par(mfrow=c(2,2)) i &lt;- 1 for(i in 1:length(sp_summary$sp)) { plot(mon_summary$date, pull(mon_summary, sp_summary$sp[i])/(mon_summary$cam_days/100), # The pull command allows you to grab a specific column in a dataframe and turn it into a vector! type=&quot;o&quot;, pch=19, las=1, ylab=&quot;Detections per 100 cam days&quot;, xlab=&quot;Date&quot;, main=sp_summary$sp[i]) } Question Any interesting patterns? 15.4.4 Spatial patterns # Spatial plots total_obs &lt;- left_join(total_obs, locs) # Jaguar focal_species &lt;- &quot;Panthera.onca&quot; focal_cr &lt;- pull(total_obs, focal_species)/(total_obs$days/100) m &lt;- leaflet() %&gt;% addProviderTiles(providers$Esri.WorldTopoMap, group=&quot;Base&quot;) %&gt;% addCircleMarkers(lng=locs$longitude, lat=locs$latitude, # Add a popup of the deployment code popup=paste(locs$placename), radius=(focal_cr/max(focal_cr)*10)+1, stroke=F, fillOpacity=0.6) m Try other species! Question Any interesting patterns? 15.4.5 Species co-occurance # Co-occurances par(mfrow=c(1,1)) # Pull the data for each of the species from tmp &lt;- total_obs[, sp_summary$sp] M &lt;- cor(tmp) corrplot(M, method=&quot;color&quot;, type=&quot;upper&quot;, order=&quot;hclust&quot;, # addCoef.col = &quot;black&quot;, # We suppress the coefs to make a cleaner plot tl.col=&quot;black&quot;, tl.srt=45, #Text label color and rotation diag=FALSE ) Question Any interesting patterns? You can now do some data exploration plots. # Prepare the data locs &lt;- locs %&gt;% mutate_if(is.character,as.factor) # If a column is a character string, make it a factor total_obs &lt;- left_join(total_obs, locs) We have the following species: sp_summary$sp And the following categories to explore: c(&quot;feature_type&quot;, &quot;elevation&quot;, &quot;mean_ndvi&quot;) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
